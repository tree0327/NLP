{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "238fe9ea",
   "metadata": {},
   "source": [
    "# Text Vectorization\n",
    "텍스트 데이터를 기계 학습이나 딥러닝 모델이 처리할 수 있도록 수치 벡터 형태로 변환하는 기법\n",
    "\n",
    "**텍스트 벡터화의 분류**\n",
    "1. **문서 표현 (Document Representation)**\n",
    "   - 텍스트를 **문서 단위로 벡터화**.\n",
    "   - 주요 기법:\n",
    "     - CountVectorizer (단어 빈도 기반)\n",
    "     - TfidfVectorizer (단어 중요도 기반)\n",
    "     - Doc2Vec (문서의 의미를 학습)\n",
    "\n",
    "2. **단어 표현 (Word Representation)**\n",
    "   - 텍스트를 **단어 단위로 벡터화**.\n",
    "   - 주요 기법:\n",
    "     - One-Hot Encoding (국소 표현)\n",
    "     - Word Embedding (연속 표현) (Word2Vec, GloVe, FastText 등)\n",
    "\n",
    "\n",
    "\n",
    "| **구분**                | **빈도 기반 문서 표현**                                  | **원핫 인코딩 단어표현** | **임베딩 단어표현**                              |\n",
    "|--------------------------|--------------------------------------------------------|-----------------|-------------------------------------------|\n",
    "| **표현 대상**           | 문서 전체                                             | 단어/카테고리         | 단어 또는 문장                                  |\n",
    "| **출력 값**             | 단어 빈도 또는 중요도 (TF-IDF)                           | 이진 벡터 (0 또는 1)  | 실수 벡터 (다차원 공간에서 의미 반영)                    |\n",
    "| **정보 표현 방식**      | 단순 빈도/중요도 기반                                    | 단순 존재 여부        | 의미 기반 (단어 간 관계와 유사도 반영)                   |\n",
    "| **의미 관계**           | 없음                                                   | 없음              | 있음 (문맥과 의미 관계 반영 가능)                      |\n",
    "| **차원 밀집도**         | 희소 벡터 (Sparse Vector)                               | 희소 벡터           | 밀집 벡터 (Dense Vector)                      |\n",
    "| **사용 사례**           | 텍스트 분류, 문서 검색                                   | 카테고리형 데이터 처리    | 단어/문장 의미 분석, 번역, 유사도 계산                   |\n",
    "| **대표 기법**           | CountVectorizer, TfidfVectorizer                        | keras.utils.to_categorical() | Word2Vec, GloVe, FastText, BERT Embedding |\n",
    "\n",
    "\n",
    "### 🌳 단어 표현 구분\n",
    "![](https://d.pr/i/zLBglx+)\n",
    "\n",
    "\n",
    "1. **Local Representation (지역적 표현)**\n",
    "\n",
    "단어 하나하나를 독립적으로 보고 표현함. 문맥은 고려하지 않는다.\n",
    "\n",
    "1-1. **One-hot Vector**\n",
    "\n",
    "* 각 단어를 고유한 벡터로 표현 (예: \\[0, 0, 1, 0, 0])\n",
    "* 단어 간 유사도를 반영하지 못함\n",
    "\n",
    "1-2. **N-gram**\n",
    "\n",
    "* 단어가 연속적으로 등장하는 패턴을 학습 (예: 2-gram → \"나는\", \"는 밥\")\n",
    "* 문맥 일부 반영 가능하지만 차원이 커짐\n",
    "\n",
    "1-3. **Count Based (BoW, DTM)**\n",
    "\n",
    "* 특정 단어가 문서 내 몇 번 등장했는지 세는 방식\n",
    "* 대표 예: **Bag of Words**, **Document-Term Matrix (DTM)**\n",
    "  → 문맥 고려는 안 됨\n",
    "\n",
    "\n",
    "2. **Continuous Representation (연속적 표현)**\n",
    "\n",
    "단어를 벡터 공간의 연속된 숫자 형태로 표현하며, **의미나 문맥을 반영**하려 함\n",
    "\n",
    "2-1. **Prediction Based (예측 기반)**\n",
    "\n",
    "* 특정 단어 주변의 단어를 예측하거나, 반대로 중심 단어를 예측하는 방식\n",
    "* 예: **Word2Vec**, **FastText**\n",
    "\n",
    "  * Word2Vec: CBOW/Skip-gram\n",
    "  * FastText: 단어를 자소 단위로 쪼개어 벡터화 → OOV 단어도 처리 가능\n",
    "\n",
    "2-2. **Count Based (의미 기반의 카운트 방식)**\n",
    "\n",
    "Count는 사용하지만, 더 정교한 통계기법과 행렬 분해를 사용하여 의미를 반영한다.\n",
    "\n",
    "* **LSA (Latent Semantic Analysis)**\n",
    "  전체 문서를 기준으로 DTM을 만들고, **SVD 분해**해서 의미 공간을 추출\n",
    "  → 문서 전체 문맥을 반영\n",
    "\n",
    "* **GloVe (Global Vectors for Word Representation)**\n",
    "  윈도우 기반으로 단어쌍 공기 행렬을 만들고, 의미를 추출\n",
    "  → Word2Vec보다 global한 정보 반영 가능\n",
    "\n",
    "* **BERT Embedding**\n",
    "  문장의 문맥을 양방향으로 고려하는 사전학습 기반 벡터화. 문장 전체의 의미를 반영해서 단어 벡터 + 문장 벡터 생성 가능\n",
    "  \n",
    "  \"나는 은행에 갔다\" vs \"은행에서 일한다\"의 **‘은행’**을 문맥에 따라 다르게 벡터화\n",
    "\n",
    "  →  특징: 가장 강력한 방식, 문맥과 의미를 모두 반영. 연산 비용이 큼.\n",
    "\n",
    "\n",
    "\n",
    "### 다양한 벡터화 기법\n",
    "\n",
    "1. BOW(Bag of Words)\n",
    "  \n",
    "    문서가 가지는 모든 단어를 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도 값을 부여해 피처 값을 추출하는 모델이다.\n",
    "    \n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*S8uW62e3AbYw3gnylm4eDg.png\" width=\"500px\">\n",
    "\n",
    "2. **One Hot Encoding**\n",
    "    \n",
    "    단어를 고유 인덱스로 변환하여 벡터화\n",
    "  \n",
    "    <img src=\"https://d.pr/i/5nWhHo+\"/>    \n",
    "\n",
    "3. **Word Embedding**\n",
    "    \n",
    "    단어를 밀집 벡터(dense vector)로 표현하는 방법으로, 단어의 의미와 관계를 보존하며 벡터로 표현한다.\n",
    "    \n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*jpnKO5X0Ii8PVdQYFO2z1Q.png\" width=\"500px\">\n",
    "\n",
    "\n",
    "| Word          | 성별   | 귀족   | 나이   | 음식   |\n",
    "|---------------|--------|--------|--------|--------|\n",
    "| Man (5391)    | -1.07  | 0.01   | 0.03   | 0.04   |\n",
    "| Woman (9853)  | 1.34   | 0.02   | 0.02   | 0.01   |\n",
    "| King (4914)   | -0.95  | 0.93   | 0.70   | 0.02   |\n",
    "| Queen (7157)  | 0.97   | 0.95   | 0.69   | 0.01   |\n",
    "| Apple (456)   | 0.00   | -0.01  | 0.03   | 0.95   |\n",
    "| Orange (6257) | 0.01   | 0.00   | -0.02  | 0.97   |\n",
    "    \n",
    "\n",
    "\n",
    "4. FastText\n",
    "\n",
    "    **Word2Vec의 확장. 단어 내부의 자소 단위까지 고려**\n",
    "\n",
    "   * 예: \"먹었다\" → \"먹\", \"었\", \"다\"로 쪼개서 벡터 생성\n",
    "   * 어휘에 없던 단어(OOV)도 처리 가능, 신조어나 오탈자 단어도 어느 정도 이해 가능\n",
    "\n",
    "5. BERT Embedding\n",
    "\n",
    "    **문장의 문맥을 양방향으로 고려하는 사전학습 기반 벡터화**\n",
    "    연산비용이 크지만, 가장 강력한 방식, 문맥과 의미를 모두 반영.\n",
    "\n",
    "   * \"나는 은행에 갔다\" vs \"은행에서 일한다\"의 **‘은행’**을 문맥에 따라 다르게 벡터화\n",
    "   * 문장 전체의 의미를 반영해서 **단어 벡터 + 문장 벡터** 생성 가능\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
