{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1936cbb1",
   "metadata": {},
   "source": [
    "# torch `nn.Embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2273d8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21294f3b",
   "metadata": {},
   "source": [
    "## 사전학습된 임베딩을 사용하지 않는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "270ec97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [          \n",
    "    'nice great best amazing',  # 긍정 문장 예시\n",
    "    'stop lies',                # 부정/비판 문장 예시\n",
    "    'pitiful nerd',             # 부정 문장 예시\n",
    "    'excellent work',           # 긍정 문장 예시\n",
    "    'supreme quality',          # 긍정 문장 예시\n",
    "    'bad',                      # 부정 문장 예시\n",
    "    'highly respectable'        # 긍정 문장 예시\n",
    "]                               # 분류 모델에 넣을 입력 문장 리스트(list[str])\n",
    "labels = [1, 0, 0, 1, 1, 0, 1]  # 각 문장에 대한 이진 라벨(1=긍정, 0=부정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bce4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nice', 'great', 'best', 'amazing'],\n",
       " ['stop', 'lies'],\n",
       " ['pitiful', 'nerd'],\n",
       " ['excellent', 'work'],\n",
       " ['supreme', 'quality'],\n",
       " ['bad'],\n",
       " ['highly', 'respectable']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰화\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_sentences = [word_tokenize(sent) for sent in sentences]    # 각 문장을 토큰 리스트(list(list[str]))로 변환\n",
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eea1eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'nice': 1, 'great': 1, 'best': 1, 'amazing': 1, 'stop': 1, 'lies': 1, 'pitiful': 1, 'nerd': 1, 'excellent': 1, 'work': 1, 'supreme': 1, 'quality': 1, 'bad': 1, 'highly': 1, 'respectable': 1})\n",
      "{'<PAD>': 0, '<UNK>': 1, 'nice': 2, 'great': 3, 'best': 4, 'amazing': 5, 'stop': 6, 'lies': 7, 'pitiful': 8, 'nerd': 9, 'excellent': 10, 'work': 11, 'supreme': 12, 'quality': 13, 'bad': 14, 'highly': 15, 'respectable': 16}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 사전 생성 + 정수 인코딩\n",
    "from collections import Counter\n",
    "\n",
    "tokens = [token for sent in tokenized_sentences for token in sent]  # 문장 리스트를 평탄화하여 전체 토큰 리스트 생성\n",
    "word_counts = Counter(tokens)  # 전체 토큰 등장 빈도 계산\n",
    "print(word_counts)  # 토큰별 빈도 딕셔너리 형태\n",
    "\n",
    "word_to_index = {word: index + 2 for index, word in enumerate(tokens)}  # 토큰을 순서대로 인덱싱(+2 : 특수토큰용)\n",
    "word_to_index['<PAD>'] = 0    # 패딩 토큰 (길이 맞추기용)\n",
    "word_to_index['<UNK>'] = 1    # OOV 토큰 (처리 불가 단어 대체)\n",
    "word_to_index = dict(sorted(word_to_index.items(), key=lambda x: x[1]))  # 인덱스를 기준으로 정렬\n",
    "print(word_to_index)  # 단어 -> 인덱스 사전\n",
    "\n",
    "vocab_size = len(word_to_index)  # 전체 어휘 수 (특수토큰 포함)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e38d05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14], [15, 16]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정수 인코딩 함수 : 토큰화된 문장 리스트를 단어 -> 인덱스 사전으로 정수 시퀀스 (list[list(int)])로 변환\n",
    "def texts_to_sequences(sentences, word_to_index):\n",
    "    sequences = []\n",
    "\n",
    "    for sent in sentences:  # 문장 단위로 반복\n",
    "        sequence = []\n",
    "\n",
    "        for token in sent:\n",
    "            if token in word_to_index:\n",
    "                sequence.append(word_to_index[token])  # 해당 단어 인덱스 추가\n",
    "            else:\n",
    "                sequence.append(word_to_index['<UNK>'])  # 사전에 없으면 UNK 토큰\n",
    "        \n",
    "        sequences.append(sequence)\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "sequences = texts_to_sequences(tokenized_sentences, word_to_index)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afbada9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  3,  4,  5],\n",
       "       [ 6,  7,  0,  0],\n",
       "       [ 8,  9,  0,  0],\n",
       "       [10, 11,  0,  0],\n",
       "       [12, 13,  0,  0],\n",
       "       [14,  0,  0,  0],\n",
       "       [15, 16,  0,  0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 서로 다른 길이의 정수 시퀀스를 0(<PAD>)으로 채우거나 잘라내 (문장수, maxlen) 형태에 맞춰주는 함수\n",
    "def pad_sequences(sequences, maxlen):\n",
    "    padded_sequences = np.zeros((len(sequences), maxlen), dtype=int)  # (문장수 x maxlen) 크기의 0 패딩 배열\n",
    "    \n",
    "    for index, seq in enumerate(sequences):  # 각 문장 시퀀스 순회\n",
    "        padded_sequences[index, :len(seq)] = seq[:maxlen]  # 앞에서부터 시퀀스 채운다. 길면 maxlen까지만 채워 자른다.\n",
    "    \n",
    "    return padded_sequences  # 패딩 작업 완료된 2D 배열\n",
    "\n",
    "padded_sequences = pad_sequences(sequences, maxlen=4)  # 모든 문장 길이 4로 패딩/자르기\n",
    "padded_sequences  # (문장 수, 4) 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6df01d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db84bcf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNet(\n",
       "  (embedding): Embedding(17, 100, padding_idx=0)\n",
       "  (rnn): RNN(100, 16, batch_first=True)\n",
       "  (out): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pytorch 텍스트 분류 모델 : Embedding + RNN + Linear로 이진 분류(logit) 출력\n",
    "import torch\n",
    "import torch.nn as nn           # 신경망 레이어\n",
    "import torch.optim as optim     # 옵티마이저(활성화함수)\n",
    "from torch.utils.data import DataLoader, TensorDataset  # 배치 로더 / 데이터셋 유틸\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    # 정수 시퀀스를 임베딩 -> RNN -> 선형층으로 처리해 이진 분류 logit(1개)를 출력\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super().__init__()                    # nn.Module 초기화\n",
    "        self.embedding = nn.Embedding(        # 단어 ID를 밀집 벡터로 변환하는 임베딩 층\n",
    "            num_embeddings = vocab_size,      # 단어 사전 크기 (어휘 수)\n",
    "            embedding_dim = embedding_dim,    # 임베딩 차원\n",
    "            padding_idx = 0                   # PAD(0) 인덱스는 0 그대로 사용\n",
    "        )\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)    # 입력(배치, 길이, 차원) 형태의 RNN\n",
    "        self.out = nn.Linear(hidden_size, 1)     # 마지막 은닉 상태를 1차원 logit으로 변환\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)     # (batch, seq_len) -> (batch, seq_len, embedding_dim)\n",
    "        out, h_n = self.rnn(embedded)    # h_n: (num_layers*directions, batch, hidden_size)\n",
    "        out = self.out(h_n.squeeze(0))   # (batch_size, hidden_size) -> (batch, 1)\n",
    "        return out  # 출력 : 시그모이드 전 logit(확률이 아님)\n",
    "    \n",
    "embedding_dim = 100  # 단어 벡터 차원 설정\n",
    "model = SimpleNet(vocab_size, embedding_dim, hidden_size=16)  # 어휘 크기 / 임베딩 차원/ 은닉크기로 모델 생성\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84068e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b500c836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "SimpleNet                                --\n",
       "├─Embedding: 1-1                         1,700\n",
       "├─RNN: 1-2                               1,888\n",
       "├─Linear: 1-3                            17\n",
       "=================================================================\n",
       "Total params: 3,605\n",
       "Trainable params: 3,605\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary  # 모델 구조를 표 형태로 요약\n",
    "\n",
    "summary(model)  # model의 레이어 구성 / 파라미터 수를 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c8c1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 100])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;PAD&gt;</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;UNK&gt;</th>\n",
       "      <td>-0.503153</td>\n",
       "      <td>1.122852</td>\n",
       "      <td>-0.589110</td>\n",
       "      <td>-1.338089</td>\n",
       "      <td>0.537808</td>\n",
       "      <td>-0.930078</td>\n",
       "      <td>0.174860</td>\n",
       "      <td>-0.660207</td>\n",
       "      <td>0.922226</td>\n",
       "      <td>2.106441</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.911769</td>\n",
       "      <td>0.122990</td>\n",
       "      <td>0.896133</td>\n",
       "      <td>0.324314</td>\n",
       "      <td>-0.060996</td>\n",
       "      <td>0.750533</td>\n",
       "      <td>0.698772</td>\n",
       "      <td>-0.087897</td>\n",
       "      <td>0.447661</td>\n",
       "      <td>-0.242309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nice</th>\n",
       "      <td>-1.100964</td>\n",
       "      <td>0.893685</td>\n",
       "      <td>0.193806</td>\n",
       "      <td>-1.329189</td>\n",
       "      <td>1.789114</td>\n",
       "      <td>-0.522488</td>\n",
       "      <td>0.262018</td>\n",
       "      <td>0.498728</td>\n",
       "      <td>-1.204775</td>\n",
       "      <td>0.857129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.413068</td>\n",
       "      <td>1.536693</td>\n",
       "      <td>1.346791</td>\n",
       "      <td>0.896332</td>\n",
       "      <td>-0.065012</td>\n",
       "      <td>-1.482238</td>\n",
       "      <td>-1.226632</td>\n",
       "      <td>0.135369</td>\n",
       "      <td>1.297657</td>\n",
       "      <td>-0.205065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>-2.082712</td>\n",
       "      <td>0.616099</td>\n",
       "      <td>-0.777128</td>\n",
       "      <td>1.908266</td>\n",
       "      <td>0.761909</td>\n",
       "      <td>1.869433</td>\n",
       "      <td>0.292291</td>\n",
       "      <td>-0.936683</td>\n",
       "      <td>-0.555840</td>\n",
       "      <td>2.168294</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.371885</td>\n",
       "      <td>1.953568</td>\n",
       "      <td>0.183500</td>\n",
       "      <td>-0.158066</td>\n",
       "      <td>0.563312</td>\n",
       "      <td>-0.492190</td>\n",
       "      <td>-0.465123</td>\n",
       "      <td>-0.958200</td>\n",
       "      <td>0.576206</td>\n",
       "      <td>1.370161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>-0.392523</td>\n",
       "      <td>0.098091</td>\n",
       "      <td>-2.461212</td>\n",
       "      <td>1.324039</td>\n",
       "      <td>0.626163</td>\n",
       "      <td>0.212848</td>\n",
       "      <td>-0.234914</td>\n",
       "      <td>-0.497381</td>\n",
       "      <td>0.345082</td>\n",
       "      <td>-1.305001</td>\n",
       "      <td>...</td>\n",
       "      <td>1.404677</td>\n",
       "      <td>1.085973</td>\n",
       "      <td>1.415279</td>\n",
       "      <td>-0.354888</td>\n",
       "      <td>0.088245</td>\n",
       "      <td>-0.147179</td>\n",
       "      <td>1.300656</td>\n",
       "      <td>1.109785</td>\n",
       "      <td>-1.246710</td>\n",
       "      <td>0.649416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>-1.052736</td>\n",
       "      <td>0.373408</td>\n",
       "      <td>-0.122459</td>\n",
       "      <td>-1.373757</td>\n",
       "      <td>-0.713679</td>\n",
       "      <td>-0.755578</td>\n",
       "      <td>-0.649324</td>\n",
       "      <td>0.685076</td>\n",
       "      <td>-1.486379</td>\n",
       "      <td>0.652252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.893645</td>\n",
       "      <td>0.573710</td>\n",
       "      <td>0.211526</td>\n",
       "      <td>-0.341499</td>\n",
       "      <td>-1.023671</td>\n",
       "      <td>1.033080</td>\n",
       "      <td>-0.349468</td>\n",
       "      <td>0.492900</td>\n",
       "      <td>-0.180742</td>\n",
       "      <td>0.708267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop</th>\n",
       "      <td>-0.497697</td>\n",
       "      <td>-0.235544</td>\n",
       "      <td>-0.375438</td>\n",
       "      <td>1.334417</td>\n",
       "      <td>-1.592178</td>\n",
       "      <td>-1.880756</td>\n",
       "      <td>-0.395881</td>\n",
       "      <td>2.018922</td>\n",
       "      <td>-0.983122</td>\n",
       "      <td>0.782248</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044324</td>\n",
       "      <td>-1.229873</td>\n",
       "      <td>0.401176</td>\n",
       "      <td>-0.847899</td>\n",
       "      <td>0.147723</td>\n",
       "      <td>-0.908863</td>\n",
       "      <td>-1.022872</td>\n",
       "      <td>0.476816</td>\n",
       "      <td>1.072323</td>\n",
       "      <td>-1.172453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lies</th>\n",
       "      <td>0.024769</td>\n",
       "      <td>-0.456872</td>\n",
       "      <td>-0.664279</td>\n",
       "      <td>1.604704</td>\n",
       "      <td>0.140491</td>\n",
       "      <td>0.072229</td>\n",
       "      <td>-0.196021</td>\n",
       "      <td>2.606885</td>\n",
       "      <td>2.345440</td>\n",
       "      <td>-0.094659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120991</td>\n",
       "      <td>-0.098169</td>\n",
       "      <td>-0.863077</td>\n",
       "      <td>-0.096738</td>\n",
       "      <td>-0.500837</td>\n",
       "      <td>0.618213</td>\n",
       "      <td>0.010688</td>\n",
       "      <td>-1.475451</td>\n",
       "      <td>0.053425</td>\n",
       "      <td>-1.066209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pitiful</th>\n",
       "      <td>0.877789</td>\n",
       "      <td>1.218326</td>\n",
       "      <td>-0.743894</td>\n",
       "      <td>-0.279423</td>\n",
       "      <td>0.816912</td>\n",
       "      <td>-1.105900</td>\n",
       "      <td>-0.822485</td>\n",
       "      <td>0.553456</td>\n",
       "      <td>2.524921</td>\n",
       "      <td>1.436808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.746145</td>\n",
       "      <td>-1.203568</td>\n",
       "      <td>0.519214</td>\n",
       "      <td>-0.210401</td>\n",
       "      <td>-0.593928</td>\n",
       "      <td>1.157350</td>\n",
       "      <td>1.680863</td>\n",
       "      <td>1.440359</td>\n",
       "      <td>0.477093</td>\n",
       "      <td>0.506625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nerd</th>\n",
       "      <td>-1.140655</td>\n",
       "      <td>-0.458999</td>\n",
       "      <td>-2.165972</td>\n",
       "      <td>1.278016</td>\n",
       "      <td>-1.035399</td>\n",
       "      <td>-1.723518</td>\n",
       "      <td>-0.620779</td>\n",
       "      <td>-0.368372</td>\n",
       "      <td>0.688099</td>\n",
       "      <td>-0.453923</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.656560</td>\n",
       "      <td>-0.213419</td>\n",
       "      <td>0.089783</td>\n",
       "      <td>0.813107</td>\n",
       "      <td>1.426436</td>\n",
       "      <td>1.674970</td>\n",
       "      <td>-0.456080</td>\n",
       "      <td>-1.502486</td>\n",
       "      <td>-1.498011</td>\n",
       "      <td>-0.500809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excellent</th>\n",
       "      <td>0.173515</td>\n",
       "      <td>1.234430</td>\n",
       "      <td>1.410012</td>\n",
       "      <td>0.114707</td>\n",
       "      <td>-0.236008</td>\n",
       "      <td>-2.808406</td>\n",
       "      <td>-0.130736</td>\n",
       "      <td>-0.156313</td>\n",
       "      <td>0.672824</td>\n",
       "      <td>1.452021</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.400905</td>\n",
       "      <td>0.251868</td>\n",
       "      <td>-1.616018</td>\n",
       "      <td>1.222195</td>\n",
       "      <td>0.213714</td>\n",
       "      <td>0.853370</td>\n",
       "      <td>0.273833</td>\n",
       "      <td>0.290795</td>\n",
       "      <td>-0.348189</td>\n",
       "      <td>-1.515442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>0.746995</td>\n",
       "      <td>0.566075</td>\n",
       "      <td>0.007253</td>\n",
       "      <td>0.227592</td>\n",
       "      <td>-0.292766</td>\n",
       "      <td>-1.246617</td>\n",
       "      <td>2.596814</td>\n",
       "      <td>-0.433214</td>\n",
       "      <td>-0.152662</td>\n",
       "      <td>0.848396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.766436</td>\n",
       "      <td>0.180232</td>\n",
       "      <td>0.817073</td>\n",
       "      <td>-0.289101</td>\n",
       "      <td>-0.826896</td>\n",
       "      <td>0.191386</td>\n",
       "      <td>-1.296251</td>\n",
       "      <td>1.203728</td>\n",
       "      <td>0.078206</td>\n",
       "      <td>-1.352532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>supreme</th>\n",
       "      <td>-0.019000</td>\n",
       "      <td>0.428264</td>\n",
       "      <td>-0.053109</td>\n",
       "      <td>-0.900704</td>\n",
       "      <td>0.521697</td>\n",
       "      <td>0.155414</td>\n",
       "      <td>-1.034958</td>\n",
       "      <td>0.237269</td>\n",
       "      <td>0.967571</td>\n",
       "      <td>1.935323</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.113440</td>\n",
       "      <td>0.212698</td>\n",
       "      <td>0.489131</td>\n",
       "      <td>1.698885</td>\n",
       "      <td>0.156688</td>\n",
       "      <td>2.001115</td>\n",
       "      <td>-3.104030</td>\n",
       "      <td>1.069025</td>\n",
       "      <td>0.926571</td>\n",
       "      <td>0.831254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>0.184762</td>\n",
       "      <td>0.986030</td>\n",
       "      <td>-0.092353</td>\n",
       "      <td>0.233340</td>\n",
       "      <td>-1.905763</td>\n",
       "      <td>-0.162851</td>\n",
       "      <td>-0.900567</td>\n",
       "      <td>0.008757</td>\n",
       "      <td>0.726377</td>\n",
       "      <td>0.725901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158457</td>\n",
       "      <td>-2.052863</td>\n",
       "      <td>1.805412</td>\n",
       "      <td>-0.554159</td>\n",
       "      <td>0.804506</td>\n",
       "      <td>-0.508005</td>\n",
       "      <td>1.574447</td>\n",
       "      <td>0.320776</td>\n",
       "      <td>1.115174</td>\n",
       "      <td>0.247685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>1.543295</td>\n",
       "      <td>1.317021</td>\n",
       "      <td>0.182978</td>\n",
       "      <td>0.845025</td>\n",
       "      <td>-1.022010</td>\n",
       "      <td>0.568679</td>\n",
       "      <td>-0.468127</td>\n",
       "      <td>-0.546817</td>\n",
       "      <td>0.296442</td>\n",
       "      <td>-0.594154</td>\n",
       "      <td>...</td>\n",
       "      <td>1.055686</td>\n",
       "      <td>0.559893</td>\n",
       "      <td>0.301968</td>\n",
       "      <td>1.103402</td>\n",
       "      <td>0.181240</td>\n",
       "      <td>0.149441</td>\n",
       "      <td>-0.629664</td>\n",
       "      <td>2.065989</td>\n",
       "      <td>0.674237</td>\n",
       "      <td>0.114477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highly</th>\n",
       "      <td>1.252463</td>\n",
       "      <td>0.928283</td>\n",
       "      <td>1.200516</td>\n",
       "      <td>1.715633</td>\n",
       "      <td>0.731993</td>\n",
       "      <td>-0.560776</td>\n",
       "      <td>-0.029089</td>\n",
       "      <td>-1.269934</td>\n",
       "      <td>-0.922150</td>\n",
       "      <td>1.299376</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.589796</td>\n",
       "      <td>-0.256082</td>\n",
       "      <td>0.360092</td>\n",
       "      <td>-0.159094</td>\n",
       "      <td>-0.911083</td>\n",
       "      <td>0.386350</td>\n",
       "      <td>0.671055</td>\n",
       "      <td>0.053867</td>\n",
       "      <td>-1.239125</td>\n",
       "      <td>0.857775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>respectable</th>\n",
       "      <td>0.003720</td>\n",
       "      <td>0.983251</td>\n",
       "      <td>1.244925</td>\n",
       "      <td>3.105837</td>\n",
       "      <td>0.996816</td>\n",
       "      <td>1.435607</td>\n",
       "      <td>-0.811739</td>\n",
       "      <td>-0.035436</td>\n",
       "      <td>1.294353</td>\n",
       "      <td>1.087980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231004</td>\n",
       "      <td>-0.232299</td>\n",
       "      <td>1.325414</td>\n",
       "      <td>-0.416263</td>\n",
       "      <td>0.152914</td>\n",
       "      <td>-0.136545</td>\n",
       "      <td>1.247855</td>\n",
       "      <td>0.871293</td>\n",
       "      <td>-0.753128</td>\n",
       "      <td>0.443237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4         5   \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "<UNK>       -0.503153  1.122852 -0.589110 -1.338089  0.537808 -0.930078   \n",
       "nice        -1.100964  0.893685  0.193806 -1.329189  1.789114 -0.522488   \n",
       "great       -2.082712  0.616099 -0.777128  1.908266  0.761909  1.869433   \n",
       "best        -0.392523  0.098091 -2.461212  1.324039  0.626163  0.212848   \n",
       "amazing     -1.052736  0.373408 -0.122459 -1.373757 -0.713679 -0.755578   \n",
       "stop        -0.497697 -0.235544 -0.375438  1.334417 -1.592178 -1.880756   \n",
       "lies         0.024769 -0.456872 -0.664279  1.604704  0.140491  0.072229   \n",
       "pitiful      0.877789  1.218326 -0.743894 -0.279423  0.816912 -1.105900   \n",
       "nerd        -1.140655 -0.458999 -2.165972  1.278016 -1.035399 -1.723518   \n",
       "excellent    0.173515  1.234430  1.410012  0.114707 -0.236008 -2.808406   \n",
       "work         0.746995  0.566075  0.007253  0.227592 -0.292766 -1.246617   \n",
       "supreme     -0.019000  0.428264 -0.053109 -0.900704  0.521697  0.155414   \n",
       "quality      0.184762  0.986030 -0.092353  0.233340 -1.905763 -0.162851   \n",
       "bad          1.543295  1.317021  0.182978  0.845025 -1.022010  0.568679   \n",
       "highly       1.252463  0.928283  1.200516  1.715633  0.731993 -0.560776   \n",
       "respectable  0.003720  0.983251  1.244925  3.105837  0.996816  1.435607   \n",
       "\n",
       "                   6         7         8         9   ...        90        91  \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "<UNK>        0.174860 -0.660207  0.922226  2.106441  ... -0.911769  0.122990   \n",
       "nice         0.262018  0.498728 -1.204775  0.857129  ...  0.413068  1.536693   \n",
       "great        0.292291 -0.936683 -0.555840  2.168294  ... -0.371885  1.953568   \n",
       "best        -0.234914 -0.497381  0.345082 -1.305001  ...  1.404677  1.085973   \n",
       "amazing     -0.649324  0.685076 -1.486379  0.652252  ...  0.893645  0.573710   \n",
       "stop        -0.395881  2.018922 -0.983122  0.782248  ... -0.044324 -1.229873   \n",
       "lies        -0.196021  2.606885  2.345440 -0.094659  ...  0.120991 -0.098169   \n",
       "pitiful     -0.822485  0.553456  2.524921  1.436808  ...  0.746145 -1.203568   \n",
       "nerd        -0.620779 -0.368372  0.688099 -0.453923  ... -0.656560 -0.213419   \n",
       "excellent   -0.130736 -0.156313  0.672824  1.452021  ... -1.400905  0.251868   \n",
       "work         2.596814 -0.433214 -0.152662  0.848396  ...  0.766436  0.180232   \n",
       "supreme     -1.034958  0.237269  0.967571  1.935323  ... -1.113440  0.212698   \n",
       "quality     -0.900567  0.008757  0.726377  0.725901  ...  0.158457 -2.052863   \n",
       "bad         -0.468127 -0.546817  0.296442 -0.594154  ...  1.055686  0.559893   \n",
       "highly      -0.029089 -1.269934 -0.922150  1.299376  ... -0.589796 -0.256082   \n",
       "respectable -0.811739 -0.035436  1.294353  1.087980  ...  0.231004 -0.232299   \n",
       "\n",
       "                   92        93        94        95        96        97  \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "<UNK>        0.896133  0.324314 -0.060996  0.750533  0.698772 -0.087897   \n",
       "nice         1.346791  0.896332 -0.065012 -1.482238 -1.226632  0.135369   \n",
       "great        0.183500 -0.158066  0.563312 -0.492190 -0.465123 -0.958200   \n",
       "best         1.415279 -0.354888  0.088245 -0.147179  1.300656  1.109785   \n",
       "amazing      0.211526 -0.341499 -1.023671  1.033080 -0.349468  0.492900   \n",
       "stop         0.401176 -0.847899  0.147723 -0.908863 -1.022872  0.476816   \n",
       "lies        -0.863077 -0.096738 -0.500837  0.618213  0.010688 -1.475451   \n",
       "pitiful      0.519214 -0.210401 -0.593928  1.157350  1.680863  1.440359   \n",
       "nerd         0.089783  0.813107  1.426436  1.674970 -0.456080 -1.502486   \n",
       "excellent   -1.616018  1.222195  0.213714  0.853370  0.273833  0.290795   \n",
       "work         0.817073 -0.289101 -0.826896  0.191386 -1.296251  1.203728   \n",
       "supreme      0.489131  1.698885  0.156688  2.001115 -3.104030  1.069025   \n",
       "quality      1.805412 -0.554159  0.804506 -0.508005  1.574447  0.320776   \n",
       "bad          0.301968  1.103402  0.181240  0.149441 -0.629664  2.065989   \n",
       "highly       0.360092 -0.159094 -0.911083  0.386350  0.671055  0.053867   \n",
       "respectable  1.325414 -0.416263  0.152914 -0.136545  1.247855  0.871293   \n",
       "\n",
       "                   98        99  \n",
       "<PAD>        0.000000  0.000000  \n",
       "<UNK>        0.447661 -0.242309  \n",
       "nice         1.297657 -0.205065  \n",
       "great        0.576206  1.370161  \n",
       "best        -1.246710  0.649416  \n",
       "amazing     -0.180742  0.708267  \n",
       "stop         1.072323 -1.172453  \n",
       "lies         0.053425 -1.066209  \n",
       "pitiful      0.477093  0.506625  \n",
       "nerd        -1.498011 -0.500809  \n",
       "excellent   -0.348189 -1.515442  \n",
       "work         0.078206 -1.352532  \n",
       "supreme      0.926571  0.831254  \n",
       "quality      1.115174  0.247685  \n",
       "bad          0.674237  0.114477  \n",
       "highly      -1.239125  0.857775  \n",
       "respectable -0.753128  0.443237  \n",
       "\n",
       "[17 rows x 100 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 임베딩 가중치 확인 : 학습 전/후 Embedding 테이블과 단어별 벡터 조회\n",
    "import pandas as pd\n",
    "\n",
    "# 학습 전 임베딩 벡터\n",
    "wv = model.embedding.weight.data  # Embedding 층의 가중치 행렬(단어ID x 임베딩 차원) 추출\n",
    "print(wv.shape)  # (vocab_size, embedding_dim)\n",
    "\n",
    "# 특정 단어 벡터\n",
    "vocab = word_to_index.keys()    # 단어사전에서 단어만 뽑아온다.\n",
    "pd.DataFrame(wv, index=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502ef96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch 학습 준비 : 텐서 변환 -> DataLoader 구성 -> 손실함수/옵티마이저 설정\n",
    "X = torch.tensor(padded_sequences, dtype=torch.long)      # 입력 시퀀스(정수 ID)를 LongTensor로 변환\n",
    "y = torch.tensor(labels, dtype=torch.float).unsqueeze(1)  # 라벨을 float으로 변환 후 (N,) -> (N, 1)로 차원 맞춤\n",
    "\n",
    "dataset = TensorDataset(X, y)  # (X, y) 쌍을 Dataset 객체로 묶음\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)  # 배치 단위로 섞어서 공급하는 로더\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # 출력 logit과 정답(0/1)로 이진분류 손실 계산 (시그모이드 포함)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)  # 모델 파라미터는 Adam으로 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9959cf7",
   "metadata": {},
   "source": [
    "BCEWithLogitsLoss을 사용할 때에는 모델 출력이 Sigmoid를 거치지 않은 logit이어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba4ef68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss 0.7636233568191528\n",
      "Epoch 2: Loss 0.6141544878482819\n",
      "Epoch 3: Loss 0.5330058261752129\n",
      "Epoch 4: Loss 0.4667740762233734\n",
      "Epoch 5: Loss 0.38759031891822815\n",
      "Epoch 6: Loss 0.31159811466932297\n",
      "Epoch 7: Loss 0.2405405081808567\n",
      "Epoch 8: Loss 0.17771075293421745\n",
      "Epoch 9: Loss 0.13561180606484413\n",
      "Epoch 10: Loss 0.11030002310872078\n",
      "Epoch 11: Loss 0.08654980733990669\n",
      "Epoch 12: Loss 0.06717688962817192\n",
      "Epoch 13: Loss 0.054683173075318336\n",
      "Epoch 14: Loss 0.04753232840448618\n",
      "Epoch 15: Loss 0.04142040014266968\n",
      "Epoch 16: Loss 0.033912552054971457\n",
      "Epoch 17: Loss 0.02996383048593998\n",
      "Epoch 18: Loss 0.026607134845107794\n",
      "Epoch 19: Loss 0.024000450037419796\n",
      "Epoch 20: Loss 0.022583614569157362\n"
     ]
    }
   ],
   "source": [
    "# 학습 루프 : 미니배치 단위로 20 epoch 학습하며 평균 손실 출력\n",
    "for epoch in range(20):\n",
    "    epoch_loss = 0    # 손실 누적\n",
    "\n",
    "    for x_batch, y_batch in dataloader:    # 미니배치 단위로 (X, y) 가져오기\n",
    "        optimizer.zero_grad()              # 이전 배치 기울기 초기화\n",
    "        output = model(x_batch)            # 순전파로 logit 계산\n",
    "        loss = criterion(output, y_batch)  # 예측 logit과 정답으로 손실 계산\n",
    "        loss.backward()                    # 역전파로 기울기 계산\n",
    "        optimizer.step()                   # 파라미터 업데이트\n",
    "\n",
    "        epoch_loss += loss.item()  # 배치손실을 float으로 누적\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: Loss {epoch_loss / len(dataloader)}\")  # epoch별 평균 손실 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9697d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 1, 0, 1]\n",
      "[1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# 평가 / 예측 : 학습된 모델로 확률 -> 0/1 예측값 생성 후 정답과 비교\n",
    "model.eval()                        # 평가 모드\n",
    "with torch.no_grad():               # 기울기 계산 비활성화\n",
    "    output = model(X)               # 전체 샘플에 대한 예측 logit 계산\n",
    "    prob = torch.sigmoid(output)    # logit에 0~1 확률로 변환\n",
    "    pred = (prob >= 0.5).int()      # 임계값 0.5 기준으로 이진 분류(0/1) 예측값 생성\n",
    "\n",
    "print(labels)\n",
    "print(pred.squeeze().detach().numpy())  # 예측라벨을 1차원 numpy 배열로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcdba32",
   "metadata": {},
   "source": [
    "## 사전학습된 임베딩을 사용하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cad4a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, 300)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_wv = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "model_wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ea8192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "(17, 300)\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 매트릭스 초기화 : 사전학습 벡터로 Embedding 레이어를 채우기 위한 준비\n",
    "print(len(word_to_index))  # 어휘 크기 (vocab_size) 확인\n",
    "\n",
    "# (vocab_size, embedding_dim) 크기의 0 행렬 생성\n",
    "embedding_matrix = np.zeros((len(word_to_index), model_wv.vectors.shape[1]))\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8398eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전학습 임베딩 매핑 : 내 단어사전을 GoogleNews 벡터로 채워 embedding_matrix 구성\n",
    "# model_wv.key_to_index['bad']  # 'bad'의 내부 인덱스 확인 (706)\n",
    "# model_wv.vectors[240]         # 특정 인덱스 벡터 직접 조회\n",
    "\n",
    "# 단어가 사전학습 모델에 있으면 임베딩 벡터(np.ndarray)를 반환, 없으면 None 반환\n",
    "def get_word_embedding(word):\n",
    "    if word in model_wv:          # 사전학습 단어가 존재하면\n",
    "        return model_wv[word]     # 해당 단어 임베딩 벡터 반환\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# get_word_embedding('bad')\n",
    "\n",
    "for word, index in word_to_index.items():  # 내 단어사전(단어-> 인덱스)를 순회\n",
    "    if index >= 2:                         # 특수토큰 제외\n",
    "        emb = get_word_embedding(word)     # 사전학습 임베딩에서 해당 단어 벡터 조회\n",
    "        if emb is not None:                # 벡터가 존재하면\n",
    "            embedding_matrix[index] = emb  # 내 인덱스 위치에 사전학습 벡터를 복사해서 채운다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33b2869e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;PAD&gt;</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;UNK&gt;</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nice</th>\n",
       "      <td>0.158203</td>\n",
       "      <td>0.105957</td>\n",
       "      <td>-0.189453</td>\n",
       "      <td>0.386719</td>\n",
       "      <td>0.083496</td>\n",
       "      <td>-0.267578</td>\n",
       "      <td>0.083496</td>\n",
       "      <td>0.113281</td>\n",
       "      <td>-0.104004</td>\n",
       "      <td>0.178711</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085449</td>\n",
       "      <td>0.189453</td>\n",
       "      <td>-0.146484</td>\n",
       "      <td>0.134766</td>\n",
       "      <td>-0.040771</td>\n",
       "      <td>0.032715</td>\n",
       "      <td>0.089355</td>\n",
       "      <td>-0.267578</td>\n",
       "      <td>0.008362</td>\n",
       "      <td>-0.213867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>0.071777</td>\n",
       "      <td>0.208008</td>\n",
       "      <td>-0.028442</td>\n",
       "      <td>0.178711</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>-0.099609</td>\n",
       "      <td>0.096191</td>\n",
       "      <td>-0.116699</td>\n",
       "      <td>-0.008545</td>\n",
       "      <td>0.148438</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011475</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>-0.289062</td>\n",
       "      <td>-0.048096</td>\n",
       "      <td>-0.199219</td>\n",
       "      <td>-0.071289</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>-0.167969</td>\n",
       "      <td>-0.020874</td>\n",
       "      <td>-0.142578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>-0.126953</td>\n",
       "      <td>0.021973</td>\n",
       "      <td>0.287109</td>\n",
       "      <td>0.153320</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>0.032715</td>\n",
       "      <td>-0.115723</td>\n",
       "      <td>-0.029541</td>\n",
       "      <td>0.153320</td>\n",
       "      <td>0.011292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006439</td>\n",
       "      <td>-0.033936</td>\n",
       "      <td>-0.166016</td>\n",
       "      <td>-0.016846</td>\n",
       "      <td>-0.048584</td>\n",
       "      <td>-0.022827</td>\n",
       "      <td>-0.152344</td>\n",
       "      <td>-0.101562</td>\n",
       "      <td>-0.090332</td>\n",
       "      <td>0.088379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>0.073730</td>\n",
       "      <td>0.004059</td>\n",
       "      <td>-0.135742</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>0.180664</td>\n",
       "      <td>-0.046631</td>\n",
       "      <td>0.224609</td>\n",
       "      <td>-0.229492</td>\n",
       "      <td>-0.040039</td>\n",
       "      <td>0.225586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018433</td>\n",
       "      <td>-0.021240</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.020142</td>\n",
       "      <td>-0.310547</td>\n",
       "      <td>-0.207031</td>\n",
       "      <td>-0.006317</td>\n",
       "      <td>-0.141602</td>\n",
       "      <td>-0.150391</td>\n",
       "      <td>-0.137695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop</th>\n",
       "      <td>-0.057861</td>\n",
       "      <td>0.013184</td>\n",
       "      <td>0.115234</td>\n",
       "      <td>0.069824</td>\n",
       "      <td>-0.306641</td>\n",
       "      <td>-0.044678</td>\n",
       "      <td>0.048584</td>\n",
       "      <td>0.152344</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>-0.100098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100098</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>-0.113281</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>-0.115723</td>\n",
       "      <td>0.048096</td>\n",
       "      <td>-0.004822</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>0.029907</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lies</th>\n",
       "      <td>0.149414</td>\n",
       "      <td>-0.012817</td>\n",
       "      <td>0.328125</td>\n",
       "      <td>0.025513</td>\n",
       "      <td>0.017334</td>\n",
       "      <td>0.190430</td>\n",
       "      <td>0.188477</td>\n",
       "      <td>-0.143555</td>\n",
       "      <td>-0.090820</td>\n",
       "      <td>0.206055</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.308594</td>\n",
       "      <td>0.183594</td>\n",
       "      <td>-0.202148</td>\n",
       "      <td>0.031494</td>\n",
       "      <td>-0.164062</td>\n",
       "      <td>-0.201172</td>\n",
       "      <td>0.080078</td>\n",
       "      <td>-0.105469</td>\n",
       "      <td>0.149414</td>\n",
       "      <td>0.157227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pitiful</th>\n",
       "      <td>0.269531</td>\n",
       "      <td>0.253906</td>\n",
       "      <td>-0.020996</td>\n",
       "      <td>0.060303</td>\n",
       "      <td>-0.010925</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>0.139648</td>\n",
       "      <td>-0.057617</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.253906</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063477</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>-0.094238</td>\n",
       "      <td>0.089355</td>\n",
       "      <td>-0.065430</td>\n",
       "      <td>-0.016235</td>\n",
       "      <td>-0.107910</td>\n",
       "      <td>-0.072266</td>\n",
       "      <td>-0.094238</td>\n",
       "      <td>0.028809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nerd</th>\n",
       "      <td>0.265625</td>\n",
       "      <td>-0.207031</td>\n",
       "      <td>-0.026611</td>\n",
       "      <td>0.419922</td>\n",
       "      <td>-0.208984</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.063965</td>\n",
       "      <td>0.149414</td>\n",
       "      <td>-0.017700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215820</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>-0.227539</td>\n",
       "      <td>-0.310547</td>\n",
       "      <td>-0.112793</td>\n",
       "      <td>-0.096680</td>\n",
       "      <td>0.255859</td>\n",
       "      <td>0.124023</td>\n",
       "      <td>-0.030273</td>\n",
       "      <td>0.082031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excellent</th>\n",
       "      <td>-0.212891</td>\n",
       "      <td>-0.004303</td>\n",
       "      <td>-0.180664</td>\n",
       "      <td>-0.007568</td>\n",
       "      <td>0.112793</td>\n",
       "      <td>0.163086</td>\n",
       "      <td>-0.014709</td>\n",
       "      <td>-0.078613</td>\n",
       "      <td>-0.164062</td>\n",
       "      <td>0.279297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>-0.173828</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>-0.081055</td>\n",
       "      <td>0.013550</td>\n",
       "      <td>-0.008362</td>\n",
       "      <td>-0.129883</td>\n",
       "      <td>-0.215820</td>\n",
       "      <td>0.012268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>-0.075684</td>\n",
       "      <td>0.033691</td>\n",
       "      <td>-0.064941</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>0.050537</td>\n",
       "      <td>0.149414</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>-0.133789</td>\n",
       "      <td>-0.020874</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.101562</td>\n",
       "      <td>-0.091309</td>\n",
       "      <td>0.052246</td>\n",
       "      <td>-0.164062</td>\n",
       "      <td>0.121582</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.012024</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>-0.091309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>supreme</th>\n",
       "      <td>0.173828</td>\n",
       "      <td>0.172852</td>\n",
       "      <td>0.112793</td>\n",
       "      <td>0.166016</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>-0.014221</td>\n",
       "      <td>0.128906</td>\n",
       "      <td>-0.217773</td>\n",
       "      <td>0.073730</td>\n",
       "      <td>0.205078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>-0.221680</td>\n",
       "      <td>-0.055664</td>\n",
       "      <td>0.034424</td>\n",
       "      <td>-0.119629</td>\n",
       "      <td>-0.081543</td>\n",
       "      <td>0.121094</td>\n",
       "      <td>-0.164062</td>\n",
       "      <td>-0.127930</td>\n",
       "      <td>0.097656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>-0.259766</td>\n",
       "      <td>0.271484</td>\n",
       "      <td>0.119629</td>\n",
       "      <td>0.007233</td>\n",
       "      <td>0.057373</td>\n",
       "      <td>0.113770</td>\n",
       "      <td>0.166992</td>\n",
       "      <td>0.025024</td>\n",
       "      <td>0.067871</td>\n",
       "      <td>0.120117</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145508</td>\n",
       "      <td>0.120117</td>\n",
       "      <td>-0.314453</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>-0.010254</td>\n",
       "      <td>0.298828</td>\n",
       "      <td>0.046387</td>\n",
       "      <td>-0.179688</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>-0.014099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>0.062988</td>\n",
       "      <td>0.124512</td>\n",
       "      <td>0.113281</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.038818</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.096191</td>\n",
       "      <td>0.220703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011353</td>\n",
       "      <td>0.341797</td>\n",
       "      <td>-0.090332</td>\n",
       "      <td>0.076660</td>\n",
       "      <td>-0.032471</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>-0.154297</td>\n",
       "      <td>-0.063477</td>\n",
       "      <td>0.114746</td>\n",
       "      <td>0.031006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highly</th>\n",
       "      <td>0.050781</td>\n",
       "      <td>-0.227539</td>\n",
       "      <td>-0.130859</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>-0.165039</td>\n",
       "      <td>0.118652</td>\n",
       "      <td>-0.230469</td>\n",
       "      <td>-0.225586</td>\n",
       "      <td>0.245117</td>\n",
       "      <td>-0.086914</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013733</td>\n",
       "      <td>-0.013489</td>\n",
       "      <td>0.175781</td>\n",
       "      <td>0.226562</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>-0.210938</td>\n",
       "      <td>-0.111816</td>\n",
       "      <td>-0.056641</td>\n",
       "      <td>0.102539</td>\n",
       "      <td>0.074707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>respectable</th>\n",
       "      <td>0.150391</td>\n",
       "      <td>0.285156</td>\n",
       "      <td>-0.023193</td>\n",
       "      <td>0.095703</td>\n",
       "      <td>-0.022095</td>\n",
       "      <td>0.058350</td>\n",
       "      <td>-0.155273</td>\n",
       "      <td>-0.051025</td>\n",
       "      <td>0.100098</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058838</td>\n",
       "      <td>0.056885</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.138672</td>\n",
       "      <td>-0.227539</td>\n",
       "      <td>0.183594</td>\n",
       "      <td>-0.033447</td>\n",
       "      <td>-0.200195</td>\n",
       "      <td>-0.112305</td>\n",
       "      <td>0.103027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0         1         2         3         4         5    \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "<UNK>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "nice         0.158203  0.105957 -0.189453  0.386719  0.083496 -0.267578   \n",
       "great        0.071777  0.208008 -0.028442  0.178711  0.132812 -0.099609   \n",
       "best        -0.126953  0.021973  0.287109  0.153320  0.127930  0.032715   \n",
       "amazing      0.073730  0.004059 -0.135742  0.022095  0.180664 -0.046631   \n",
       "stop        -0.057861  0.013184  0.115234  0.069824 -0.306641 -0.044678   \n",
       "lies         0.149414 -0.012817  0.328125  0.025513  0.017334  0.190430   \n",
       "pitiful      0.269531  0.253906 -0.020996  0.060303 -0.010925  0.217773   \n",
       "nerd         0.265625 -0.207031 -0.026611  0.419922 -0.208984  0.390625   \n",
       "excellent   -0.212891 -0.004303 -0.180664 -0.007568  0.112793  0.163086   \n",
       "work        -0.075684  0.033691 -0.064941  0.131836  0.050537  0.149414   \n",
       "supreme      0.173828  0.172852  0.112793  0.166016  0.058594 -0.014221   \n",
       "quality     -0.259766  0.271484  0.119629  0.007233  0.057373  0.113770   \n",
       "bad          0.062988  0.124512  0.113281  0.073242  0.038818  0.079102   \n",
       "highly       0.050781 -0.227539 -0.130859  0.062500 -0.165039  0.118652   \n",
       "respectable  0.150391  0.285156 -0.023193  0.095703 -0.022095  0.058350   \n",
       "\n",
       "                  6         7         8         9    ...       290       291  \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "<UNK>        0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "nice         0.083496  0.113281 -0.104004  0.178711  ... -0.085449  0.189453   \n",
       "great        0.096191 -0.116699 -0.008545  0.148438  ... -0.011475  0.064453   \n",
       "best        -0.115723 -0.029541  0.153320  0.011292  ...  0.006439 -0.033936   \n",
       "amazing      0.224609 -0.229492 -0.040039  0.225586  ...  0.018433 -0.021240   \n",
       "stop         0.048584  0.152344  0.073242 -0.100098  ...  0.100098  0.171875   \n",
       "lies         0.188477 -0.143555 -0.090820  0.206055  ... -0.308594  0.183594   \n",
       "pitiful      0.139648 -0.057617  0.312500  0.253906  ... -0.063477  0.132812   \n",
       "nerd         0.164062  0.063965  0.149414 -0.017700  ...  0.215820  0.125000   \n",
       "excellent   -0.014709 -0.078613 -0.164062  0.279297  ...  0.136719  0.000282   \n",
       "work         0.109375 -0.133789 -0.020874  0.054688  ... -0.187500  0.101562   \n",
       "supreme      0.128906 -0.217773  0.073730  0.205078  ...  0.140625 -0.221680   \n",
       "quality      0.166992  0.025024  0.067871  0.120117  ... -0.145508  0.120117   \n",
       "bad          0.050781  0.171875  0.096191  0.220703  ...  0.011353  0.341797   \n",
       "highly      -0.230469 -0.225586  0.245117 -0.086914  ... -0.013733 -0.013489   \n",
       "respectable -0.155273 -0.051025  0.100098  0.001183  ...  0.058838  0.056885   \n",
       "\n",
       "                  292       293       294       295       296       297  \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "<UNK>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "nice        -0.146484  0.134766 -0.040771  0.032715  0.089355 -0.267578   \n",
       "great       -0.289062 -0.048096 -0.199219 -0.071289  0.064453 -0.167969   \n",
       "best        -0.166016 -0.016846 -0.048584 -0.022827 -0.152344 -0.101562   \n",
       "amazing     -0.250000 -0.020142 -0.310547 -0.207031 -0.006317 -0.141602   \n",
       "stop        -0.113281  0.064453 -0.115723  0.048096 -0.004822  0.086426   \n",
       "lies        -0.202148  0.031494 -0.164062 -0.201172  0.080078 -0.105469   \n",
       "pitiful     -0.094238  0.089355 -0.065430 -0.016235 -0.107910 -0.072266   \n",
       "nerd        -0.227539 -0.310547 -0.112793 -0.096680  0.255859  0.124023   \n",
       "excellent   -0.173828  0.004242 -0.081055  0.013550 -0.008362 -0.129883   \n",
       "work        -0.091309  0.052246 -0.164062  0.121582  0.062500  0.012024   \n",
       "supreme     -0.055664  0.034424 -0.119629 -0.081543  0.121094 -0.164062   \n",
       "quality     -0.314453  0.022095 -0.010254  0.298828  0.046387 -0.179688   \n",
       "bad         -0.090332  0.076660 -0.032471  0.133789 -0.154297 -0.063477   \n",
       "highly       0.175781  0.226562  0.086914 -0.210938 -0.111816 -0.056641   \n",
       "respectable -0.187500  0.138672 -0.227539  0.183594 -0.033447 -0.200195   \n",
       "\n",
       "                  298       299  \n",
       "<PAD>        0.000000  0.000000  \n",
       "<UNK>        0.000000  0.000000  \n",
       "nice         0.008362 -0.213867  \n",
       "great       -0.020874 -0.142578  \n",
       "best        -0.090332  0.088379  \n",
       "amazing     -0.150391 -0.137695  \n",
       "stop         0.029907  0.007812  \n",
       "lies         0.149414  0.157227  \n",
       "pitiful     -0.094238  0.028809  \n",
       "nerd        -0.030273  0.082031  \n",
       "excellent   -0.215820  0.012268  \n",
       "work         0.135742 -0.091309  \n",
       "supreme     -0.127930  0.097656  \n",
       "quality      0.000706 -0.014099  \n",
       "bad          0.114746  0.031006  \n",
       "highly       0.102539  0.074707  \n",
       "respectable -0.112305  0.103027  \n",
       "\n",
       "[17 rows x 300 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(embedding_matrix, index=word_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c0eff5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet(\n",
      "  (embedding): Embedding(17, 300, padding_idx=0)\n",
      "  (rnn): RNN(300, 16, batch_first=True)\n",
      "  (out): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Pytorch 텍스트 분류 모델 : Embedding + RNN + Linear로 이진 분류(logit) 출력\n",
    "import torch\n",
    "import torch.nn as nn           # 신경망 레이어\n",
    "import torch.optim as optim     # 옵티마이저(활성화함수)\n",
    "from torch.utils.data import DataLoader, TensorDataset  # 배치 로더 / 데이터셋 유틸\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    # 정수 시퀀스를 임베딩 -> RNN -> 선형층으로 처리해 이진 분류 logit(1개)를 출력\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super().__init__()                    # nn.Module 초기화\n",
    "        self.embedding = nn.Embedding(        # 단어 ID를 밀집 벡터로 변환하는 임베딩 층\n",
    "            num_embeddings = vocab_size,      # 단어 사전 크기 (어휘 수)\n",
    "            embedding_dim = embedding_dim,    # 임베딩 차원\n",
    "            padding_idx = 0                   # PAD(0) 인덱스는 0 그대로 사용\n",
    "        )\n",
    "\n",
    "        # 사전학습된 임베딩 벡터로 초기화 : Embedding 가중치를 사전학습 행렬로 덮어쓰기\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float))\n",
    "\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)    # 입력(배치, 길이, 차원) 형태의 RNN\n",
    "        self.out = nn.Linear(hidden_size, 1)     # 마지막 은닉 상태를 1차원 logit으로 변환\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)     # (batch, seq_len) -> (batch, seq_len, embedding_dim)\n",
    "        out, h_n = self.rnn(embedded)    # h_n: (num_layers*directions, batch, hidden_size)\n",
    "        out = self.out(h_n.squeeze(0))   # (batch_size, hidden_size) -> (batch, 1)\n",
    "        return out  # 출력 : 시그모이드 전 logit(확률이 아님)\n",
    "    \n",
    "embedding_dim = model_wv.vectors.shape[1]  # 사전학습 임베딩 차원 (300)으로 임베딩 차원 설정\n",
    "model = SimpleNet(vocab_size, embedding_dim, hidden_size=16)  # 어휘 크기 / 임베딩 차원/ 은닉크기로 모델 생성\n",
    "print(model)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # 출력 logit과 정답(0/1)로 이진분류 손실 계산 (시그모이드 포함)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)  # 모델 파라미터는 Adam으로 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "837c50ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss 0.6975708156824112\n",
      "Epoch 2: Loss 0.5997142046689987\n",
      "Epoch 3: Loss 0.4891001582145691\n",
      "Epoch 4: Loss 0.38117462396621704\n",
      "Epoch 5: Loss 0.2806808315217495\n",
      "Epoch 6: Loss 0.20620886608958244\n",
      "Epoch 7: Loss 0.15090016275644302\n",
      "Epoch 8: Loss 0.1125807985663414\n",
      "Epoch 9: Loss 0.0887848399579525\n",
      "Epoch 10: Loss 0.06743127293884754\n",
      "Epoch 11: Loss 0.053572106175124645\n",
      "Epoch 12: Loss 0.04420152306556702\n",
      "Epoch 13: Loss 0.03629125561565161\n",
      "Epoch 14: Loss 0.03122431505471468\n",
      "Epoch 15: Loss 0.026289566420018673\n",
      "Epoch 16: Loss 0.02287012478336692\n",
      "Epoch 17: Loss 0.020185789559036493\n",
      "Epoch 18: Loss 0.018032597843557596\n",
      "Epoch 19: Loss 0.016256834845989943\n",
      "Epoch 20: Loss 0.014796118019148707\n"
     ]
    }
   ],
   "source": [
    "# 학습 루프 : 미니배치 단위로 20 epoch 학습하며 평균 손실 출력\n",
    "for epoch in range(20):\n",
    "    epoch_loss = 0    # 손실 누적\n",
    "\n",
    "    for x_batch, y_batch in dataloader:    # 미니배치 단위로 (X, y) 가져오기\n",
    "        optimizer.zero_grad()              # 이전 배치 기울기 초기화\n",
    "        output = model(x_batch)            # 순전파로 logit 계산\n",
    "        loss = criterion(output, y_batch)  # 예측 logit과 정답으로 손실 계산\n",
    "        loss.backward()                    # 역전파로 기울기 계산\n",
    "        optimizer.step()                   # 파라미터 업데이트\n",
    "\n",
    "        epoch_loss += loss.item()  # 배치손실을 float으로 누적\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: Loss {epoch_loss / len(dataloader)}\")  # epoch별 평균 손실 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a692a897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 1, 0, 1]\n",
      "[1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# 평가 / 예측 : 학습된 모델로 확률 -> 0/1 예측값 생성 후 정답과 비교\n",
    "model.eval()                        # 평가 모드\n",
    "with torch.no_grad():               # 기울기 계산 비활성화\n",
    "    output = model(X)               # 전체 샘플에 대한 예측 logit 계산\n",
    "    prob = torch.sigmoid(output)    # logit에 0~1 확률로 변환\n",
    "    pred = (prob >= 0.5).int()      # 임계값 0.5 기준으로 이진 분류(0/1) 예측값 생성\n",
    "\n",
    "print(labels)\n",
    "print(pred.squeeze().detach().numpy())  # 예측라벨을 1차원 numpy 배열로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aeabe0",
   "metadata": {},
   "source": [
    "사전학습 임베딩을 사용했을 때에도 학습 데이터 분류가 잘 되는지 파악한다.  \n",
    "만약 틀린 샘플이 있다면 해당 문장이 OOV(0벡터) 비중이 큰지 확인해봐야 한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
