{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74b37905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "코사인 유사도: 0.9925833339709303\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# 임의의 단어 벡터\n",
    "word1 = np.array([[1, 2, 3]])\n",
    "word2 = np.array([[2, 3, 4]])\n",
    "\n",
    "# 코사인 유사도 계산\n",
    "similarity = cosine_similarity(word1, word2)\n",
    "print(\"코사인 유사도:\", similarity[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78bf4c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소 분석 결과: ['자연어', '처리', '는', '재미있다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 형태소 벡터화\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 텍스트 데이터\n",
    "text = \"자연어 처리는 재미있다.\"\n",
    "\n",
    "# 형태소 분석\n",
    "okt = Okt()\n",
    "morphs = okt.morphs(text)    # 문장을 형태소 리스트로 분리\n",
    "print(\"형태소 분석 결과:\", morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fe9e193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW 벡터화 결과:\\n [[0 0 1 1 1]\n",
      " [1 1 0 0 0]]\n",
      "단어 사전: {'자연어': 2, '처리는': 4, '재미있다': 3, 'python으로': 0, '가능하다': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer    # BoW 도구\n",
    "\n",
    "# 문장 데이터\n",
    "corpus = [\"자연어 처리는 재미있다.\", \"Python으로 가능하다.\"]\n",
    "\n",
    "# BoW 벡터화\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)    # 단어 사전 학습 + BoW(빈도) 희소행렬 벡터로 변환\n",
    "\n",
    "print(\"BoW 벡터화 결과:\\\\n\", X.toarray())    # 희소행렬을 밀집 배열 형태로 출력\n",
    "print(\"단어 사전:\", vectorizer.vocabulary_)  # {단어 : 인덱스} 형태의 단어사전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eac009c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF 결과:\\n [[0.37997836 0.         0.53404633 0.53404633 0.53404633]\n",
      " [0.57973867 0.81480247 0.         0.         0.        ]]\n",
      "단어 사전: {'python으로': 0, '자연어': 2, '처리는': 4, '재미있다': 3, '가능하다': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 문장 데이터\n",
    "corpus = [\"Python으로 자연어 처리는 재미있다.\", \"Python으로 가능하다.\"]\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)  # IDF 학습(fit) + TF-IDF 벡터 변환 -> 희소행렬\n",
    "\n",
    "print(\"TF-IDF 결과:\\\\n\", X.toarray())  # 희소행렬 -> 밀집 배열 형태로 변환\n",
    "\n",
    "print(\"단어 사전:\", vectorizer.vocabulary_)  # {단어 : 인덱스} 형태의 단어사전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c6295fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached gensim-4.4.0-cp312-cp312-win_amd64.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from gensim) (2.4.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from gensim) (1.17.0)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
      "Using cached gensim-4.4.0-cp312-cp312-win_amd64.whl (24.4 MB)\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610a7914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 '자연어'의 벡터: [-0.00713902  0.00124103 -0.00717672 -0.00224462  0.0037193   0.00583312\n",
      "  0.00119818  0.00210273 -0.00411039  0.00722533 -0.00630704  0.00464722\n",
      " -0.00821997  0.00203647 -0.00497705 -0.00424769 -0.00310898  0.00565521\n",
      "  0.0057984  -0.00497465  0.00077333 -0.00849578  0.00780981  0.00925729\n",
      " -0.00274233  0.00080022  0.00074665  0.00547788 -0.00860608  0.00058446\n",
      "  0.00686942  0.00223159  0.00112468 -0.00932216  0.00848237 -0.00626413\n",
      " -0.00299237  0.00349379 -0.00077263  0.00141129  0.00178199 -0.0068289\n",
      " -0.00972481  0.00904058  0.00619805 -0.00691293  0.00340348  0.00020606\n",
      "  0.00475375 -0.00711994  0.00402695  0.00434743  0.00995737 -0.00447374\n",
      " -0.00138926 -0.00731732 -0.00969783 -0.00908026 -0.00102275 -0.00650329\n",
      "  0.00484973 -0.00616403  0.00251919  0.00073944 -0.00339215 -0.00097922\n",
      "  0.00997913  0.00914589 -0.00446183  0.00908303 -0.00564176  0.00593092\n",
      " -0.00309722  0.00343175  0.00301723  0.00690046 -0.00237388  0.00877504\n",
      "  0.00758943 -0.00954765 -0.00800821 -0.0076379   0.00292326 -0.00279472\n",
      " -0.00692952 -0.00812826  0.00830918  0.00199049 -0.00932802 -0.00479272\n",
      "  0.00313674 -0.00471321  0.00528084 -0.00423344  0.0026418  -0.00804569\n",
      "  0.00620989  0.00481889  0.00078719  0.00301345]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec  # 단어 임베딩 학습 모델 Word2Vec\n",
    "\n",
    "# 샘플 문장\n",
    "sentences = [[\"자연어\", \"처리\", \"재미있다\"], [\"Python\", \"가능하다\"]]\n",
    "\n",
    "# Word2Vec 모델 학습\n",
    "model = Word2Vec(\n",
    "    sentences,          # 학습에 사용할 토큰화 문장들\n",
    "    vector_size=100,    # 단어 벡터 차원 수\n",
    "    window=3,           # 주변 문맥으로 볼 단어 범위 (좌우 3개)\n",
    "    min_count=1,        # 최소 등장 횟수 (1개 이상)\n",
    "    sg=0                # 0 = CBOW (주변->중심 단어 예측), 1 = Skip-gram(중심->주변 단어 예측)\n",
    ")\n",
    "\n",
    "# 단어 벡터 확인\n",
    "print(\"단어 '자연어'의 벡터:\", model.wv[\"자연어\"])  # 학습된 '자연어'의 임베딩 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3d0c7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.17018887)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(\"자연어\", \"재미있다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59c5a23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('재미있다', 0.17018885910511017),\n",
       " ('Python', 0.004503022879362106),\n",
       " ('가능하다', -0.027750348672270775),\n",
       " ('처리', -0.04461711645126343)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"자연어\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df83fbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king 벡터 차원: (50,)\n",
      "similarity(king, queen): 0.7839043\n",
      "similarity(king, banana): 0.2207408\n",
      "most_similar('king'): [('prince', 0.8236179351806641), ('queen', 0.7839043140411377), ('ii', 0.7746230363845825), ('emperor', 0.7736247777938843), ('son', 0.766719400882721)]\n",
      "[('queen', 0.8523604273796082), ('throne', 0.7664334177970886), ('prince', 0.7592144012451172), ('daughter', 0.7473883628845215), ('elizabeth', 0.7460219860076904)]\n"
     ]
    }
   ],
   "source": [
    "# GloVe\n",
    "import gensim.downloader as api\n",
    "\n",
    "# 1) 미리 학습된 GloVe 로드 (처음 1번은 다운로드 시간이 걸림)\n",
    "# 50차원 버전: 가볍고 실습용으로 좋음\n",
    "glove = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "# 2) 단어 벡터 확인 (길이=50)\n",
    "print(\"king 벡터 차원:\", glove[\"king\"].shape)\n",
    "\n",
    "# 3) 코사인 유사도\n",
    "print(\"similarity(king, queen):\", glove.similarity(\"king\", \"queen\"))\n",
    "print(\"similarity(king, banana):\", glove.similarity(\"king\", \"banana\"))\n",
    "\n",
    "# 4) 가장 비슷한 단어 top-n\n",
    "print(\"most_similar('king'):\", glove.most_similar(\"king\", topn=5))\n",
    "\n",
    "# 5) 벡터 연산(관계) 예시: king - man + woman ≈ queen\n",
    "print(glove.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71a308ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOC_0 벡터 일부: [-0.01058207 -0.01189603 -0.01985634  0.01694183  0.00701691  0.00072557\n",
      " -0.01981407 -0.01017613 -0.01967002  0.00412226]\n",
      "\n",
      "DOC_0와 유사한 문서:\n",
      "[('DOC_1', 0.2878270447254181), ('DOC_2', 0.1307842880487442), ('DOC_3', -0.07168306410312653)]\n",
      "\n",
      "새 문장과 유사한 문서:\n",
      "[('DOC_2', -0.03598365560173988), ('DOC_0', -0.06311464309692383), ('DOC_3', -0.11277913302183151)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument    # 문서 임베딩(Doc2Vec) 모델과 태그 문서 클래스\n",
    "\n",
    "# 1) 예시 문서(문장) 준비\n",
    "docs = [\n",
    "    \"자연어 처리는 재미있다\",\n",
    "    \"파이썬으로 자연어 처리를 할 수 있다\",\n",
    "    \"축구 경기는 정말 재미있다\",\n",
    "    \"오늘 날씨가 좋다\"\n",
    "]\n",
    "\n",
    "# 2) Doc2Vec은 문서마다 태그(아이디)가 필요함\n",
    "tagged_docs = [\n",
    "    TaggedDocument(words=d.split(), tags=[f\"DOC_{i}\"])\n",
    "    for i, d in enumerate(docs)\n",
    "]  # 각 문서를 (토큰리스트, 문서 ID)로 변환\n",
    "\n",
    "# 3) Doc2Vec 모델 학습\n",
    "model = Doc2Vec(\n",
    "    documents=tagged_docs,    # 태그가 붙은 문서들로 학습\n",
    "    vector_size=50,   # 문서 벡터 차원\n",
    "    window=3,         # 문맥 범위\n",
    "    min_count=1,      # 최소 등장 횟수\n",
    "    workers=2,        # 병렬 처리 CPU 쓰레드 갯수\n",
    "    epochs=50         # 학습 반복\n",
    ")\n",
    "\n",
    "# 4) 학습된 문서 벡터 확인\n",
    "print(\"DOC_0 벡터 일부:\", model.dv[\"DOC_0\"][:10])\n",
    "\n",
    "# 5) 특정 문서와 가장 유사한 문서 찾기\n",
    "print(\"\\nDOC_0와 유사한 문서:\")\n",
    "print(model.dv.most_similar(\"DOC_0\", topn=3))\n",
    "\n",
    "# 6) 새 문장(학습에 없던 문장)도 벡터로 추정 가능(infer_vector)\n",
    "new_doc = \"자연어 처리는 파이썬으로 가능하다\"\n",
    "new_vec = model.infer_vector(new_doc.split())\n",
    "\n",
    "print(\"\\n새 문장과 유사한 문서:\")\n",
    "print(model.dv.most_similar([new_vec], topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cda21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주제별 단어 분포:\\n [[1.4913572  1.4913572  0.50945773 0.50945773 0.50945773]\n",
      " [0.5086428  0.5086428  1.49054227 1.49054227 1.49054227]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation    # LDA(토픽 모델링) 알고리즘 클래스\n",
    "from sklearn.feature_extraction.text import CountVectorizer    # 문서의 단어 등장 횟수(BoW)로 변환하는 벡터라이저\n",
    "\n",
    "# 문장 데이터\n",
    "corpus = [\"자연어 처리는 재미있다.\", \"Python으로 가능하다.\"]\n",
    "\n",
    "# BoW 벡터화\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)    # 단어 사전 학습 + 문서-단어 희소행렬 변환\n",
    "\n",
    "# LDA 모델 학습\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=2,    # 찾을 주제 갯수\n",
    "    random_state=0     # 재현성 (시드 고정)\n",
    ")\n",
    "lda.fit(X)    # BoW 행렬로 LDA 모델 학습 (주제-단어 분포/문서-주제 분포 추정)\n",
    "\n",
    "# 주제별 단어 분포\n",
    "print(\"주제별 단어 분포:\\\\n\", lda.components_)    # 각 주제에서 단어가 얼마나 중요한지(가중치)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d95d58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  [('python으로', np.float64(1.4913571964571597)), ('가능하다', np.float64(1.491357196456652)), ('자연어', np.float64(0.5094577321201083)), ('처리는', np.float64(0.5094577321189349)), ('재미있다', np.float64(0.5094577321184829))]\n",
      "Topic 1:  [('재미있다', np.float64(1.4905422678815157)), ('처리는', np.float64(1.4905422678810638)), ('자연어', np.float64(1.4905422678798905)), ('가능하다', np.float64(0.5086428035433468)), ('python으로', np.float64(0.5086428035428391))]\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()  # 단어사전에서 이름\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_idx = topic.argsort()[::-1][:5]    # 가중치가 가장 큰 단어 인덱스 TOP5 추출 (오름차순 정렬 후 역순)\n",
    "    print(f\"Topic {topic_idx}: \", [(feature_names[i], topic[i]) for i in top_idx])    # (단어, 가중치) 형태로 TOP5 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b04f39",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (143229693.py, line 7)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m]\u001b[39m\n     ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText    # 서브워드(n-그램) 기반 단어 임베딩 모델\n",
    "\n",
    "# 토큰화된 문장 준비\n",
    "sentences = [\n",
    "    [\"자연어\", \"처리는\", \"재미있다\"],\n",
    "    [\"파이썬\", \"으로\", \"자연어\", \"처리\", \"가능하다\"],\n",
    "    [\"자연어\", \"처리\", \"공부\", \"하자\"]\n",
    "]\n",
    "\n",
    "# FastText : 단어를 '문자 n-그램(서브워드)' 조합으로도 표현하여 OOV(미등장 단어) 벡터 생성에 강함\n",
    "model = FastText(\n",
    "    sentences,           # 학습 데이터\n",
    "    vector_size = 50,    # 단어 벡터 차원 수\n",
    "    window = 3,          # 주변 문맥 범위 (좌우 3개씩)\n",
    "    min_count = 1,       # 최소 등장 횟수 (1번 이상일때 사용)\n",
    "    sg = 1,              # 0 = CBOW (주변->중심 단어 예측), 1 = Skip-gram(중심->주변 단어 예측)\n",
    "    min_n = 2,           # 서브워드 (문자 n-gram) 최소 길이\n",
    "    max_n = 5,           # 서브워드 (문자 n-gram) 최대 길이\n",
    "    epochs = 50          # 학습 반복 횟수\n",
    ")\n",
    "\n",
    "# 단어 벡터 확인 ('자연어') 임베딩 벡터 중 앞 10개 차원\n",
    "print('자연어 벡터 일부 :', model.wv['자연어'][:10])\n",
    "\n",
    "# 유사단어 : 코사인 유사도가 높은 단어 TOP3\n",
    "print('자연어와 비슷한 단어 :', model.wv.most_similar(\"자연어\", topn=3))\n",
    "\n",
    "# 단어 간 코사인 유사도 확인\n",
    "print(\"similarity(자연어, 처리) :\", model.wv.similarity(\"자연어\", \"처리\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755f126d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
