{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "209f9ade",
   "metadata": {},
   "source": [
    "# fastText\n",
    "\n",
    "https://github.com/facebookresearch/fastText\n",
    "\n",
    "**FastText**는 자연어 처리(NLP) 작업에서 사용되는 오픈소스 라이브러리로, 텍스트 분류 및 단어 임베딩을 위한 빠르고 효율적인 도구이다. 이는 Facebook AI Research 팀에서 개발했으며, 특히 대규모 텍스트 데이터에서도 높은 성능과 속도를 제공한다.\n",
    "\n",
    "FastText는 2016년에 Facebook AI Research(FAIR) 팀에 의해 처음 공개되었다.  \n",
    "처음 발표된 논문은 **\"Enriching Word Vectors with Subword Information\"** (2016)이며, 이후 오픈소스로 GitHub에 공개되어 많은 사용자들에게 활용되고 있다.  \n",
    "\n",
    "FastText는 Word2Vec 이후 등장한 모델로, 특히 서브워드 정보 활용과 텍스트 분류에서 효율성을 강조하면서 주목받았다.\n",
    "\n",
    "**주요 특징**\n",
    "1. **단어 벡터 학습 (Word Embeddings)**  \n",
    "   - FastText는 단어를 고정된 크기의 벡터로 변환하는 단어 임베딩 모델을 학습한다. 이는 단어의 의미를 벡터 공간에 매핑하여 유사한 단어가 가까운 벡터로 표현되도록 한다.\n",
    "   - 기존의 Word2Vec과 유사하지만, FastText는 단어를 **서브워드(subword)** 단위로 처리한다.\n",
    "\n",
    "2. **서브워드 기반 모델 (Subword-based Model)**  \n",
    "   - 단어를 n-그램(예: 'apple' → ['app', 'ppl', 'ple'])으로 분해하여 학습하기 때문에, **희귀 단어**나 **철자 오류**에도 강건하다.\n",
    "   - 이는 단어 외에도 철자 패턴과 같은 더 세밀한 정보를 학습하는 데 유용하다.\n",
    "\n",
    "3. **텍스트 분류 (Text Classification)**  \n",
    "   - FastText는 문서나 문장을 빠르고 정확하게 분류하는 데 최적화되어 있다.\n",
    "   - 학습 과정이 빠르고, 모델의 크기가 작으며, 정확도도 뛰어나다.\n",
    "\n",
    "4. **효율적인 구현**  \n",
    "   - FastText는 CPU 기반으로도 높은 성능을 내도록 설계되었으며, 대규모 데이터셋에서도 빠르게 작동한다.\n",
    "\n",
    "**FastText의 작동 원리**\n",
    "1. **단어 표현**  \n",
    "   - 단어를 n-그램 서브워드로 나눈 후, 각 서브워드에 대해 벡터를 학습한다.\n",
    "   - 예를 들어, \"cat\"이라는 단어는 'c', 'ca', 'cat'과 같은 다양한 조합으로 분해된다.\n",
    "   - 결과적으로 단어 벡터는 각 서브워드 벡터의 합으로 표현된다.\n",
    "\n",
    "2. **모델 구조**  \n",
    "   - FastText는 Skip-gram 모델이나 CBOW 모델을 기반으로 동작한다.\n",
    "   - 단, 기존 모델과 달리 단어 자체가 아닌 서브워드를 사용하여 학습한다.\n",
    "\n",
    "**FastText의 장점**\n",
    "1. **희귀 단어 처리 능력**  \n",
    "   - 서브워드 기반 접근 방식 덕분에 희귀 단어 또는 새로운 단어에 대해 더 좋은 일반화 성능을 발휘한다.\n",
    "2. **빠른 학습 속도**  \n",
    "   - 단순한 모델 구조와 최적화된 구현으로 매우 빠르게 학습할 수 있다.\n",
    "3. **다양한 언어 지원**  \n",
    "   - 다양한 언어에서 동작하며, 특히 굴절어(inflected languages)와 같은 복잡한 언어에서도 효과적이다.\n",
    "\n",
    "**활용 사례**\n",
    "1. **단어 임베딩**  \n",
    "   - 단어 간 유사도 계산, 문장 표현 학습.\n",
    "2. **텍스트 분류**  \n",
    "   - 스팸 필터링, 감정 분석, 뉴스 분류.\n",
    "3. **다언어 지원**  \n",
    "   - 다국어 데이터셋에서 빠른 응답 성능 제공."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de27d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39751953",
   "metadata": {},
   "source": [
    "## gensim - fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59141421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1vyZuUgbejAnx8xicQnQ1HgtW6avN8ZRQ\n",
      "From (redirected): https://drive.google.com/uc?id=1vyZuUgbejAnx8xicQnQ1HgtW6avN8ZRQ&confirm=t&uuid=cc166271-7f06-4546-9fcd-0aa1dd7b5b68\n",
      "To: c:\\Users\\Playdata\\nlp\\03_text_vectorization\\ted_en-20160408.xml\n",
      "\n",
      "  0%|          | 0.00/74.5M [00:00<?, ?B/s]\n",
      "  1%|          | 524k/74.5M [00:00<00:26, 2.84MB/s]\n",
      "  3%|▎         | 2.10M/74.5M [00:00<00:08, 8.22MB/s]\n",
      "  9%|▉         | 6.82M/74.5M [00:00<00:03, 22.5MB/s]\n",
      " 14%|█▍        | 10.5M/74.5M [00:00<00:02, 26.7MB/s]\n",
      " 19%|█▉        | 14.2M/74.5M [00:00<00:02, 29.5MB/s]\n",
      " 24%|██▍       | 17.8M/74.5M [00:00<00:01, 31.2MB/s]\n",
      " 29%|██▉       | 21.5M/74.5M [00:00<00:02, 21.5MB/s]\n",
      " 34%|███▍      | 25.2M/74.5M [00:01<00:02, 24.6MB/s]\n",
      " 38%|███▊      | 28.3M/74.5M [00:01<00:01, 26.2MB/s]\n",
      " 44%|████▎     | 32.5M/74.5M [00:01<00:01, 28.8MB/s]\n",
      " 49%|████▊     | 36.2M/74.5M [00:01<00:01, 30.6MB/s]\n",
      " 53%|█████▎    | 39.8M/74.5M [00:01<00:01, 29.8MB/s]\n",
      " 58%|█████▊    | 43.5M/74.5M [00:01<00:01, 30.8MB/s]\n",
      " 63%|██████▎   | 47.2M/74.5M [00:01<00:00, 31.5MB/s]\n",
      " 68%|██████▊   | 50.9M/74.5M [00:01<00:00, 32.5MB/s]\n",
      " 73%|███████▎  | 54.5M/74.5M [00:01<00:00, 32.7MB/s]\n",
      " 78%|███████▊  | 58.2M/74.5M [00:02<00:00, 32.4MB/s]\n",
      " 83%|████████▎ | 61.9M/74.5M [00:02<00:00, 30.1MB/s]\n",
      " 88%|████████▊ | 65.5M/74.5M [00:02<00:00, 31.5MB/s]\n",
      " 93%|█████████▎| 69.2M/74.5M [00:02<00:00, 32.4MB/s]\n",
      " 98%|█████████▊| 72.9M/74.5M [00:02<00:00, 32.9MB/s]\n",
      "100%|██████████| 74.5M/74.5M [00:02<00:00, 28.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown https://drive.google.com/uc?id=1vyZuUgbejAnx8xicQnQ1HgtW6avN8ZRQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e27f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2085\n"
     ]
    }
   ],
   "source": [
    "# TED XML 파싱 : <content> 텍스트를 추출해 코퍼스(corpus) 생성\n",
    "from lxml import etree    # XML 파싱/검색을 위한 도구\n",
    "\n",
    "xml = open('ted_en-20160408.xml', 'r', encoding='UTF-8')\n",
    "tree = etree.parse(xml)    # XML을 파싱해 트리 구조로 로드\n",
    "\n",
    "content = tree.xpath('//content/text()')    # Xpath로 모든 <content> 태그의 텍스트를 추출\n",
    "print(len(content))    # 추출된 content 항목 개수\n",
    "\n",
    "corpus = '\\n'.join(content)    # content를 줄바꿈 기준으로 합쳐 하나의 텍스트 copus를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14eb0094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 문장 수 271088\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 정제 + 토큰화 + 불용어 제거 : 코퍼스를 문장/단어 단위로 정규화\n",
    "# 정제\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpus = re.sub(r'\\([^)]*\\)', '', corpus)    # 괄호(...) 안에 있는 텍스트 제거\n",
    "corpus = re.sub(r'[^a-z0-9\\s.,!?]+', '', corpus, flags=re.IGNORECASE)  # 영문/숫자/공백,일부 구두점 외 문자는 제거\n",
    "corpus = corpus.lower()    # 대문자 -> 소문자\n",
    "\n",
    "en_stopwords = stopwords.words('english')    # 영어 불용어 리스트\n",
    "\n",
    "sentences = sent_tokenize(corpus)  # corpus를 문장 리스트로 분리\n",
    "normalized_sentences = []          # 정규화된 토큰 리스트\n",
    "\n",
    "# 토큰화 + 불용어 제거\n",
    "for sent in sentences:\n",
    "    tokens = word_tokenize(sent)    # 문장을 단어 리스트로 분리\n",
    "    normalized_tokens = [token for token in tokens if token not in en_stopwords]  # 불용어 제거\n",
    "    normalized_sentences.append(normalized_tokens)              # 문장별 토큰 리스트 누적\n",
    "\n",
    "print('총 문장 수', len(normalized_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59228a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 39s\n",
      "Wall time: 27.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time    # 이 셀의 실행 시간을 표시\n",
    "from gensim.models.fasttext import FastText  # FastText 임베딩 모델 클래스 (단어를 서브워드 n-gram까지 활용)\n",
    "\n",
    "model = FastText(\n",
    "    normalized_sentences,  # 입력 : 문장별 토큰 리스트 (list[list[str]])\n",
    "    vector_size = 100,     # 단어 벡터 차원\n",
    "    window = 5,            # 중심 단어 기준 주변 문맥 범위\n",
    "    min_count = 5,         # 빈도 5 이상 단어만 사용\n",
    "    workers = 4,           # CPU 스레드 (병렬처리)\n",
    "    sg = 1                 # 학습 알고리즘 (Skip-gram)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886641d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22174, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FastText 임베딩 크기 확인\n",
    "model.wv.vectors.shape    # (어휘 수, 임베딩 차원)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d89bfc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8340353965759277), ('batman', 0.8104761242866516), ('hoffman', 0.7864769101142883), ('shaman', 0.7565771341323853), ('roman', 0.7469342350959778), ('stuntman', 0.7465909123420715), ('lehman', 0.7402477860450745), ('manju', 0.7346705794334412), ('salman', 0.7294331192970276), ('wurman', 0.7237182259559631)]\n",
      "0.83403546\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('man'))\n",
    "print(model.wv.similarity('man', 'woman'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0c04bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1618\n",
      "[-0.01161957  0.33882692 -0.09070975  0.07605629  0.17311509]\n",
      "[ 0.03545292  0.45548013 -0.22000746  0.13983151 -0.05012467]\n",
      "[-0.0246458   0.16244182  0.14189483 -0.03576891  0.02247881]\n"
     ]
    }
   ],
   "source": [
    "# FastText 단어 벡터 조회 : 인덱스 접근 / 직접 접근. get_vector(서브워드 포함)\n",
    "print(model.wv.get_index('apple'))         # apple이 어휘에 있으므로 내부 인덱스 조회\n",
    "print(model.wv.vectors[1620][:5])          # 임베딩 행렬의 1620번째 벡터 앞 5개 값 (직접 인덱스 접근)\n",
    "print(model.wv.get_vector('apple')[:5])    # 'apple'의 벡터 앞 5개 값 (단어 기반 조회)\n",
    "\n",
    "# print(model.wv.get_index('ultramicrosafe'))  # KeyError (어휘에 없는 단어)\n",
    "print(model.wv.get_vector('ultramicrosafe')[:5])   # OOV 여도 서브워드(n-gram)으로 벡터를 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d9540b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03545292  0.45548013 -0.22000746  0.13983151 -0.05012467]\n",
      "\n",
      "1618\n",
      "\n",
      "[-0.1384791  -0.16164464 -0.13383614  0.00123979 -0.07405686]\n",
      "(14, 100)\n",
      "[[-0.08217657  1.1787535  -0.12257717  0.45731306 -0.08481173]\n",
      " [ 0.1670078   0.99128276 -0.07756011  0.37146685  0.1980966 ]\n",
      " [ 0.6103568   0.7550701   0.11018092  0.05052335  0.0242861 ]\n",
      " [ 0.03916723  0.09206647 -0.19266936 -0.01723336 -0.42351663]\n",
      " [-1.2337476   0.7820073   0.18461485 -0.17574532  0.12768432]\n",
      " [ 0.5625615   0.7397617  -0.07761648  0.11251777  0.11866154]\n",
      " [-0.07852228 -0.10636034 -0.17878072  0.04443979 -0.05383481]\n",
      " [-0.18734238 -0.21120086 -0.2099507   0.03821679  0.04345696]\n",
      " [-0.3679805   1.7279097  -0.38177884  0.06601056  0.0581628 ]\n",
      " [ 0.12337523 -0.07190139 -0.14842488  0.08098454 -0.09023755]\n",
      " [-0.1387259  -0.19262601 -0.25595814  0.07856406 -0.05199584]\n",
      " [-0.26066566  0.30421472 -0.9866203   0.8656037   0.10955898]\n",
      " [ 0.654573    0.31017753 -0.27825233  0.9064319  -0.13314211]\n",
      " [ 0.8623923   0.6946923  -0.5508828  -0.7828608  -0.52018183]]\n",
      "\n",
      "[ 0.03545292  0.45548013 -0.22000746  0.13983151 -0.05012467]\n"
     ]
    }
   ],
   "source": [
    "# fasttext 임베딩 생성과정 : (단어 자체 벡터 + 서브워드 벡터) 평균\n",
    "import numpy as np\n",
    "\n",
    "word = 'apple'\n",
    "\n",
    "word_vec = model.wv.get_vector(word)\n",
    "print(word_vec[:5])    # 'apple' 벡터의 앞 5개 값\n",
    "print()\n",
    "\n",
    "word_idx = model.wv.get_index(word)  # 'apple'의 인덱스\n",
    "print(word_idx)\n",
    "print()\n",
    "\n",
    "# 단어 자체 벡터\n",
    "base_vec = model.wv.vectors_vocab[word_idx]    # 'apple'의 단어 자체 벡터\n",
    "print(base_vec[:5])\n",
    "\n",
    "# 서브워드 벡터\n",
    "subword_index = model.wv.buckets_word[word_idx]         # 'apple'의 n-gram들이 매핑된 bucket 인덱스들\n",
    "subword_vecs = model.wv.vectors_ngrams[subword_index]   # 해당 n-gram bucket들의 벡터 묶음 (서브워드 벡터들)\n",
    "print(subword_vecs.shape)    # (서브워드 벡터 갯수 x 임베딩 차원)\n",
    "print(subword_vecs[:, :5])   # 서브워드 벡터들의 앞 5개 값\n",
    "print()\n",
    "\n",
    "# 최종 벡터 계산\n",
    "calc_vec = np.vstack((base_vec, subword_vecs)).mean(axis=0)  # (base + 모든 subword 벡터) 평균으로 최종 벡터 구성\n",
    "print(calc_vec[:5])  # 앞 5개만 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73db750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 벡터 생성 : FastText로 문장 전체를 하나의 벡터로 표현\n",
    "# 문장을 토큰단위로 나눠 단어 벡터들을 합쳐 문장 벡터 생성\n",
    "sent_vec = model.wv.get_sentence_vector(\"I love apple very much\")\n",
    "sent_vec.shape  # (vector_size, ) 형태의 문장 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75c754ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i love apple</th>\n",
       "      <th>apple is good</th>\n",
       "      <th>i hate banana</th>\n",
       "      <th>banana is yellow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>i love apple</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.915900</td>\n",
       "      <td>0.760621</td>\n",
       "      <td>0.899043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apple is good</th>\n",
       "      <td>0.915900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.722506</td>\n",
       "      <td>0.860076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i hate banana</th>\n",
       "      <td>0.760621</td>\n",
       "      <td>0.722506</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.896105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>banana is yellow</th>\n",
       "      <td>0.899043</td>\n",
       "      <td>0.860076</td>\n",
       "      <td>0.896105</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  i love apple  apple is good  i hate banana  banana is yellow\n",
       "i love apple          1.000000       0.915900       0.760621          0.899043\n",
       "apple is good         0.915900       1.000000       0.722506          0.860076\n",
       "i hate banana         0.760621       0.722506       1.000000          0.896105\n",
       "banana is yellow      0.899043       0.860076       0.896105          1.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "sentences = [\n",
    "    \"i love apple\",\n",
    "    \"apple is good\",\n",
    "    \"i hate banana\",\n",
    "    \"banana is yellow\"\n",
    "]\n",
    "\n",
    "# 각 문장을 문장벡터로 변환해 2D 배열로 생성\n",
    "sent_vecs = np.array([model.wv.get_sentence_vector(sent) for sent in sentences])\n",
    "print(sent_vecs.shape)  # (문장수, vector_size)\n",
    "\n",
    "sent_cos_sim = cosine_similarity(sent_vecs)  # 문장벡터들 간 코사인 유사도 행렬 계산\n",
    "\n",
    "pd.DataFrame(\n",
    "    sent_cos_sim,         # 유사도 행렬(2D)\n",
    "    columns = sentences,  # 비교대상 문장\n",
    "    index = sentences     # 기준 문장\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86be9496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastText 모델 저장\n",
    "model.save('ted_en_fasttext.model')    # FastText 학습 결과(가중치/어휘/설정 포함) 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c63209a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.fasttext.FastText at 0x1f7de82b620>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FastText 모델 로드\n",
    "model2 = FastText.load('ted_en_fasttext.model')\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc55a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.8340353965759277),\n",
       " ('batman', 0.8104761242866516),\n",
       " ('hoffman', 0.7864769101142883),\n",
       " ('shaman', 0.7565771341323853),\n",
       " ('roman', 0.7469342350959778),\n",
       " ('stuntman', 0.7465909123420715),\n",
       " ('lehman', 0.7402477860450745),\n",
       " ('manju', 0.7346705794334412),\n",
       " ('salman', 0.7294331192970276),\n",
       " ('wurman', 0.7237182259559631)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.most_similar('man')  # man과 가장 가까운 단어들 (단어, 유사도) 리스트로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774a44c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
