{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro_md",
            "metadata": {},
            "source": [
                "# 01. ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ (Attention Mechanism) ì‹¬ì¸µ ë¶„ì„\n",
                "\n",
                "## ğŸ’¡ \"ì–´í…ì…˜(Attention)\"ì´ ë­”ê°€ìš”?\n",
                "ì‚¬ëŒì´ ë¬´ì–¸ê°€ë¥¼ ë³¼ ë•Œ ëª¨ë“  ê²ƒì— ë˜‘ê°™ì´ ì§‘ì¤‘í•˜ì§€ ì•Šì£ ? ì¤‘ìš”í•œ ë¶€ë¶„ì— **'ì§‘ì¤‘(Attention)'**í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” íë¦¿í•˜ê²Œ ì¸ì‹í•©ë‹ˆë‹¤. ë”¥ëŸ¬ë‹ ëª¨ë¸ì—ê²Œë„ ì´ëŸ° ëŠ¥ë ¥ì„ ì‹¬ì–´ì¤€ ê²Œ ë°”ë¡œ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤.\n",
                "\n",
                "### ğŸ“š í•µì‹¬ ë¹„ìœ : ë„ì„œê´€ì˜ ì‚¬ì„œ\n",
                "ì–´í…ì…˜ì„ **'ë„ì„œê´€ì—ì„œ ì±… ì°¾ê¸°'**ì— ë¹„ìœ í•´ë³¼ê²Œìš”.\n",
                "\n",
                "*   **Query (Q, ì§ˆë¬¸)**: _\"ê¸ˆìœµ ìœ„ê¸°ì— ëŒ€í•œ ì±… ì°¾ì•„ì¤˜\"_ (ë‚´ê°€ ì°¾ê³ ì í•˜ëŠ” ì˜ë„)\n",
                "*   **Key (K, ë¶„ë¥˜í‘œ/ê¼¬ë¦¬í‘œ)**: _[ê²½ì œ, ì—­ì‚¬, ì†Œì„¤, ê³¼í•™...]_ (ì •ë³´ë¥¼ ì°¾ê¸° ìœ„í•œ ì¸ë±ìŠ¤)\n",
                "*   **Value (V, ì±… ë‚´ìš©)**: _[ê²½ì œí•™ ì›ë¡  ë‚´ìš©, ì¡°ì„ ì™•ì¡°ì‹¤ë¡ ë‚´ìš©...]_ (ì‹¤ì œ ë‹´ê³  ìˆëŠ” ì •ë³´)\n",
                "\n",
                "**ì‘ë™ ì›ë¦¬:**\n",
                "1.  ë‚´ ì§ˆë¬¸(Q)ê³¼ ë¶„ë¥˜í‘œ(K)ë¥¼ ë¹„êµí•´ì„œ ì–¼ë§ˆë‚˜ ì—°ê´€ì„± ìˆëŠ”ì§€ **ìœ ì‚¬ë„ ì ìˆ˜(Attention Score)**ë¥¼ ë§¤ê¹ë‹ˆë‹¤.\n",
                "2.  ì ìˆ˜ê°€ ë†’ì€ ì±…ì¼ìˆ˜ë¡ ë” ë§ì´ ì°¸ê³ í•˜ë¼ê³  **ê°€ì¤‘ì¹˜(Weight)**ë¥¼ ì¤ë‹ˆë‹¤.\n",
                "3.  ì´ ê°€ì¤‘ì¹˜ëŒ€ë¡œ ì±… ë‚´ìš©(V)ì„ ì„ì–´ì„œ **ìµœì¢… ìš”ì•½ë³¸(Context Vector)**ì„ ë§Œë“­ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "import_cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import math\n",
                "\n",
                "# ì‹œê°í™”ë¥¼ ìœ„í•œ ì„¤ì • (ì„ íƒ ì‚¬í•­)\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "input_setup",
            "metadata": {},
            "source": [
                "## 1. ì…ë ¥ ë°ì´í„° ì¤€ë¹„\n",
                "3ê°œì˜ ë‹¨ì–´ë¡œ ì´ë£¨ì–´ì§„ ë¬¸ì¥ì´ í•˜ë‚˜ ìˆë‹¤ê³  ê°€ì •í•´ë´…ì‹œë‹¤. (Batch=1, Seq_len=3)\n",
                "ê° ë‹¨ì–´ëŠ” 4ì°¨ì› ë²¡í„°ë¡œ í‘œí˜„ë©ë‹ˆë‹¤. (Embedding_dim=4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "data_input",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ì…ë ¥ x: (batch_size, seq_len, embedding_dim)\n",
                "# ì˜ˆ: [\"I\", \"love\", \"AI\"] ë¼ëŠ” ë¬¸ì¥ì´ ì„ë² ë”©ëœ ìƒíƒœ\n",
                "x = torch.tensor([\n",
                "    [\n",
                "        [1.0, 0.0, 1.0, 0.0],   # ë‹¨ì–´ 1\n",
                "        [0.0, 2.0, 0.0, 2.0],   # ë‹¨ì–´ 2\n",
                "        [1.0, 1.0, 1.0, 1.0]    # ë‹¨ì–´ 3\n",
                "    ]\n",
                "]) \n",
                "\n",
                "print('ì…ë ¥ x í˜•íƒœ:', x.shape)  # torch.Size([1, 3, 4])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "qkv_transform",
            "metadata": {},
            "source": [
                "## 2. Q, K, V ìƒì„± (Linear Transformation)\n",
                "ì…ë ¥ ë²¡í„° $x$ë¥¼ ê·¸ëŒ€ë¡œ ì“°ëŠ” ê²Œ ì•„ë‹ˆë¼, ê°ê°ì˜ ì—­í• ì— ë§ê²Œ ë³€ì‹ ì‹œì¼œì¤ë‹ˆë‹¤.\n",
                "ë§ˆì¹˜ ì¬ë£Œ($x$)ë¥¼ ê°€ì§€ê³  ìš”ë¦¬ì‚¬($W$)ë“¤ì´ ê°ì ë‹¤ë¥¸ ìš”ë¦¬(Q, K, V)ë¥¼ ì¤€ë¹„í•˜ëŠ” ê²ƒê³¼ ê°™ì•„ìš”.\n",
                "\n",
                "*   **$W^Q$**: ì§ˆë¬¸(Query)ì„ ë§Œë“œëŠ” ê°€ì¤‘ì¹˜\n",
                "*   **$W^K$**: ê¼¬ë¦¬í‘œ(Key)ë¥¼ ë§Œë“œëŠ” ê°€ì¤‘ì¹˜\n",
                "*   **$W^V$**: ë‚´ìš©ë¬¼(Value)ì„ ë‹¤ë“¬ëŠ” ê°€ì¤‘ì¹˜"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "liner_layer",
            "metadata": {},
            "outputs": [],
            "source": [
                "d_model = 4  # ì„ë² ë”© ì°¨ì›\n",
                "\n",
                "# ì„ í˜• ë³€í™˜ ë ˆì´ì–´ ì •ì˜ (biasëŠ” í¸ì˜ìƒ ë”)\n",
                "W_q = nn.Linear(d_model, d_model, bias=False)\n",
                "W_k = nn.Linear(d_model, d_model, bias=False)\n",
                "W_v = nn.Linear(d_model, d_model, bias=False)\n",
                "\n",
                "# ì…ë ¥ xë¥¼ í†µê³¼ì‹œì¼œ Q, K, V ìƒì„±\n",
                "Q = W_q(x)  # (1, 3, 4)\n",
                "K = W_k(x)  # (1, 3, 4)\n",
                "V = W_v(x)  # (1, 3, 4)\n",
                "\n",
                "print(\"Query shape :\", Q.shape)\n",
                "print(\"Key shape   :\", K.shape)\n",
                "print(\"Value shape :\", V.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dot_product",
            "metadata": {},
            "source": [
                "## 3. Scaled Dot-Product Attention\n",
                "ì–´í…ì…˜ì˜ í•µì‹¬ ê³µì‹ì…ë‹ˆë‹¤!\n",
                "\n",
                "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\n",
                "\n",
                "### ë‹¨ê³„ë³„ë¡œ ëœ¯ì–´ë³´ê¸°\n",
                "\n",
                "#### 3-1. ì ìˆ˜ ë§¤ê¸°ê¸° (QK^T)\n",
                "Queryì™€ Keyë¥¼ **ë‚´ì (Dot Product)**í•©ë‹ˆë‹¤. ë‚´ì ì€ ë‘ ë²¡í„°ê°€ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì£ .\n",
                "- ë‚´ì  ê°’ì´ í¬ë‹¤? -> \"ì–´? ë‚´ ì§ˆë¬¸ì´ë‘ ë”± ë§ëŠ” ì±…ì´ë„¤!\" (ê´€ë ¨ì„± ë†’ìŒ)\n",
                "- ë‚´ì  ê°’ì´ ì‘ë‹¤? -> \"ë³„ë¡œ ìƒê´€ì—†ëŠ” ì±…ì´êµ°.\" (ê´€ë ¨ì„± ë‚®ìŒ)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "attn_score",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Q(1, 3, 4)ì™€ Kì˜ ì „ì¹˜í–‰ë ¬(1, 4, 3)ì„ ê³±í•¨ -> (1, 3, 3)\n",
                "# ê²°ê³¼ í–‰ë ¬ì˜ ì˜ë¯¸: [ë‹¨ì–´1ê³¼ ë‹¨ì–´1ì˜ ê´€ê³„, ë‹¨ì–´1ê³¼ ë‹¨ì–´2ì˜ ê´€ê³„, ...]\n",
                "attn_scores = torch.matmul(Q, K.transpose(-2, -1)) \n",
                "\n",
                "print(\"ì–´í…ì…˜ ìŠ¤ì½”ì–´ í˜•íƒœ:\", attn_scores.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "scaling",
            "metadata": {},
            "source": [
                "#### 3-2. ìŠ¤ì¼€ì¼ë§ (Scaling)\n",
                "ë²¡í„° ì°¨ì›($d_k$)ì´ ì»¤ì§€ë©´ ë‚´ì  ê°’ë„ ë©ë‹¬ì•„ ì—„ì²­ ì»¤ì§ˆ ìˆ˜ ìˆì–´ìš”.\n",
                "ê°’ì´ ë„ˆë¬´ ì»¤ì§€ë©´ Softmaxë¥¼ ê±°ì¹  ë•Œ ê¸°ìš¸ê¸°(Gradient)ê°€ ì‚¬ë¼ì§€ëŠ” ë¬¸ì œê°€ ìƒê¹ë‹ˆë‹¤.\n",
                "ê·¸ë˜ì„œ ì°¨ì›ì˜ ì œê³±ê·¼($\\sqrt{d_k}$)ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ê°’ì„ ì¢€ ì§„ì •ì‹œì¼œì¤ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "apply_scaling",
            "metadata": {},
            "outputs": [],
            "source": [
                "d_k = Q.size(-1)\n",
                "scaled_scores = attn_scores / math.sqrt(d_k)\n",
                "\n",
                "print(\"ìŠ¤ì¼€ì¼ë§ ëœ ìŠ¤ì½”ì–´ (ì¼ë¶€):\\n\", scaled_scores[0].detach().numpy())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "softmax_step",
            "metadata": {},
            "source": [
                "#### 3-3. í™•ë¥  ë¶„í¬ ë³€í™˜ (Softmax)\n",
                "ì ìˆ˜ë“¤ì„ í™•ë¥ (%)ë¡œ ë°”ê¿‰ë‹ˆë‹¤. ëª¨ë“  ì ìˆ˜ì˜ í•©ì´ 1ì´ ë˜ë„ë¡ ë§Œë“¤ì–´ì„œ, \"ì´ ë‹¨ì–´ëŠ” 70% ì°¸ê³ í•˜ê³ , ì € ë‹¨ì–´ëŠ” 30% ì°¸ê³ í•´ë¼\"ë¼ëŠ” ëª…í™•í•œ ë¹„ìœ¨ì„ ë§Œë“­ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "apply_softmax",
            "metadata": {},
            "outputs": [],
            "source": [
                "attn_weights = F.softmax(scaled_scores, dim=-1)\n",
                "\n",
                "print(\"ì–´í…ì…˜ ê°€ì¤‘ì¹˜ (í™•ë¥  ë¶„í¬, í–‰ë³„ í•©=1):\\n\", attn_weights[0].detach().numpy())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "weighted_sum",
            "metadata": {},
            "source": [
                "#### 3-4. ìµœì¢… ê²°ê³¼ ìƒì„± (Value ê°€ì¤‘í•©)\n",
                "êµ¬í•´ë‘” í™•ë¥ ($\\text{Attention Weights}$)ì„ ì‹¤ì œ ì •ë³´($V$)ì— ê³±í•´ì„œ ë”í•©ë‹ˆë‹¤.\n",
                "ì¤‘ìš”í•œ ì •ë³´ëŠ” ì§„í•˜ê²Œ ë‚¨ê³ , ì¤‘ìš”í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” í¬ë¯¸í•´ì ¸ì„œ ì„ì´ê²Œ ë˜ì£ .\n",
                "\n",
                "ì´ê²Œ ë°”ë¡œ **Self-Attention**ì˜ ë§ˆë²•ì…ë‹ˆë‹¤! ë¬¸ì¥ ë‚´ì˜ ë‹¨ì–´ë“¤ì´ ì„œë¡œ ë¬¸ë§¥ì„ íŒŒì•…í•´ì„œ ì •ë³´ë¥¼ ì„ëŠ” ê³¼ì •ì´ì£ ."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "final_output",
            "metadata": {},
            "outputs": [],
            "source": [
                "output = torch.matmul(attn_weights, V)\n",
                "\n",
                "print(\"ìµœì¢… ì¶œë ¥ í˜•íƒœ :\", output.shape)\n",
                "print(\"ìµœì¢… ì¶œë ¥ ê°’ (ì²«ë²ˆì§¸ ë‹¨ì–´ì— ëŒ€í•œ ìš”ì•½):\\n\", output[0, 0].detach().numpy())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "mha_intro",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. ë©€í‹° í—¤ë“œ ì–´í…ì…˜ (Multi-Head Attention)\n",
                "\n",
                "í˜¼ìì„œ ê³µë¶€í•˜ëŠ” ê²ƒë³´ë‹¤, ì—¬ëŸ¬ ëª…ì˜ ì „ë¬¸ê°€ê°€ ê°ì ë‹¤ë¥¸ ê´€ì ì—ì„œ ë¶„ì„í•˜ë©´ ë” ì¢‹ê² ì£ ?\n",
                "*   **í—¤ë“œ 1**: \"ë‚˜ëŠ” ì£¼ì–´ì™€ ë™ì‚¬ì˜ ê´€ê³„ë¥¼ ì§‘ì¤‘í•´ì„œ ë³¼ê²Œ.\"\n",
                "*   **í—¤ë“œ 2**: \"ë‚˜ëŠ” í˜•ìš©ì‚¬ê°€ ë­˜ ê¾¸ë©°ì£¼ëŠ”ì§€ ë³¼ê²Œ.\"\n",
                "*   **í—¤ë“œ 3**: \"ë‚˜ëŠ” ì‹œì œ ì •ë³´ë¥¼ ë³¼ê²Œ.\"\n",
                "\n",
                "ì´ë ‡ê²Œ ì…ë ¥ì„ ì—¬ëŸ¬ ê°œ(`num_heads`)ë¡œ ìª¼ê°œì„œ ë³‘ë ¬ë¡œ ì–´í…ì…˜ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ **Multi-Head Attention**ì…ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "mha_input",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ì…ë ¥ ì°¨ì›ì„ ì¡°ê¸ˆ í‚¤ì›Œë´…ì‹œë‹¤ (8ì°¨ì›)\n",
                "x_large = torch.randn(1, 3, 8)  # ë°°ì¹˜ 1, ê¸¸ì´ 3, ì°¨ì› 8\n",
                "B, T, d_model = x_large.shape\n",
                "\n",
                "# í—¤ë“œ ì„¤ì •\n",
                "n_head = 4\n",
                "d_head = d_model // n_head  # ê° í—¤ë“œê°€ ë‹´ë‹¹í•  ì°¨ì› í¬ê¸° (8 / 4 = 2)\n",
                "\n",
                "print(f\"ì „ì²´ ì°¨ì›: {d_model}, í—¤ë“œ ê°œìˆ˜: {n_head}, í—¤ë“œ ë‹¹ ì°¨ì›: {d_head}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "mha_split",
            "metadata": {},
            "source": [
                "### 4-1. Q, K, V í—¤ë“œ ë‚˜ëˆ„ê¸° (Splitting Heads)\n",
                "í–‰ë ¬ ì°¨ì›ì„ ì¡°ì‘í•´ì„œ ì—¬ëŸ¬ ê°œì˜ í—¤ë“œë¡œ ìª¼ê°­ë‹ˆë‹¤. `view`ì™€ `transpose`ë¥¼ ì‚¬ìš©í•´ìš”."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "mha_projection",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Q, K, V ìƒì„±ì„ ìœ„í•œ ì„ í˜•ì¸µ\n",
                "W_q = nn.Linear(d_model, d_model, bias=False)\n",
                "W_k = nn.Linear(d_model, d_model, bias=False)\n",
                "W_v = nn.Linear(d_model, d_model, bias=False)\n",
                "\n",
                "# 1. ì„ í˜• ë³€í™˜\n",
                "q = W_q(x_large)  # (1, 3, 8)\n",
                "k = W_k(x_large)\n",
                "v = W_v(x_large)\n",
                "\n",
                "# 2. í—¤ë“œ ë‚˜ëˆ„ê¸° (Split Heads)\n",
                "# (Batch, Time, D_model) -> (Batch, Time, Head, D_head) -> (Batch, Head, Time, D_head)\n",
                "# transpose(1, 2)ë¥¼ í•´ì£¼ëŠ” ì´ìœ : \n",
                "# ì–´í…ì…˜ ê³„ì‚°(matmul)ì€ ë’¤ìª½ 2ê°œ ì°¨ì›(Time, D_head)ë¼ë¦¬ ì´ë£¨ì–´ì ¸ì•¼ í•˜ë¯€ë¡œ Head ì°¨ì›ì„ ì•ìœ¼ë¡œ ë¹¼ì¤ë‹ˆë‹¤.\n",
                "q = q.view(B, T, n_head, d_head).transpose(1, 2)\n",
                "k = k.view(B, T, n_head, d_head).transpose(1, 2)\n",
                "v = v.view(B, T, n_head, d_head).transpose(1, 2)\n",
                "\n",
                "print(\"í—¤ë“œ ë¶„í•  í›„ Q shape:\", q.shape)  # (1, 4, 3, 2)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "mha_calc",
            "metadata": {},
            "source": [
                "### 4-2. í—¤ë“œë³„ ì–´í…ì…˜ ìˆ˜í–‰\n",
                "ì´ì œ 4ê°œì˜ í—¤ë“œê°€ ê°ì ì•Œì•„ì„œ ìê¸° ëª«ì˜ ì–´í…ì…˜ì„ ê³„ì‚°í•©ë‹ˆë‹¤. (ë³‘ë ¬ ì²˜ë¦¬)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "mha_attention",
            "metadata": {},
            "outputs": [],
            "source": [
                "# (B, Head, T, D) * (B, Head, D, T) -> (B, Head, T, T)\n",
                "scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_head)\n",
                "weights = F.softmax(scores, dim=-1)\n",
                "\n",
                "# ê°€ì¤‘í•©\n",
                "# (B, Head, T, T) * (B, Head, T, D) -> (B, Head, T, D)\n",
                "attn_val = torch.matmul(weights, v)\n",
                "\n",
                "print(\"í—¤ë“œë³„ ì–´í…ì…˜ ê²°ê³¼:\", attn_val.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "mha_concat",
            "metadata": {},
            "source": [
                "### 4-3. ë‹¤ì‹œ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸° (Concat)\n",
                "4ëª…ì˜ ì „ë¬¸ê°€ê°€ ë‚´ë†“ì€ ê²°ê³¼ë¥¼ ë‹¤ì‹œ í•˜ë‚˜ë¡œ ì´ì–´ ë¶™ì…ë‹ˆë‹¤. ì›ë˜ ì°¨ì›(8ì°¨ì›)ìœ¼ë¡œ ë³µêµ¬ë˜ëŠ” ê±°ì£ ."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "mha_merge",
            "metadata": {},
            "outputs": [],
            "source": [
                "# (Batch, Head, Time, D_head) -> (Batch, Time, Head, D_head)\n",
                "attn_val = attn_val.transpose(1, 2).contiguous()\n",
                "\n",
                "# (Batch, Time, D_model)ë¡œ ì«™ í´ì£¼ê¸° (Flatten)\n",
                "output = attn_val.view(B, T, d_model)\n",
                "\n",
                "# ë§ˆì§€ë§‰ìœ¼ë¡œ í•œ ë²ˆ ë” ì˜ ì„ì–´ì£¼ëŠ” ì„ í˜•ì¸µ (W_o) í†µê³¼\n",
                "W_o = nn.Linear(d_model, d_model, bias=False)\n",
                "final_output = W_o(output)\n",
                "\n",
                "print(\"ìµœì¢… ì¶œë ¥ í˜•íƒœ (ì›ìƒë³µêµ¬):\", final_output.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "summary",
            "metadata": {},
            "source": [
                "### âœ¨ ìš”ì•½\n",
                "1.  **Q, K, V**: ì§ˆë¬¸, ê¼¬ë¦¬í‘œ, ë‚´ìš©ë¬¼ë¡œ ë³€í™˜.\n",
                "2.  **Scaled Dot-Product**: ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚° ë° í™•ë¥  ë³€í™˜.\n",
                "3.  **Multi-Head**: ì—¬ëŸ¬ ê´€ì ì—ì„œ ë™ì‹œì— ë¶„ì„í•˜ê¸° ìœ„í•´ ìª¼ê°œì„œ ê³„ì‚° í›„ ë³‘í•©.\n",
                "\n",
                "ì´ ë©”ì»¤ë‹ˆì¦˜ì´ ë°”ë¡œ **Transformer**ê°€ ë¬¸ë§¥ì„ ê¸°ê°€ ë§‰íˆê²Œ íŒŒì•…í•˜ëŠ” ë¹„ê²°ì…ë‹ˆë‹¤! ğŸ‘"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}