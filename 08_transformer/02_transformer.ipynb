{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "transformer_intro",
            "metadata": {},
            "source": [
                "# Transformer: ì–¸ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ê±°ëŒ€í•œ ê³µì¥ (Factory)\n",
                "\n",
                "**Transformer**ëŠ” 2017ë…„ êµ¬ê¸€ì˜ \"Attention is All You Need\" ë…¼ë¬¸ì—ì„œ ë“±ì¥í•œ ëª¨ë¸ë¡œ, í˜„ëŒ€ NLP(ìì—°ì–´ ì²˜ë¦¬)ì˜ ê¸°í‹€ì´ ëœ ì•„ì£¼ ì¤‘ìš”í•œ ëª¨ë¸ì…ë‹ˆë‹¤. (BERT, GPT ëª¨ë‘ ì´ ì¹œêµ¬ì˜ í›„ì†ì´ì£ !)\n",
                "\n",
                "## ğŸ­ ë¹„ìœ : ë²ˆì—­ ê³µì¥ (Translation Factory)\n",
                "Transformer ëª¨ë¸ì„ í•˜ë‚˜ì˜ ê±°ëŒ€í•œ **'ìë™ ë²ˆì—­ ê³µì¥'**ì´ë¼ê³  ìƒìƒí•´ë´…ì‹œë‹¤.\n",
                "\n",
                "1.  **ì›ìì¬ ì…ê³  (Input Embedding)**: ì˜ì–´ ë¬¸ì¥ì´ë¼ëŠ” ì›ìì¬ê°€ ë“¤ì–´ì˜µë‹ˆë‹¤. ê¸°ê³„ê°€ ì´í•´í•  ìˆ˜ ìˆê²Œ ìˆ«ì(ë²¡í„°)ë¡œ ë°”ë€ë‹ˆë‹¤.\n",
                "2.  **ì‹œê°„í‘œ ì°ê¸° (Positional Encoding)**: ì´ ê³µì¥ì€ ì»¨ë² ì´ì–´ ë²¨íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤(RNNì²˜ëŸ¼ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ). ê·¸ë˜ì„œ ìì¬ë§ˆë‹¤ \"ë‚˜ëŠ” 1ë²ˆì§¸ ë‹¨ì–´\", \"ë‚˜ëŠ” 2ë²ˆì§¸ ë‹¨ì–´\"ë¼ëŠ” ì‹œê°„í‘œ(ìˆœì„œ ì •ë³´)ë¥¼ ë¶™ì—¬ì¤ë‹ˆë‹¤.\n",
                "3.  **ë¶„ì„ ë¶€ì„œ (Encoder)**: \n",
                "    - ë“¤ì–´ì˜¨ ë¬¸ì¥ì„ ê¼¼ê¼¼íˆ ë¶„ì„í•©ë‹ˆë‹¤. \n",
                "    - \"ì´ ë‹¨ì–´ëŠ” ì € ë‹¨ì–´ë‘ ê´€ë ¨ì´ ìˆë„¤?\" (Self-Attention)\n",
                "    - ë¶„ì„í•œ ë‚´ìš©ì„ ìš”ì•½í•´ì„œ ë””ì½”ë”ì—ê²Œ ë„˜ê¹ë‹ˆë‹¤.\n",
                "4.  **ì¡°ë¦½ ë¶€ì„œ (Decoder)**: \n",
                "    - ì¸ì½”ë”ê°€ ì¤€ ë¶„ì„ ë³´ê³ ì„œ(Encoder Output)ë¥¼ ì°¸ê³ í•©ë‹ˆë‹¤.\n",
                "    - ì§€ê¸ˆê¹Œì§€ ë§Œë“  ë²ˆì—­ë¬¸ì„ ë‹¤ì‹œ ë³´ë©´ì„œ(Masked Attention) ë‹¤ìŒ ë‹¨ì–´ë¥¼ í•˜ë‚˜ì”© ì¡°ë¦½í•©ë‹ˆë‹¤.\n",
                "5.  **ì¶œí•˜ (Output Linear)**: ì¡°ë¦½ëœ ë‹¨ì–´ë“¤ì„ ìµœì¢… ì–¸ì–´(í•œêµ­ì–´) ë‹¨ì–´ ì¤‘ í•˜ë‚˜ë¡œ ì„ íƒí•´ì„œ ë‚´ë³´ëƒ…ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "hyperparameters_md",
            "metadata": {},
            "source": [
                "## 1. ê³µì¥ ì„¤ë¹„ ì„¤ì • (Hyperparameter)\n",
                "ê³µì¥ì„ ì§“ê¸° ì „ì— ì„¤ê³„ë„ë¥¼ ë´…ë‹ˆë‹¤. ì´ ìˆ˜ì¹˜ë“¤ì€ ëª¨ë¸ì˜ í¬ê¸°ì™€ ì„±ëŠ¥ì„ ê²°ì •í•©ë‹ˆë‹¤.\n",
                "\n",
                "- **$d_{model} = 512$**: ê° ë‹¨ì–´(ìì¬)ê°€ ê°€ì§ˆ ì •ë³´ëŸ‰ì˜ í¬ê¸°ì…ë‹ˆë‹¤. (ì„ë² ë”© ì°¨ì›)\n",
                "- **$num\\_layers = 6$**: ë¶„ì„ ë¶€ì„œ(Encoder)ì™€ ì¡°ë¦½ ë¶€ì„œ(Decoder)ë¥¼ ëª‡ ì¸µì´ë‚˜ ìŒ“ì„ì§€ ê²°ì •í•©ë‹ˆë‹¤. 6ì¸µì§œë¦¬ ê³µì¥ì´ë„¤ìš”.\n",
                "- **$num\\_heads = 8$**: í•œ ë²ˆì— ëª‡ êµ°ë°ë¥¼ ì§‘ì¤‘í•´ì„œ ë³¼ì§€(Attention Head) ì •í•©ë‹ˆë‹¤. ì§ì› 8ëª…ì´ ë™ì‹œì— ë¶„ì„í•œë‹¤ê³  ë³´ë©´ ë©ë‹ˆë‹¤.\n",
                "- **$d_{ff} = 2048$**: ê° ì¸µ ë‚´ë¶€ì˜ ê°œë³„ ì²˜ë¦¬ ê³µê°„(Feed Forward) í¬ê¸°ì…ë‹ˆë‹¤.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "hyperparameters_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import torch.nn.functional as F\n",
                "import math\n",
                "\n",
                "# ëª¨ë¸ ì„¤ì •ê°’ (Hyperparameters)\n",
                "d_model = 512              # ì„ë² ë”© ì°¨ì› (ë‹¨ì–´ ë²¡í„°ì˜ í¬ê¸°)\n",
                "num_layers = 6             # ì¸µ(Layer)ì˜ ê°œìˆ˜ (Encoder, Decoder ê°ê°)\n",
                "num_heads = 8              # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì˜ í—¤ë“œ ê°œìˆ˜\n",
                "d_k = d_model // num_heads # ê° í—¤ë“œê°€ ë‹´ë‹¹í•  ì°¨ì› í¬ê¸° (512 / 8 = 64)\n",
                "d_ff = 2048                # í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ì˜ ì€ë‹‰ì¸µ í¬ê¸°\n",
                "drop_out_rate = 0.1        # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ (ê³¼ì í•© ë°©ì§€)\n",
                "\n",
                "# ë‹¨ì–´ì¥ í¬ê¸° (ê°€ì •)\n",
                "src_vocab_size = 10000     # ì›ë¬¸(Source) ë‹¨ì–´ ê°œìˆ˜\n",
                "trg_vocab_size = 10000     # ë²ˆì—­ë¬¸(Target) ë‹¨ì–´ ê°œìˆ˜\n",
                "\n",
                "batch_size = 64            # ë°°ì¹˜ í¬ê¸°\n",
                "seq_len = 128              # ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´\n",
                "\n",
                "# GPU ì‚¬ìš© ì„¤ì •\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "positional_encoding_md",
            "metadata": {},
            "source": [
                "## 2. ìœ„ì¹˜ ì •ë³´ ì£¼ì… (Positional Encoding)\n",
                "RNNì€ ë‹¨ì–´ë¥¼ ìˆœì„œëŒ€ë¡œ ë„£ì§€ë§Œ, íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ë¬¸ì¥ ì „ì²´ë¥¼ í•œë²ˆì— 'ì¾…'í•˜ê³  ë„£ìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ë‹¨ì–´ë“¤ì´ ì›ë˜ ì–´ë””ì— ìˆì—ˆëŠ”ì§€ ëª¨ë¦…ë‹ˆë‹¤. \n",
                "**\"ë„ˆëŠ” ì²« ë²ˆì§¸, ë„ˆëŠ” ë‘ ë²ˆì§¸...\"** ë¼ê³  ì•Œë ¤ì£¼ëŠ” ëª…ì°°í‘œê°€ í•„ìš”í•˜ì£ .\n",
                "\n",
                "ë‹¨ìˆœíˆ 1, 2, 3... ìˆ«ìë¥¼ ë”í•˜ë©´ ê°’ì´ ë„ˆë¬´ ì»¤ì ¸ì„œ, **ì‚¬ì¸(sin)ê³¼ ì½”ì‚¬ì¸(cos) ì£¼ê¸° í•¨ìˆ˜**ë¥¼ ì´ìš©í•´ -1~1 ì‚¬ì´ì˜ ë…íŠ¹í•œ íŒ¨í„´ ê°’ì„ ë§Œë“¤ì–´ ë”í•´ì¤ë‹ˆë‹¤. ë§ˆì¹˜ ì‹œê³„ë°”ëŠ˜ì²˜ëŸ¼ ê·œì¹™ì ìœ¼ë¡œ ëŒì•„ê°€ëŠ” ê°’ì„ ì´ìš©í•˜ëŠ” ì›ë¦¬ì…ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "positional_encoding_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "class PositionalEncoding(nn.Module):\n",
                "    def __init__(self, seq_len, d_model):\n",
                "        super().__init__()\n",
                "\n",
                "        # (seq_len, d_model) í¬ê¸°ì˜ 0 í–‰ë ¬ ìƒì„±\n",
                "        pe = torch.zeros(seq_len, d_model)\n",
                "        \n",
                "        # ìœ„ì¹˜(pos) ì¸ë±ìŠ¤ ìƒì„± (0, 1, ..., seq_len-1). (seq_len, 1) í˜•íƒœë¡œ ë³€í™˜.\n",
                "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
                "        \n",
                "        # ì£¼ê¸°ë¥¼ ê²°ì •í•˜ëŠ” ë¶„ëª¨ ê³„ì‚° (10000 ^ (2i / d_model))\n",
                "        # log ê³µê°„ì—ì„œ ê³„ì‚°í•˜ì—¬ ìˆ˜ì¹˜ ì•ˆì •ì„± í™•ë³´\n",
                "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
                "\n",
                "        # ì§ìˆ˜ ì¸ë±ìŠ¤(2i)ì—ëŠ” sin í•¨ìˆ˜ ì ìš©\n",
                "        pe[:, 0::2] = torch.sin(position * div_term)\n",
                "        # í™€ìˆ˜ ì¸ë±ìŠ¤(2i+1)ì—ëŠ” cos í•¨ìˆ˜ ì ìš©\n",
                "        pe[:, 1::2] = torch.cos(position * div_term)\n",
                "\n",
                "        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€ (1, seq_len, d_model)\n",
                "        pe = pe.unsqueeze(0)\n",
                "        \n",
                "        # í•™ìŠµë˜ëŠ” íŒŒë¼ë¯¸í„°ê°€ ì•„ë‹ˆë¯€ë¡œ bufferë¡œ ë“±ë¡ (optimizerê°€ ì—…ë°ì´íŠ¸ ì•ˆ í•¨)\n",
                "        self.register_buffer('pe', pe)\n",
                "\n",
                "    def forward(self, x):\n",
                "        # x: (batch_size, seq_len, d_model)\n",
                "        x = x * math.sqrt(d_model) # ì„ë² ë”© ê°’ ìŠ¤ì¼€ì¼ë§ (ìœ„ì¹˜ì •ë³´ì— ë¬»íˆì§€ ì•Šê²Œ)\n",
                "        \n",
                "        # ì…ë ¥ ê¸¸ì´ì— ë§ì¶° ì˜ë¼ì„œ ë”í•´ì£¼ê¸°\n",
                "        x = x + self.pe[:, :x.size(1), :]\n",
                "        return x"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "layernorm_md",
            "metadata": {},
            "source": [
                "## 3. í’ˆì§ˆ ê´€ë¦¬íŒ€ (Layer Normalization)\n",
                "ê³µì¥ ê° ë‹¨ê³„ë§ˆë‹¤ ë°ì´í„° ê°’ì´ ë„ˆë¬´ ì»¤ì§€ê±°ë‚˜ ì‘ì•„ì§€ì§€ ì•Šê²Œ **ì •ê·œí™”(Normalization)**ë¥¼ í•´ì¤ë‹ˆë‹¤. \n",
                "ë°ì´í„°ì˜ í‰ê· ì„ 0, ë¶„ì‚°ì„ 1ë¡œ ë§ì¶°ì„œ í•™ìŠµì´ ì•ˆì •ì ìœ¼ë¡œ ë˜ê²Œ ë•ëŠ” 'í’ˆì§ˆ ê´€ë¦¬íŒ€' ê°™ì€ ì—­í• ì…ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "layernorm_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "class LayerNormalization(nn.Module):\n",
                "    def __init__(self, d_model, eps=1e-6):\n",
                "        super().__init__()\n",
                "        # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° (scale, shift) - ê°ë§ˆ, ë² íƒ€\n",
                "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
                "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
                "        self.eps = eps\n",
                "\n",
                "    def forward(self, x):\n",
                "        # ë§ˆì§€ë§‰ ì°¨ì›(d_model)ì— ëŒ€í•´ í‰ê· ê³¼ ë¶„ì‚° êµ¬í•˜ê¸°\n",
                "        mean = x.mean(-1, keepdim=True)\n",
                "        std = x.std(-1, keepdim=True)\n",
                "        \n",
                "        # ì •ê·œí™” ë° ìŠ¤ì¼€ì¼ë§\n",
                "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ffn_md",
            "metadata": {},
            "source": [
                "## 4. ê°œë³„ ì—°êµ¬ì‹¤ (Feed Forward Network)\n",
                "Multi-Head Attentionì´ \"íŒ€ì›ë“¤ê³¼ íšŒì˜\"í•˜ëŠ” ê²ƒì´ë¼ë©´, FFNì€ \"í˜¼ì ìë¦¬ì— ì•‰ì•„ì„œ ì •ë¦¬\"í•˜ëŠ” ì‹œê°„ì…ë‹ˆë‹¤.\n",
                "ê° ë‹¨ì–´(ìœ„ì¹˜)ë§ˆë‹¤ ê°œë³„ì ìœ¼ë¡œ ì ìš©ë˜ëŠ” ê°„ë‹¨í•œ ì‹ ê²½ë§(Linear -> ReLU -> Linear)ì…ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "ffn_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "class FeedForwardLayer(nn.Module):\n",
                "    def __init__(self, d_model, d_ff, drop_out_rate):\n",
                "        super().__init__()\n",
                "        # 1. ì°¨ì› í™•ì¥ (512 -> 2048)\n",
                "        self.linear1 = nn.Linear(d_model, d_ff)\n",
                "        self.relu = nn.ReLU()\n",
                "        self.dropout = nn.Dropout(drop_out_rate)\n",
                "        # 2. ì°¨ì› ë³µêµ¬ (2048 -> 512)\n",
                "        self.linear2 = nn.Linear(d_ff, d_model)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self.linear1(x)\n",
                "        x = self.relu(x)\n",
                "        x = self.dropout(x)\n",
                "        x = self.linear2(x)\n",
                "        return x"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "mha_md",
            "metadata": {},
            "source": [
                "## 5. íŒ€ íšŒì˜ (Multi-Head Attention)\n",
                "ì´ ëª¨ë¸ì˜ í•µì‹¬! ë‹¨ì–´ë“¤ë¼ë¦¬ ì„œë¡œ ì •ë³´ë¥¼ ì£¼ê³ ë°›ëŠ” ê³³ì…ë‹ˆë‹¤. \n",
                "ì—¬ëŸ¬ ê°œì˜ í—¤ë“œ(Head)ê°€ ê°ì ë‹¤ë¥¸ ê´€ì (ë¬¸ë²•, ì˜ë¯¸, ìœ„ì¹˜ ë“±)ì—ì„œ ë‹¨ì–´ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.\n",
                "\n",
                "*   **Query**: ì§ˆë¬¸í•˜ëŠ” ì£¼ì²´\n",
                "*   **Key**: ë‹µë³€í•  ìˆ˜ ìˆëŠ” ëŒ€ìƒ\n",
                "*   **Value**: ê·¸ ëŒ€ìƒì´ ê°€ì§„ ì •ë³´\n",
                "\n",
                "`attention()` í•¨ìˆ˜ì—ì„œ Scaled Dot-Product Attentionì„ ìˆ˜í–‰í•˜ê³ , `MultiHeadAttention` í´ë˜ìŠ¤ì—ì„œ ì…ë ¥ ë¶„í•  ë° ë³‘í•©ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "mha_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "class MultiheadAttention(nn.Module):\n",
                "    def __init__(self, d_model, drop_out_rate):\n",
                "        super().__init__()\n",
                "        self.num_heads = num_heads\n",
                "        self.d_k = d_k # í—¤ë“œ ë‹¹ ì°¨ì›\n",
                "\n",
                "        # Q, K, V ìƒì„±ì„ ìœ„í•œ ì„ í˜•ì¸µ\n",
                "        self.w_q = nn.Linear(d_model, d_model)\n",
                "        self.w_k = nn.Linear(d_model, d_model)\n",
                "        self.w_v = nn.Linear(d_model, d_model)\n",
                "        \n",
                "        self.w_o = nn.Linear(d_model, d_model)  # ìµœì¢… ì¶œë ¥ ì„ í˜•ì¸µ\n",
                "        self.dropout = nn.Dropout(drop_out_rate)\n",
                "        self.softmax = nn.Softmax(dim=-1)\n",
                "\n",
                "    def attention(self, q, k, v, mask=None):\n",
                "        # 1. ì ìˆ˜ ê³„ì‚° (Queryì™€ Keyì˜ ìœ ì‚¬ë„)\n",
                "        # (B, h, T, d_k) * (B, h, d_k, T) -> (B, h, T, T)\n",
                "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
                "\n",
                "        # 2. ë§ˆìŠ¤í‚¹ ì ìš© (í•„ìš”í•  ê²½ìš°)\n",
                "        if mask is not None:\n",
                "            # scoreê°€ ë§¤ìš° ì‘ì€ ìˆ˜(-1e9)ë¡œ ë§Œë“¤ì–´ softmax í™•ë¥ ì´ 0ì´ ë˜ê²Œ í•¨\n",
                "            # maskê°€ 0ì¸ ë¶€ë¶„ì´ ê°€ë ¤ì§ˆ ë¶€ë¶„ (êµ¬í˜„ ë°©ì‹ì— ë”°ë¼ 1/0 ì˜ë¯¸ëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)\n",
                "            # ì—¬ê¸°ì„  maskê°€ 0ì¸ ìœ„ì¹˜ë¥¼ -1e9ë¡œ ì±„ì›€\n",
                "            if mask.dim() == 2:\n",
                "                mask = mask.unsqueeze(1).unsqueeze(1)\n",
                "            scores = scores.masked_fill(mask == 0, -1e9)\n",
                "\n",
                "        # 3. í™•ë¥  ë¶„í¬ (Softmax)\n",
                "        attn_weights = self.softmax(scores)\n",
                "        attn_weights = self.dropout(attn_weights)\n",
                "\n",
                "        # 4. ê°€ì¤‘í•© (Value ì„ê¸°)\n",
                "        output = torch.matmul(attn_weights, v)\n",
                "        return output\n",
                "\n",
                "    def forward(self, q, k, v, mask=None):\n",
                "        batch_size = q.size(0)\n",
                "\n",
                "        # 1. ì„ í˜• ë³€í™˜\n",
                "        q = self.w_q(q)\n",
                "        k = self.w_k(k)\n",
                "        v = self.w_v(v)\n",
                "\n",
                "        # 2. í—¤ë“œ ë¶„í•  (view & transpose)\n",
                "        # (B, T, d_model) -> (B, T, h, d_k) -> (B, h, T, d_k)\n",
                "        q = q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
                "        k = k.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
                "        v = v.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
                "\n",
                "        # 3. ì–´í…ì…˜ ìˆ˜í–‰\n",
                "        output = self.attention(q, k, v, mask)\n",
                "\n",
                "        # 4. í—¤ë“œ ë³‘í•© (transpose & view)\n",
                "        # (B, h, T, d_k) -> (B, T, h, d_k) -> (B, T, d_model)\n",
                "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, d_model)\n",
                "\n",
                "        # 5. ìµœì¢… ì„ í˜• ë³€í™˜ (ì„ê¸°)\n",
                "        output = self.w_o(output)\n",
                "        return output"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "encoder_layer_md",
            "metadata": {},
            "source": [
                "## 6. ì¸ì½”ë” ë ˆì´ì–´ (Encoder Layer)\n",
                "ë¶„ì„ ë¶€ì„œì˜ ì‘ì—…ìëŠ” ë‘ ê°€ì§€ ì¼ì„ í•©ë‹ˆë‹¤.\n",
                "1.  **Multi-Head Attention**: ì£¼ë³€ ë‹¨ì–´ë“¤ê³¼ ì†Œí†µí•˜ë©° ë¬¸ë§¥ íŒŒì•… (íŒ€ íšŒì˜)\n",
                "2.  **Feed Forward**: íŒŒì•…ëœ ë‚´ìš©ì„ í˜¼ì ì •ë¦¬ (ê°œë³„ í•™ìŠµ)\n",
                "\n",
                "ê·¸ë¦¬ê³  **Residual Connection (ì”ì°¨ ì—°ê²°)**ì„ í†µí•´, ì²˜ë¦¬ ì „ì˜ ì›ë˜ ì •ë³´ë„ ìŠì§€ ì•Šê³  ì±™ê²¨ê°‘ë‹ˆë‹¤. (`x + sublayer(x)`)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "encoder_layer_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "class EncoderLayer(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.mha = MultiheadAttention(d_model, drop_out_rate)\n",
                "        self.ffn = FeedForwardLayer(d_model, d_ff, drop_out_rate)\n",
                "\n",
                "        self.layernorm1 = LayerNormalization(d_model)\n",
                "        self.layernorm2 = LayerNormalization(d_model)\n",
                "        \n",
                "        self.dropout1 = nn.Dropout(drop_out_rate)\n",
                "        self.dropout2 = nn.Dropout(drop_out_rate)\n",
                "\n",
                "    def forward(self, x, mask):\n",
                "        # 1. Multi-Head Attention (Self-Attention)\n",
                "        # ì¸ì½”ë”ì—ì„  Q, K, Vê°€ ëª¨ë‘ ìê¸° ìì‹ (x)ì…ë‹ˆë‹¤.\n",
                "        attn_output = self.mha(x, x, x, mask)\n",
                "        attn_output = self.dropout1(attn_output)\n",
                "        # Residual Connection + Norm\n",
                "        out1 = self.layernorm1(x + attn_output)\n",
                "\n",
                "        # 2. Feed Forward Network\n",
                "        ffn_output = self.ffn(out1)\n",
                "        ffn_output = self.dropout2(ffn_output)\n",
                "        # Residual Connection + Norm\n",
                "        out2 = self.layernorm2(out1 + ffn_output)\n",
                "\n",
                "        return out2\n",
                "\n",
                "class Encoder(nn.Module):\n",
                "    def __init__(self, d_model, num_layers):\n",
                "        super().__init__()\n",
                "        self.d_model = d_model\n",
                "        self.num_layers = num_layers\n",
                "        # EncoderLayerë¥¼ 6ì¸µ(num_layers) ìŒ“ìŒ\n",
                "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(num_layers)])\n",
                "\n",
                "    def forward(self, x, mask):\n",
                "        for layer in self.layers:\n",
                "            x = layer(x, mask)\n",
                "        return x"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "decoder_layer_md",
            "metadata": {},
            "source": [
                "## 7. ë””ì½”ë” ë ˆì´ì–´ (Decoder Layer)\n",
                "ì¡°ë¦½ ë¶€ì„œì—ì„œëŠ” ì¡°ê¸ˆ ë” ë³µì¡í•œ ì¼ì„ í•©ë‹ˆë‹¤.\n",
                "\n",
                "1.  **Masked Self-Attention**: ì§€ê¸ˆê¹Œì§€ ì“´ ìê¸° ìì‹ ì˜ ê¸€ì„ ë˜ëŒì•„ë´…ë‹ˆë‹¤. ë‹¨, **ë¯¸ë˜ì˜ ë‹¨ì–´ë¥¼ ë¯¸ë¦¬ ì»¨ë‹í•˜ë©´ ì•ˆ ë˜ë¯€ë¡œ** (`Look-ahead Mask`) ê°€ë ¤ë‘ê³  ë´…ë‹ˆë‹¤. ë§ˆì¹˜ ì¼ê¸°ë¥¼ ì“¸ ë•Œ, ì•„ì§ ì•ˆ ì“´ ë’·ë‚´ìš©ì€ ì°¸ê³ í•  ìˆ˜ ì—†ëŠ” ê²ƒê³¼ ê°™ì£ .\n",
                "2.  **Cross-Attention (Encoder-Decoder Attention)**: ì¸ì½”ë” ë¶„ì„íŒ€ì´ ë³´ë‚´ì¤€ ìë£Œ(Key, Value)ë¥¼ ì°¸ê³ í•´ì„œ ë‚´ í•  ë§(Query)ì„ ë§Œë“­ë‹ˆë‹¤. \"ì•„ê¹Œ ê·¸ ë¬¸ë§¥ì´ ì´ê±°ì˜€ì§€?\" í•˜ê³  í™•ì¸í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.\n",
                "3.  **Feed Forward**: ì •ë¦¬ ì‹œê°„."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "decoder_layer_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "class DecoderLayer(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.mha1 = MultiheadAttention(d_model, drop_out_rate)  # Masked Self-Attention\n",
                "        self.mha2 = MultiheadAttention(d_model, drop_out_rate)  # Cross-Attention\n",
                "        self.ffn = FeedForwardLayer(d_model, d_ff, drop_out_rate)\n",
                "\n",
                "        self.layernorm1 = LayerNormalization(d_model)\n",
                "        self.layernorm2 = LayerNormalization(d_model)\n",
                "        self.layernorm3 = LayerNormalization(d_model)\n",
                "        \n",
                "        self.dropout1 = nn.Dropout(drop_out_rate)\n",
                "        self.dropout2 = nn.Dropout(drop_out_rate)\n",
                "        self.dropout3 = nn.Dropout(drop_out_rate)\n",
                "\n",
                "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
                "        # 1. Masked Self-Attention (ìì‹ ë§Œ ë³´ê¸° + ë¯¸ë˜ ê°€ë¦¬ê¸°)\n",
                "        attn1 = self.mha1(x, x, x, look_ahead_mask)\n",
                "        attn1 = self.dropout1(attn1)\n",
                "        out1 = self.layernorm1(x + attn1)\n",
                "\n",
                "        # 2. Cross-Attention (ì¸ì½”ë” ì¶œë ¥ ì°¸ê³ )\n",
                "        # Q: ë””ì½”ë”(ìì‹ ), K: ì¸ì½”ë” ì¶œë ¥, V: ì¸ì½”ë” ì¶œë ¥\n",
                "        attn2 = self.mha2(out1, enc_output, enc_output, padding_mask)\n",
                "        attn2 = self.dropout2(attn2)\n",
                "        out2 = self.layernorm2(out1 + attn2)\n",
                "\n",
                "        # 3. Feed Forward\n",
                "        ffn_output = self.ffn(out2)\n",
                "        ffn_output = self.dropout3(ffn_output)\n",
                "        out3 = self.layernorm3(out2 + ffn_output)\n",
                "\n",
                "        return out3\n",
                "\n",
                "class Decoder(nn.Module):\n",
                "    def __init__(self, d_model, num_layers):\n",
                "        super().__init__()\n",
                "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(num_layers)])\n",
                "\n",
                "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
                "        for layer in self.layers:\n",
                "            x = layer(x, enc_output, look_ahead_mask, padding_mask)\n",
                "        return x"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "transformer_main_md",
            "metadata": {},
            "source": [
                "## 8. ê³µì¥ ê°€ë™ (Transformer Main)\n",
                "ì´ì œ ëª¨ë“  ë¶€í’ˆì„ ì¡°ë¦½í•´ ì™„ì„±ëœ Transformer ëª¨ë¸ì„ ë§Œë“­ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "transformer_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "class Transformer(nn.Module):\n",
                "    def __init__(self, src_vocab_size, trg_vocab_size, d_model, seq_len, num_layers):\n",
                "        super().__init__()\n",
                "        # 1. ì„ë² ë”© + ìœ„ì¹˜ ì •ë³´\n",
                "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
                "        self.trg_embedding = nn.Embedding(trg_vocab_size, d_model)\n",
                "        self.positional_encoding = PositionalEncoding(seq_len, d_model)\n",
                "\n",
                "        # 2. ì¸ì½”ë” & ë””ì½”ë” ìŠ¤íƒ\n",
                "        self.encoder = Encoder(d_model, num_layers)\n",
                "        self.decoder = Decoder(d_model, num_layers)\n",
                "\n",
                "        # 3. ìµœì¢… ì¶œë ¥ì¸µ (ë‹¨ì–´ì¥ í¬ê¸°ë¡œ ë³€í™˜)\n",
                "        self.output_layer = nn.Linear(d_model, trg_vocab_size)\n",
                "        self.softmax = nn.LogSoftmax(dim=-1)\n",
                "\n",
                "    def forward(self, src_inputs, trg_inputs, src_mask=None, trg_mask=None):\n",
                "        # --- Encoder Part ---\n",
                "        # 1. ì†ŒìŠ¤ ì„ë² ë”© + ìœ„ì¹˜ ì •ë³´\n",
                "        src_emb = self.src_embedding(src_inputs) # (B, T_src) -> (B, T_src, d_model)\n",
                "        src_emb = self.positional_encoding(src_emb)\n",
                "\n",
                "        # 2. ì¸ì½”ë” í†µê³¼\n",
                "        # enc_outputs: ì¸ì½”ë”ê°€ ë¶„ì„í•œ ë¬¸ë§¥ ì •ë³´ (ë””ì½”ë”ì˜ K, Vë¡œ ì“°ì„)\n",
                "        enc_outputs = self.encoder(src_emb, src_mask) \n",
                "\n",
                "        # --- Decoder Part ---\n",
                "        # 3. íƒ€ê²Ÿ ì„ë² ë”© + ìœ„ì¹˜ ì •ë³´\n",
                "        trg_emb = self.trg_embedding(trg_inputs) # (B, T_trg) -> (B, T_trg, d_model)\n",
                "        trg_emb = self.positional_encoding(trg_emb)\n",
                "\n",
                "        # 4. ë””ì½”ë” í†µê³¼\n",
                "        # ì¸ì½”ë” ì¶œë ¥(enc_outputs)ì„ ë°›ì•„ì„œ ì²˜ë¦¬\n",
                "        dec_outputs = self.decoder(trg_emb, enc_outputs, trg_mask, src_mask)\n",
                "\n",
                "        # --- Output Part ---\n",
                "        # 5. ë‹¤ìŒ ë‹¨ì–´ í™•ë¥  ì˜ˆì¸¡\n",
                "        logits = self.output_layer(dec_outputs) # (B, T_trg, vocab_size)\n",
                "        log_probs = self.softmax(logits)\n",
                "        \n",
                "        return log_probs"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "test_md",
            "metadata": {},
            "source": [
                "## 9. ëª¨ë¸ í…ŒìŠ¤íŠ¸ (Dummy Data)\n",
                "ëª¨ë¸ì´ ì˜ ì¡°ë¦½ë˜ì—ˆëŠ”ì§€ ê°€ì§œ ë°ì´í„°ë¥¼ ë„£ì–´ì„œ í™•ì¸í•´ë´…ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "test_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ëª¨ë¸ ìƒì„±\n",
                "model = Transformer(src_vocab_size, trg_vocab_size, d_model, seq_len, num_layers)\n",
                "model = model.to(device)\n",
                "\n",
                "# ë”ë¯¸ ë°ì´í„° ìƒì„± (Batch=64, Seq_Len=128)\n",
                "src_input = torch.randint(0, src_vocab_size, (batch_size, seq_len)).to(device)\n",
                "trg_input = torch.randint(0, trg_vocab_size, (batch_size, seq_len)).to(device)\n",
                "\n",
                "# ëª¨ë¸ ì‹¤í–‰\n",
                "model.eval() # í‰ê°€ ëª¨ë“œ\n",
                "with torch.no_grad():\n",
                "    output = model(src_input, trg_input)\n",
                "\n",
                "print(\"===== ëª¨ë¸ ì…ì¶œë ¥ í¬ê¸° í™•ì¸ =====\")\n",
                "print(f\"Source Input Shape: {src_input.shape} (Batch, SeqLen)\")\n",
                "print(f\"Target Input Shape: {trg_input.shape} (Batch, SeqLen)\")\n",
                "print(f\"Model Output Shape: {output.shape} (Batch, SeqLen, VocabSize)\")\n",
                "print(\"ì„±ê³µì ìœ¼ë¡œ ëª¨ë¸ì´ ë™ì‘í•©ë‹ˆë‹¤! ğŸ‰\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}