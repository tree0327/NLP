{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro_md",
            "metadata": {},
            "source": [
                "# 01. N-gram 언어 모델: 통계로 단어 맞히기\n",
                "\n",
                "## 1. N-gram의 개념\n",
                "\n",
                "**언어 모델(Language Model)**의 기초 원리는 **\"앞에 나온 단어들을 보고, 그 다음에 나올 단어를 확률적으로 예측하는 것\"**입니다.\n",
                "우리가 자주 사용하는 **스마트폰 키보드 자동완성** 기능이 대표적인 예시입니다.\n",
                "\n",
                "### 쉬운 비유\n",
                "마치 **빈칸 채우기** 문제입니다.\n",
                "- 문제: \"오늘 점심은 맛있는 [ ? ]\"\n",
                "- 예측 후보: 밥(80%), 거(10%), 비(5%)...\n",
                "\n",
                "이 중에서 가장 확률이 높은 \"밥\"을 추천하는 것이 언어 모델의 역할입니다.\n",
                "\n",
                "### 핵심 용어: N (몇 개를 볼까?)\n",
                "몇 개의 단어를 한 덩어리로 묶어서 볼 것인지를 결정하는 숫자입니다.\n",
                "이 숫자를 **N**이라고 부릅니다.\n",
                "\n",
                "| 이름 | N | 설명 | 구조 시각화 |\n",
                "|---|---|---|---|\n",
                "| **Unigram** | 1 | 단어 1개씩 따로 봅니다 | `[I]`, `[love]`, `[NLP]` |\n",
                "| **Bigram** | 2 | **2개씩** 짝지어 봅니다 | `[I -> love]`, `[love -> NLP]` |\n",
                "| **Trigram** | 3 | 3개씩 짝지어 봅니다 | `[I, love -> NLP]` |\n",
                "\n",
                "이 실습에서는 **Bigram (2개씩 짝짓기)** 모델을 만들어 봅니다.\n",
                "즉, **\"바로 앞 단어 1개를 힌트로, 다음 단어를 맞추는 모델\"**입니다."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "step1_md",
            "metadata": {},
            "source": [
                "## 2. 도구 준비\n",
                "\n",
                "언어 처리에 필요한 기본 도구들을 불러옵니다.\n",
                "\n",
                "- `nltk`: 자연어 처리 도구 모음 (단어를 자르는 칼 포함)\n",
                "- `ngrams`: 단어들을 N개씩 묶어주는 도구\n",
                "- `Counter`: 개수를 세어주는 계산기"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "code_import",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "라이브러리 로드 완료\n"
                    ]
                }
            ],
            "source": [
                "import nltk\n",
                "from nltk.util import ngrams      # N-gram 생성 함수\n",
                "from collections import Counter   # 빈도수 계산 클래스\n",
                "\n",
                "# NLTK 토크나이저 데이터 다운로드 (최초 1회)\n",
                "nltk.download('punkt', quiet=True) \n",
                "print(\"라이브러리 로드 완료\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "step2_md",
            "metadata": {},
            "source": [
                "## 3. 데이터 자르기 (Tokenization)\n",
                "\n",
                "모델을 학습시키기 위한 문장 데이터(Corpus)를 준비합니다.\n",
                "문장을 단어 단위인 **토큰(Token)**으로 잘게 자릅니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "code_tokenization",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "원본 문장: \"오늘은 날씨가 좋다. 오늘은 기분이 좋다. 오늘은 일이 많다. 오늘은 사람이 많다. 오늘은 날씨가 맑다.\"\n",
                        "자른 결과: ['오늘은', '날씨가', '좋다', '.', '오늘은', '기분이', '좋다', '.', '오늘은', '일이', '많다', '.', '오늘은', '사람이', '많다', '.', '오늘은', '날씨가', '맑다', '.']\n"
                    ]
                }
            ],
            "source": [
                "# 학습 데이터\n",
                "text = \"오늘은 날씨가 좋다. 오늘은 기분이 좋다. 오늘은 일이 많다. 오늘은 사람이 많다. 오늘은 날씨가 맑다.\"\n",
                "\n",
                "# 문장을 단어 단위로 분리\n",
                "tokens = nltk.word_tokenize(text)\n",
                "\n",
                "print(f\"원본 문장: \\\"{text}\\\"\")\n",
                "print(f\"자른 결과: {tokens}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "step3_md",
            "metadata": {},
            "source": [
                "## 4. 짝꿍 만들기 (Bigram)\n",
                "\n",
                "토큰들을 2개씩 묶어서 패턴을 찾습니다.\n",
                "\n",
                "### 구조 시각화\n",
                "```\n",
                "원본 데이터: [오늘은, 날씨가, 좋다]\n",
                "\n",
                "1번 짝꿍:  [오늘은] ---> [날씨가]\n",
                "2번 짝꿍:            [날씨가] ---> [좋다]\n",
                "```\n",
                "\n",
                "이렇게 짝을 지은 뒤, 각 짝이 **몇 번 등장했는지** 세어봅니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "code_bigram",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[분석 결과: '오늘은' 뒤에 무엇이 왔을까?]\n",
                        "  ('오늘은', '날씨가') -> 2번 등장\n",
                        "  ('오늘은', '기분이') -> 1번 등장\n",
                        "  ('오늘은', '일이') -> 1번 등장\n",
                        "  ('오늘은', '사람이') -> 1번 등장\n"
                    ]
                }
            ],
            "source": [
                "# 1. Bigram 생성 (2개씩 묶기)\n",
                "bigram = list(ngrams(tokens, 2))\n",
                "\n",
                "# 2. 개수 세기 (Counter 활용)\n",
                "unigram_freq = Counter(tokens)  # 각 단어가 몇 번 나왔나? (분모)\n",
                "bigram_freq = Counter(bigram)   # 짝꿍 단어가 몇 번 나왔나? (분자)\n",
                "\n",
                "print(\"[분석 결과: '오늘은' 뒤에 무엇이 왔을까?]\")\n",
                "for pair, count in bigram_freq.items():\n",
                "    if pair[0] == '오늘은':\n",
                "        print(f\"  {pair} -> {count}번 등장\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "step4_md",
            "metadata": {},
            "source": [
                "## 5. 확률 계산하기\n",
                "\n",
                "이제 통계를 바탕으로 확률을 계산합니다.\n",
                "복잡한 공식 없이, 간단한 산수로 생각하면 됩니다.\n",
                "\n",
                "### 확률 계산 방법\n",
                "**확률 = (둘이 함께 나온 횟수) ÷ (앞 단어가 나온 전체 횟수)**\n",
                "\n",
                "예를 들어:\n",
                "1. \"오늘은\" 이라는 단어가 총 **5번** 나왔습니다.\n",
                "2. 그 중 \"오늘은 -> 날씨가\" 패턴은 **2번** 나왔습니다.\n",
                "\n",
                "그러면 확률은?\n",
                "-> **2 나누기 5 = 0.4 (40%)** 가 됩니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "code_prob",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[확률 계산 결과]\n",
                        "앞 단어 -> 뒷 단어 : 확률\n",
                        "==============================\n",
                        "오늘은 -> 날씨가 : 0.40 (40%) \n",
                        "날씨가 -> 좋다 : 0.50 (50%) \n",
                        "좋다 -> . : 1.00 (100%) \n",
                        ". -> 오늘은 : 0.80 (80%) \n",
                        "오늘은 -> 기분이 : 0.20 (20%) \n",
                        "기분이 -> 좋다 : 1.00 (100%) \n",
                        "오늘은 -> 일이 : 0.20 (20%) \n",
                        "일이 -> 많다 : 1.00 (100%) \n",
                        "많다 -> . : 1.00 (100%) \n",
                        "오늘은 -> 사람이 : 0.20 (20%) \n",
                        "사람이 -> 많다 : 1.00 (100%) \n",
                        "날씨가 -> 맑다 : 0.50 (50%) \n",
                        "맑다 -> . : 1.00 (100%) \n"
                    ]
                }
            ],
            "source": [
                "print(\"[확률 계산 결과]\")\n",
                "print(\"앞 단어 -> 뒷 단어 : 확률\")\n",
                "print(\"=\" * 30)\n",
                "\n",
                "for (w1, w2), count in bigram_freq.items():\n",
                "    # 확률 = (함께 나온 횟수) / (앞 단어 전체 횟수)\n",
                "    prob = count / unigram_freq[w1]\n",
                "    \n",
                "    print(f\"{w1} -> {w2} : {prob:.2f} ({prob*100:.0f}%) \")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "step5_md",
            "metadata": {},
            "source": [
                "## 6. 모델 평가: 얼마나 헷갈려 할까? (PPL)\n",
                "\n",
                "모델의 성능을 평가할 때는 **PPL(Perplexity, 혼란도)**라는 점수를 씁니다.\n",
                "\n",
                "### 개념 시각화\n",
                "PPL은 **\"다음에 올 단어 후보의 개수\"**라고 이해하면 쉽습니다.\n",
                "\n",
                "**상황 A: 정답을 확신할 때 (좋은 상태)**\n",
                "- \"나는 학교에 [   ]\"\n",
                "- 후보: 간다 (확률 99%)\n",
                "- **PPL = 1** (후보가 1개뿐이다!)\n",
                "\n",
                "**상황 B: 헷갈릴 때 (나쁜 상태)**\n",
                "- \"나는 [   ]\"\n",
                "- 후보: 밥을, 학교에, 집에, 친구를... (후보가 100개)\n",
                "- **PPL = 100** (후보가 100개나 되네...)\n",
                "\n",
                "따라서 PPL 점수는 **낮을수록 (1에 가까울수록) 좋은 모델**입니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "code_ppl_func",
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "\n",
                "def compute_bigram_perplexity(test_text, unigram_freq, bigram_freq):\n",
                "    \"\"\"\n",
                "    PPL을 계산하는 함수입니다.\n",
                "    결과값이 낮을수록 모델이 문장을 잘 이해한 것입니다.\n",
                "    \"\"\"\n",
                "    test_tokens = nltk.word_tokenize(test_text)\n",
                "    test_bigrams = list(ngrams(test_tokens, 2))\n",
                "\n",
                "    N = len(test_bigrams)\n",
                "    if N == 0: return 0\n",
                "\n",
                "    log_prob_sum = 0\n",
                "\n",
                "    for w1, w2 in test_bigrams:\n",
                "        w1_count = unigram_freq.get(w1, 0)\n",
                "        \n",
                "        # 학습하지 않아서 모르는 단어가 나오면 아주 작은 확률을 줍니다.\n",
                "        if w1_count == 0:\n",
                "             prob = 1e-10\n",
                "        else:\n",
                "            prob = bigram_freq.get((w1, w2), 0) / w1_count\n",
                "            \n",
                "        if prob == 0:\n",
                "            prob = 1e-10\n",
                "            \n",
                "        log_prob_sum += math.log2(prob)\n",
                "\n",
                "    # 평균을 내고 2의 제곱을 해줍니다.\n",
                "    cross_entropy = -log_prob_sum / N\n",
                "    perplexity = math.pow(2, cross_entropy)\n",
                "\n",
                "    return perplexity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "code_ppl_test",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[모델 평가]\n",
                        "점수가 1에 가까울수록 좋습니다.\n",
                        "----------------------------------------\n",
                        "문장: 오늘은 날씨가 좋다.     | PPL 점수: 1.71  [우수] AI: \"이건 확실히 아는 문장이야!\"\n",
                        "문장: 오늘은 기분이 좋다.     | PPL 점수: 1.71  [우수] AI: \"이건 확실히 아는 문장이야!\"\n",
                        "문장: 기계 번역은 어렵다.     | PPL 점수: 10000000000.00  [실패] AI: \"모르는 단어 투성이야...\"\n"
                    ]
                }
            ],
            "source": [
                "test_sentences = [\n",
                "    \"오늘은 날씨가 좋다.\",      # 학습했던 문장 그대로 (예상: 점수 좋음)\n",
                "    \"오늘은 기분이 좋다.\",      # 배운 패턴 (예상: 점수 좋음)\n",
                "    \"기계 번역은 어렵다.\"        # 안 배운 단어들 (예상: 점수 나쁨)\n",
                "]\n",
                "\n",
                "print(\"[모델 평가]\")\n",
                "print(\"점수가 1에 가까울수록 좋습니다.\")\n",
                "print(\"-\" * 40)\n",
                "\n",
                "for sent in test_sentences:\n",
                "    ppl = compute_bigram_perplexity(sent, unigram_freq, bigram_freq)\n",
                "    \n",
                "    # 결과 해석\n",
                "    analysis = \"\"\n",
                "    if ppl < 2: analysis = \"[우수] AI: \\\"이건 확실히 아는 문장이야!\\\"\"\n",
                "    elif ppl > 1000: analysis = \"[실패] AI: \\\"모르는 단어 투성이야...\\\"\"\n",
                "    \n",
                "    print(f\"문장: {sent:<15} | PPL 점수: {ppl:.2f}  {analysis}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "quiz_md",
            "metadata": {},
            "source": [
                "## 7. 핵심 요약 퀴즈\n",
                "\n",
                "1. **Bigram 확률**을 구할 때 분모에 들어가는 값은 무엇인가요?\n",
                "   - [ ] 앞 단어가 등장한 전체 횟수\n",
                "   - [ ] 뒷 단어가 등장한 전체 횟수\n",
                "   \n",
                "2. 만약 **PPL 점수가 100점**이라면 무슨 뜻일까요?\n",
                "   - [ ] 다음에 올 단어 후보가 1개뿐이라 확실하다.\n",
                "   - [ ] 다음에 올 단어 후보가 100개쯤 되어서 매우 헷갈린다.\n",
                "\n",
                "정답은 위에서 배운 내용을 다시 확인해보세요."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "nlp_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
