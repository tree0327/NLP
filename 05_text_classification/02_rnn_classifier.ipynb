{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbcIPWYG4Qj6"
   },
   "source": [
    "# RNN기반 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1694,
     "status": "ok",
     "timestamp": 1750140631111,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "6U_oVjNc4c3I",
    "outputId": "339f9151-f4ae-4015-8be5-784639f4fc96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comp.graphics', 'rec.sport.baseball', 'sci.space']\n",
      "From: kjenks@gothamcity.jsc.nasa.gov\n",
      "Subject: Life on Mars???\n",
      "Organization: NASA/JSC/GM2, Space Shuttle Program Office \n",
      "X-Newsreader: TIN [version 1.1 PL8]\n",
      "Lines: 12\n",
      "\n",
      "I know it's only wishful thinking, with our current President,\n",
      "but this is from last fall:\n",
      "\n",
      "     \"Is there life on Mars?  Maybe not now.  But there will be.\"\n",
      "        -- Daniel S. Goldin, NASA Administrator, 24 August 1992\n",
      "\n",
      "-- Ken Jenks, NASA/JSC/GM2, Space Shuttle Program Office\n",
      "      kjenks@gothamcity.jsc.nasa.gov  (713) 483-4368\n",
      "\n",
      "     \"The man who makes no mistakes does not usually make\n",
      "      anything.\"\n",
      "        -- Edward John Phelps, American Diplomat/Lawyer (1825-1895)\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로딩\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['comp.graphics', 'sci.space', 'rec.sport.baseball']\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
    "X = newsgroups.data\n",
    "y = newsgroups.target\n",
    "print(newsgroups.target_names)\n",
    "print(X[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6431,
     "status": "ok",
     "timestamp": 1750140647214,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "yXBtkCIp4H51",
    "outputId": "e593d05d-06d7-4379-efa1-6ddb83765ef1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2954, 200)\n"
     ]
    }
   ],
   "source": [
    "# 데이터전처리\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocab_size = 10000    # 사용할 단어 사전 크기 (상위 빈도 단어만 유지)\n",
    "max_len = 200         # 모델 입력으로 사용할 시퀀스 길이\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X)    # 토크나이저 단어 사전 학습\n",
    "X_encoded = tokenizer.texts_to_sequences(X)  # 문서를 정수 시퀀스로 변환\n",
    "X_padded = pad_sequences(X_encoded, maxlen=max_len)  # 길이를 max_len으로 통일(패딩/자르기)\n",
    "print(X_padded.shape)    # (문서 수, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0suBZZ3V6ZuL"
   },
   "outputs": [],
   "source": [
    "# 데이터분리/텐서변환\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 전체 데이터에서 train/valid/test로 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(torch.tensor(X_padded), torch.tensor(y), test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# dataset/dataloader\n",
    "train_dataset = TensorDataset(X_train, y_train)  # 학습용(입력, 라벨) Dataset\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # 학습 데이터로더 생성 (셔플)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)     # 검증 데이터로더 생성 (고정)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)   # 테스트 데이터로더 생성 (고정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7BjT5FY8Tnt"
   },
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "import torch.nn as nn\n",
    "\n",
    "# 임베딩 -> LSTM -> FC로 문장을 요약해 3개 클래스 로짓을 출력하는 분류기 모델\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        # embedding - lstm - dense\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)  # 토큰 ID -> 임베딩(0 토큰은 PAD: 학습사용안함)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)        # (B, T, *)\n",
    "        self.fc = nn.Linear(hidden_size, 3)  # 마지막 은닉 -> 3 클래스 로짓 반환\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)       # (B, T) -> (B, T, E)\n",
    "        _, (h, c) = self.lstm(x)    # h:(L, B, H), C: (L, B, H)\n",
    "        out = self.fc(h[-1])        # 마지막 레이어 은닉상태 (B, H) -> (B, 3)\n",
    "        return out                  # softmax 전 로짓 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11441,
     "status": "ok",
     "timestamp": 1750139218556,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "-vO1L30O8Vf5",
    "outputId": "3485c91a-9816-4db2-9410-6c819ee4562a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss 1.0480, Train Acc 0.4788, Val Loss 0.9969, Val Acc 0.5222, \n",
      "Epoch 2/50: Train Loss 0.8941, Train Acc 0.6159, Val Loss 0.8736, Val Acc 0.6173, \n",
      "Epoch 3/50: Train Loss 0.6872, Train Acc 0.7243, Val Loss 0.7383, Val Acc 0.6850, \n",
      "Epoch 4/50: Train Loss 0.5185, Train Acc 0.8085, Val Loss 0.6547, Val Acc 0.7315, \n",
      "Epoch 5/50: Train Loss 0.3808, Train Acc 0.8635, Val Loss 0.6521, Val Acc 0.7146, \n",
      "Epoch 6/50: Train Loss 0.2608, Train Acc 0.9153, Val Loss 0.5672, Val Acc 0.7822, \n",
      "Epoch 7/50: Train Loss 0.1833, Train Acc 0.9418, Val Loss 0.5817, Val Acc 0.7780, \n",
      "Epoch 8/50: Train Loss 0.1265, Train Acc 0.9656, Val Loss 0.5555, Val Acc 0.7822, \n",
      "Epoch 9/50: Train Loss 0.0963, Train Acc 0.9735, Val Loss 0.5628, Val Acc 0.8055, \n",
      "Epoch 10/50: Train Loss 0.0888, Train Acc 0.9751, Val Loss 0.6335, Val Acc 0.7801, \n",
      "Epoch 11/50: Train Loss 0.0741, Train Acc 0.9767, Val Loss 0.6443, Val Acc 0.7717, \n",
      "Epoch 12/50: Train Loss 0.0616, Train Acc 0.9836, Val Loss 0.6387, Val Acc 0.8097, \n",
      "Epoch 13/50: Train Loss 0.0195, Train Acc 0.9963, Val Loss 0.6204, Val Acc 0.8182, \n",
      "Epoch 14/50: Train Loss 0.0172, Train Acc 0.9963, Val Loss 0.6160, Val Acc 0.8140, \n",
      "Epoch 15/50: Train Loss 0.0150, Train Acc 0.9989, Val Loss 0.6452, Val Acc 0.8118, \n",
      "Epoch 16/50: Train Loss 0.0089, Train Acc 0.9995, Val Loss 0.7128, Val Acc 0.8034, \n",
      "Epoch 17/50: Train Loss 0.0056, Train Acc 0.9995, Val Loss 0.7338, Val Acc 0.8182, \n",
      "Epoch 18/50: Train Loss 0.0029, Train Acc 1.0000, Val Loss 0.7870, Val Acc 0.8309, \n",
      "Epoch 19/50: Train Loss 0.0020, Train Acc 1.0000, Val Loss 0.7948, Val Acc 0.8266, \n",
      "Epoch 20/50: Train Loss 0.0015, Train Acc 1.0000, Val Loss 0.8211, Val Acc 0.8309, \n",
      "Epoch 21/50: Train Loss 0.0012, Train Acc 1.0000, Val Loss 0.8309, Val Acc 0.8309, \n",
      "Epoch 22/50: Train Loss 0.0010, Train Acc 1.0000, Val Loss 0.8407, Val Acc 0.8309, \n",
      "Epoch 23/50: Train Loss 0.0008, Train Acc 1.0000, Val Loss 0.8665, Val Acc 0.8288, \n",
      "Epoch 24/50: Train Loss 0.0007, Train Acc 1.0000, Val Loss 0.8467, Val Acc 0.8309, \n",
      "Epoch 25/50: Train Loss 0.0007, Train Acc 1.0000, Val Loss 0.8579, Val Acc 0.8266, \n",
      "Epoch 26/50: Train Loss 0.0006, Train Acc 1.0000, Val Loss 0.8633, Val Acc 0.8309, \n",
      "Epoch 27/50: Train Loss 0.0005, Train Acc 1.0000, Val Loss 0.8696, Val Acc 0.8330, \n",
      "Epoch 28/50: Train Loss 0.0005, Train Acc 1.0000, Val Loss 0.8959, Val Acc 0.8309, \n",
      "Epoch 29/50: Train Loss 0.0004, Train Acc 1.0000, Val Loss 0.9007, Val Acc 0.8351, \n",
      "Epoch 30/50: Train Loss 0.0004, Train Acc 1.0000, Val Loss 0.9076, Val Acc 0.8288, \n",
      "Epoch 31/50: Train Loss 0.0004, Train Acc 1.0000, Val Loss 0.8943, Val Acc 0.8414, \n",
      "Epoch 32/50: Train Loss 0.0003, Train Acc 1.0000, Val Loss 0.8882, Val Acc 0.8478, \n",
      "Epoch 33/50: Train Loss 0.0783, Train Acc 0.9757, Val Loss 0.6939, Val Acc 0.7738, \n",
      "Epoch 34/50: Train Loss 0.1109, Train Acc 0.9651, Val Loss 0.6740, Val Acc 0.8161, \n",
      "Epoch 35/50: Train Loss 0.0987, Train Acc 0.9709, Val Loss 0.8338, Val Acc 0.7505, \n",
      "Epoch 36/50: Train Loss 0.0784, Train Acc 0.9762, Val Loss 0.7447, Val Acc 0.7590, \n",
      "Epoch 37/50: Train Loss 0.0204, Train Acc 0.9958, Val Loss 0.7480, Val Acc 0.8055, \n",
      "Epoch 38/50: Train Loss 0.0043, Train Acc 1.0000, Val Loss 0.7447, Val Acc 0.8055, \n",
      "Epoch 39/50: Train Loss 0.0076, Train Acc 0.9979, Val Loss 0.8024, Val Acc 0.8013, \n",
      "Epoch 40/50: Train Loss 0.0255, Train Acc 0.9947, Val Loss 0.7356, Val Acc 0.7907, \n",
      "Epoch 41/50: Train Loss 0.0111, Train Acc 0.9984, Val Loss 0.7568, Val Acc 0.8161, \n",
      "Epoch 42/50: Train Loss 0.0037, Train Acc 0.9995, Val Loss 0.7269, Val Acc 0.8266, \n",
      "Epoch 43/50: Train Loss 0.0014, Train Acc 1.0000, Val Loss 0.7430, Val Acc 0.8266, \n",
      "Epoch 44/50: Train Loss 0.0011, Train Acc 1.0000, Val Loss 0.7500, Val Acc 0.8288, \n",
      "Epoch 45/50: Train Loss 0.0009, Train Acc 1.0000, Val Loss 0.7583, Val Acc 0.8351, \n",
      "Epoch 46/50: Train Loss 0.0007, Train Acc 1.0000, Val Loss 0.7645, Val Acc 0.8330, \n",
      "Epoch 47/50: Train Loss 0.0006, Train Acc 1.0000, Val Loss 0.7715, Val Acc 0.8309, \n",
      "Epoch 48/50: Train Loss 0.0006, Train Acc 1.0000, Val Loss 0.7820, Val Acc 0.8266, \n",
      "Epoch 49/50: Train Loss 0.0005, Train Acc 1.0000, Val Loss 0.7884, Val Acc 0.8245, \n",
      "Epoch 50/50: Train Loss 0.0004, Train Acc 1.0000, Val Loss 0.7958, Val Acc 0.8224, \n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # cuda 또는 cpu\n",
    "embedding_dim = 100\n",
    "hidden_size = 128\n",
    "\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습루프\n",
    "train_losses, train_accs = [], []\n",
    "val_losses, val_accs = [], []\n",
    "\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # 학습\n",
    "    model.train()\n",
    "    train_loss, train_correct, train_total = 0, 0, 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.detach().cpu().item()\n",
    "        pred = output.argmax(dim=1)  # 가장 큰 로짓 인덱스로 클래스 예측\n",
    "        train_correct += (pred == y_batch).sum().detach().cpu().item()\n",
    "        train_total += len(y_batch)\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = train_correct / train_total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    # 검증\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "\n",
    "            val_loss += loss.detach().cpu().item()\n",
    "            pred = output.argmax(dim=1)  # 가장 큰 로짓 인덱스로 클래스 예측\n",
    "            val_correct += (pred == y_batch).sum().detach().cpu().item()\n",
    "            val_total += len(y_batch)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "    # 출력(train_loss, val_loss)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}: '\n",
    "          f'Train Loss {train_loss:.4f}, '\n",
    "          f'Train Acc {train_acc:.4f}, '\n",
    "          f'Val Loss {val_loss:.4f}, '\n",
    "          f'Val Acc {val_acc:.4f}, ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1750139872430,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "clNCNefO9nBv",
    "outputId": "4505f72f-01c6-4123-81bc-30dc2c86a93c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     comp.graphics       0.83      0.88      0.85       202\n",
      "rec.sport.baseball       0.82      0.86      0.84       202\n",
      "         sci.space       0.81      0.73      0.77       187\n",
      "\n",
      "          accuracy                           0.82       591\n",
      "         macro avg       0.82      0.82      0.82       591\n",
      "      weighted avg       0.82      0.82      0.82       591\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "# - 정답, 모델 예측값을 가지고, classification_report 작성\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        pred = output.argmax(dim=1)\n",
    "\n",
    "        all_preds.extend(pred.detach().cpu().numpy())      # 배치 예측을 리스트에 누적\n",
    "        all_labels.extend(y_batch.detach().cpu().numpy())  # 배치 정답을 리스트에 누적\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=newsgroups.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRcaUXF_gGi-"
   },
   "source": [
    "## 사전학습된 임베딩 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22000,
     "status": "ok",
     "timestamp": 1750140554021,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "a9UBkjQlgQdh",
    "outputId": "2f5e4a97-1014-4754-f96f-9a54a3da4027"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ted_en_fasttext.model : 모델 본체 (메타데이터 + 학습된 파라미터 경로)\n",
    "- ted_en_fasttext.model.wv.vectors_ngrams.npy : FastText의 핵심 특징인 서브워드(n-gram) 벡터 행렬(Numpy 배열 파일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1440,
     "status": "ok",
     "timestamp": 1750140871457,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "w8s1Ezq6hQlW",
    "outputId": "da2ef527-897d-4b87-ae81-c7ba2bb5980d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "fasttext_model = FastText.load('ted_en_fasttext.model')\n",
    "print(fasttext_model.vector_size)  # 단어 벡터 차원 (단어 임베딩 벡터의 차원수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 104,
     "status": "ok",
     "timestamp": 1750141482707,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "bv-YDGQJhyTf",
    "outputId": "aa62b304-44f1-45b2-c9ac-3a1094b38e5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedding_dim = fasttext_model.vector_size\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))  # (단어사전크기, 임베딩차원) 0 행렬 초기화\n",
    "\n",
    "word_index = tokenizer.word_index # 단어 -> 인덱스 사전\n",
    "word_index = {word:index \\\n",
    "              for word, index in word_index.items() \\\n",
    "                if index < vocab_size}  # vocab_size 범위 내 단어만 필터링해서 사용\n",
    "print(len(word_index)) # 10000\n",
    "\n",
    "for word, index in word_index.items():\n",
    "    if word in fasttext_model.wv:  # FastText가 해당 단어 벡터를 가지고있으면\n",
    "        embedding_matrix[index] = fasttext_model.wv[word]  # 해당 인덱스 위치에 임베딩 벡터를 채움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wiQxp4Ag9xz"
   },
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "import torch.nn as nn\n",
    "\n",
    "# 사전학습 임베딩(embedding_matrix)으로 임베딩 레이어를 초기화한 LSTM 분류기\n",
    "class LSTMClassifier2(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, embedding_matrix, hidden_size):\n",
    "        super().__init__()\n",
    "        # embedding - lstm - dense\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))  # 사전학습 임베딩 가중치로 초기화\n",
    "        self.embedding.weight.requires_grad = True    # 임베딩을 학습에 포함할지 여부 (True =미세조정, False=고정)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)       # (B, T) -> (B, T, E)\n",
    "        _, (h, c) = self.lstm(x)    # h: (L, B, H)\n",
    "        out = self.fc(h[-1])        # 마지막 레이어 은닉 사용 (B, H) -> (B, 3)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23895,
     "status": "ok",
     "timestamp": 1750142124548,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "47_k6ozcg9x1",
    "outputId": "c2d0c0d9-98bb-40f9-bf06-bf56a79d8627"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: Train Loss 1.0987, Train Acc 0.3460, Val Loss 1.0951, Val Acc 0.3594, \n",
      "Epoch 2/100: Train Loss 1.0951, Train Acc 0.3735, Val Loss 1.0920, Val Acc 0.3573, \n",
      "Epoch 3/100: Train Loss 1.0916, Train Acc 0.3963, Val Loss 1.0885, Val Acc 0.4165, \n",
      "Epoch 4/100: Train Loss 1.0875, Train Acc 0.4519, Val Loss 1.0835, Val Acc 0.4841, \n",
      "Epoch 5/100: Train Loss 1.0815, Train Acc 0.4884, Val Loss 1.0755, Val Acc 0.4947, \n",
      "Epoch 6/100: Train Loss 1.0709, Train Acc 0.5063, Val Loss 1.0580, Val Acc 0.5264, \n",
      "Epoch 7/100: Train Loss 1.0103, Train Acc 0.5460, Val Loss 0.9119, Val Acc 0.6068, \n",
      "Epoch 8/100: Train Loss 0.9099, Train Acc 0.6228, Val Loss 0.8564, Val Acc 0.6448, \n",
      "Epoch 9/100: Train Loss 0.8446, Train Acc 0.6608, Val Loss 0.8696, Val Acc 0.6490, \n",
      "Epoch 10/100: Train Loss 1.0720, Train Acc 0.4439, Val Loss 1.0577, Val Acc 0.4778, \n",
      "Epoch 11/100: Train Loss 0.9016, Train Acc 0.6296, Val Loss 0.8597, Val Acc 0.6850, \n",
      "Epoch 12/100: Train Loss 0.7975, Train Acc 0.7127, Val Loss 0.7622, Val Acc 0.7040, \n",
      "Epoch 13/100: Train Loss 0.7448, Train Acc 0.7323, Val Loss 0.7258, Val Acc 0.7188, \n",
      "Epoch 14/100: Train Loss 0.6412, Train Acc 0.7497, Val Loss 0.6358, Val Acc 0.7188, \n",
      "Epoch 15/100: Train Loss 0.5695, Train Acc 0.7815, Val Loss 0.6180, Val Acc 0.7611, \n",
      "Epoch 16/100: Train Loss 0.5396, Train Acc 0.8164, Val Loss 0.5734, Val Acc 0.7738, \n",
      "Epoch 17/100: Train Loss 0.4685, Train Acc 0.8354, Val Loss 0.5279, Val Acc 0.7949, \n",
      "Epoch 18/100: Train Loss 0.4400, Train Acc 0.8476, Val Loss 0.4943, Val Acc 0.8076, \n",
      "Epoch 19/100: Train Loss 0.3892, Train Acc 0.8683, Val Loss 0.5565, Val Acc 0.7780, \n",
      "Epoch 20/100: Train Loss 0.5572, Train Acc 0.7767, Val Loss 0.5292, Val Acc 0.7949, \n",
      "Epoch 21/100: Train Loss 0.5015, Train Acc 0.8233, Val Loss 0.4748, Val Acc 0.8309, \n",
      "Epoch 22/100: Train Loss 0.3371, Train Acc 0.8984, Val Loss 0.4439, Val Acc 0.8182, \n",
      "Epoch 23/100: Train Loss 0.3049, Train Acc 0.8974, Val Loss 0.4489, Val Acc 0.8224, \n",
      "Epoch 24/100: Train Loss 0.4164, Train Acc 0.8439, Val Loss 0.8685, Val Acc 0.6152, \n",
      "Epoch 25/100: Train Loss 0.6428, Train Acc 0.6788, Val Loss 0.6252, Val Acc 0.6660, \n",
      "Epoch 26/100: Train Loss 0.4944, Train Acc 0.7735, Val Loss 0.5503, Val Acc 0.7569, \n",
      "Epoch 27/100: Train Loss 0.4586, Train Acc 0.8365, Val Loss 1.0700, Val Acc 0.5751, \n",
      "Epoch 28/100: Train Loss 0.6108, Train Acc 0.7831, Val Loss 0.4984, Val Acc 0.8097, \n",
      "Epoch 29/100: Train Loss 0.4255, Train Acc 0.8624, Val Loss 0.5027, Val Acc 0.8076, \n",
      "Epoch 30/100: Train Loss 0.3890, Train Acc 0.8725, Val Loss 0.4522, Val Acc 0.8288, \n",
      "Epoch 31/100: Train Loss 0.3533, Train Acc 0.8836, Val Loss 0.4308, Val Acc 0.8330, \n",
      "Epoch 32/100: Train Loss 0.3093, Train Acc 0.9037, Val Loss 0.3948, Val Acc 0.8499, \n",
      "Epoch 33/100: Train Loss 0.2476, Train Acc 0.9190, Val Loss 0.3504, Val Acc 0.8732, \n",
      "Epoch 34/100: Train Loss 0.2162, Train Acc 0.9286, Val Loss 0.3680, Val Acc 0.8605, \n",
      "Epoch 35/100: Train Loss 0.2042, Train Acc 0.9339, Val Loss 0.4005, Val Acc 0.8541, \n",
      "Epoch 36/100: Train Loss 0.1924, Train Acc 0.9349, Val Loss 0.3530, Val Acc 0.8710, \n",
      "Epoch 37/100: Train Loss 0.1593, Train Acc 0.9545, Val Loss 0.3265, Val Acc 0.8668, \n",
      "Epoch 38/100: Train Loss 0.1469, Train Acc 0.9587, Val Loss 0.3517, Val Acc 0.8605, \n",
      "Epoch 39/100: Train Loss 0.1379, Train Acc 0.9608, Val Loss 0.3373, Val Acc 0.8774, \n",
      "Epoch 40/100: Train Loss 0.1739, Train Acc 0.9487, Val Loss 0.3937, Val Acc 0.8795, \n",
      "Epoch 41/100: Train Loss 0.1306, Train Acc 0.9624, Val Loss 0.3257, Val Acc 0.8837, \n",
      "Epoch 42/100: Train Loss 0.1191, Train Acc 0.9688, Val Loss 0.3411, Val Acc 0.8732, \n",
      "Epoch 43/100: Train Loss 0.1023, Train Acc 0.9746, Val Loss 0.3462, Val Acc 0.8879, \n",
      "Epoch 44/100: Train Loss 0.1095, Train Acc 0.9693, Val Loss 0.3136, Val Acc 0.8985, \n",
      "Epoch 45/100: Train Loss 0.1206, Train Acc 0.9651, Val Loss 0.3239, Val Acc 0.8858, \n",
      "Epoch 46/100: Train Loss 0.1003, Train Acc 0.9757, Val Loss 0.3569, Val Acc 0.8753, \n",
      "Epoch 47/100: Train Loss 0.0830, Train Acc 0.9757, Val Loss 0.3258, Val Acc 0.8858, \n",
      "Epoch 48/100: Train Loss 0.0831, Train Acc 0.9788, Val Loss 0.3258, Val Acc 0.8837, \n",
      "Epoch 49/100: Train Loss 0.0793, Train Acc 0.9788, Val Loss 0.2924, Val Acc 0.9070, \n",
      "Epoch 50/100: Train Loss 0.0961, Train Acc 0.9730, Val Loss 0.3329, Val Acc 0.8901, \n",
      "Epoch 51/100: Train Loss 0.0860, Train Acc 0.9767, Val Loss 0.3375, Val Acc 0.8837, \n",
      "Epoch 52/100: Train Loss 0.0774, Train Acc 0.9772, Val Loss 0.3107, Val Acc 0.9049, \n",
      "Epoch 53/100: Train Loss 0.0636, Train Acc 0.9847, Val Loss 0.3319, Val Acc 0.8858, \n",
      "Epoch 54/100: Train Loss 0.0604, Train Acc 0.9831, Val Loss 0.2754, Val Acc 0.9027, \n",
      "Epoch 55/100: Train Loss 0.0530, Train Acc 0.9878, Val Loss 0.3061, Val Acc 0.9070, \n",
      "Epoch 56/100: Train Loss 0.0468, Train Acc 0.9878, Val Loss 0.3416, Val Acc 0.8901, \n",
      "Epoch 57/100: Train Loss 0.0646, Train Acc 0.9825, Val Loss 0.2822, Val Acc 0.9006, \n",
      "Epoch 58/100: Train Loss 0.0460, Train Acc 0.9899, Val Loss 0.3365, Val Acc 0.8964, \n",
      "Epoch 59/100: Train Loss 0.0370, Train Acc 0.9921, Val Loss 0.3346, Val Acc 0.9049, \n",
      "Epoch 60/100: Train Loss 0.0379, Train Acc 0.9910, Val Loss 0.3302, Val Acc 0.8858, \n",
      "Epoch 61/100: Train Loss 0.0377, Train Acc 0.9926, Val Loss 0.2881, Val Acc 0.9049, \n",
      "Epoch 62/100: Train Loss 0.0605, Train Acc 0.9820, Val Loss 0.3994, Val Acc 0.8837, \n",
      "Epoch 63/100: Train Loss 0.0550, Train Acc 0.9841, Val Loss 0.2589, Val Acc 0.9260, \n",
      "Epoch 64/100: Train Loss 0.0476, Train Acc 0.9889, Val Loss 0.2991, Val Acc 0.9070, \n",
      "Epoch 65/100: Train Loss 0.0305, Train Acc 0.9937, Val Loss 0.3039, Val Acc 0.9133, \n",
      "Epoch 66/100: Train Loss 0.0277, Train Acc 0.9937, Val Loss 0.2928, Val Acc 0.9154, \n",
      "Epoch 67/100: Train Loss 0.0262, Train Acc 0.9942, Val Loss 0.3558, Val Acc 0.9006, \n",
      "Epoch 68/100: Train Loss 0.0251, Train Acc 0.9931, Val Loss 0.3483, Val Acc 0.9091, \n",
      "Epoch 69/100: Train Loss 0.0256, Train Acc 0.9947, Val Loss 0.3450, Val Acc 0.9070, \n",
      "Epoch 70/100: Train Loss 0.0233, Train Acc 0.9942, Val Loss 0.3567, Val Acc 0.9049, \n",
      "Epoch 71/100: Train Loss 0.0377, Train Acc 0.9899, Val Loss 0.3121, Val Acc 0.9112, \n",
      "Epoch 72/100: Train Loss 0.0270, Train Acc 0.9947, Val Loss 0.2843, Val Acc 0.9133, \n",
      "Epoch 73/100: Train Loss 0.0523, Train Acc 0.9847, Val Loss 0.3155, Val Acc 0.9091, \n",
      "Epoch 74/100: Train Loss 0.0293, Train Acc 0.9942, Val Loss 0.2928, Val Acc 0.9133, \n",
      "Epoch 75/100: Train Loss 0.0208, Train Acc 0.9958, Val Loss 0.3115, Val Acc 0.9154, \n",
      "Epoch 76/100: Train Loss 0.0192, Train Acc 0.9958, Val Loss 0.3261, Val Acc 0.9049, \n",
      "Epoch 77/100: Train Loss 0.0182, Train Acc 0.9958, Val Loss 0.3316, Val Acc 0.9197, \n",
      "Epoch 78/100: Train Loss 0.0175, Train Acc 0.9974, Val Loss 0.3779, Val Acc 0.9133, \n",
      "Epoch 79/100: Train Loss 0.0169, Train Acc 0.9963, Val Loss 0.3566, Val Acc 0.9091, \n",
      "Epoch 80/100: Train Loss 0.0168, Train Acc 0.9968, Val Loss 0.3442, Val Acc 0.9154, \n",
      "Epoch 81/100: Train Loss 0.0160, Train Acc 0.9974, Val Loss 0.3785, Val Acc 0.9091, \n",
      "Epoch 82/100: Train Loss 0.0149, Train Acc 0.9974, Val Loss 0.3682, Val Acc 0.9091, \n",
      "Epoch 83/100: Train Loss 0.0146, Train Acc 0.9974, Val Loss 0.3811, Val Acc 0.9049, \n",
      "Epoch 84/100: Train Loss 0.0151, Train Acc 0.9974, Val Loss 0.3494, Val Acc 0.9175, \n",
      "Epoch 85/100: Train Loss 0.0253, Train Acc 0.9952, Val Loss 0.2822, Val Acc 0.9175, \n",
      "Epoch 86/100: Train Loss 0.0243, Train Acc 0.9958, Val Loss 0.3197, Val Acc 0.9175, \n",
      "Epoch 87/100: Train Loss 0.0152, Train Acc 0.9974, Val Loss 0.3254, Val Acc 0.9197, \n",
      "Epoch 88/100: Train Loss 0.0138, Train Acc 0.9979, Val Loss 0.3471, Val Acc 0.9197, \n",
      "Epoch 89/100: Train Loss 0.0129, Train Acc 0.9979, Val Loss 0.3713, Val Acc 0.9197, \n",
      "Epoch 90/100: Train Loss 0.0125, Train Acc 0.9979, Val Loss 0.3781, Val Acc 0.9175, \n",
      "Epoch 91/100: Train Loss 0.0126, Train Acc 0.9979, Val Loss 0.3847, Val Acc 0.9175, \n",
      "Epoch 92/100: Train Loss 0.1715, Train Acc 0.9386, Val Loss 0.5837, Val Acc 0.7273, \n",
      "Epoch 93/100: Train Loss 0.3124, Train Acc 0.8910, Val Loss 0.4282, Val Acc 0.8372, \n",
      "Epoch 94/100: Train Loss 0.2346, Train Acc 0.9291, Val Loss 0.3941, Val Acc 0.8710, \n",
      "Epoch 95/100: Train Loss 0.1442, Train Acc 0.9545, Val Loss 0.2951, Val Acc 0.9112, \n",
      "Epoch 96/100: Train Loss 0.0695, Train Acc 0.9788, Val Loss 0.2326, Val Acc 0.9239, \n",
      "Epoch 97/100: Train Loss 0.0506, Train Acc 0.9847, Val Loss 0.2475, Val Acc 0.9323, \n",
      "Epoch 98/100: Train Loss 0.0228, Train Acc 0.9963, Val Loss 0.2703, Val Acc 0.9345, \n",
      "Epoch 99/100: Train Loss 0.0157, Train Acc 0.9974, Val Loss 0.2781, Val Acc 0.9281, \n",
      "Epoch 100/100: Train Loss 0.0143, Train Acc 0.9968, Val Loss 0.2976, Val Acc 0.9218, \n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # cuda 또는 cpu\n",
    "embedding_dim = 100\n",
    "hidden_size = 128\n",
    "\n",
    "model = LSTMClassifier2(vocab_size, embedding_dim, embedding_matrix, hidden_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# 학습루프\n",
    "train_losses, train_accs = [], []\n",
    "val_losses, val_accs = [], []\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # 학습\n",
    "    model.train()\n",
    "    train_loss, train_correct, train_total = 0, 0, 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.detach().cpu().item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        train_correct += (pred == y_batch).sum().detach().cpu().item()\n",
    "        train_total += len(y_batch)\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = train_correct / train_total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    # 검증\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "\n",
    "            val_loss += loss.detach().cpu().item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            val_correct += (pred == y_batch).sum().detach().cpu().item()\n",
    "            val_total += len(y_batch)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "    # 출력(train_loss, val_loss)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}: '\n",
    "          f'Train Loss {train_loss:.4f}, '\n",
    "          f'Train Acc {train_acc:.4f}, '\n",
    "          f'Val Loss {val_loss:.4f}, '\n",
    "          f'Val Acc {val_acc:.4f}, ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1750142126818,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "-JkT4PSYg9x3",
    "outputId": "95531598-aa4b-4ba6-daa1-f95780ffa7a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     comp.graphics       0.94      0.93      0.93       202\n",
      "rec.sport.baseball       0.98      0.95      0.96       202\n",
      "         sci.space       0.90      0.95      0.92       187\n",
      "\n",
      "          accuracy                           0.94       591\n",
      "         macro avg       0.94      0.94      0.94       591\n",
      "      weighted avg       0.94      0.94      0.94       591\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "# - 정답, 모델 예측값을 가지고, classification_report 작성\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        pred = output.argmax(dim=1)\n",
    "\n",
    "        all_preds.extend(pred.detach().cpu().numpy())\n",
    "        all_labels.extend(y_batch.detach().cpu().numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=newsgroups.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLaznsT8lukE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNEca1JB5mJdMj6HFe4GY4d",
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
