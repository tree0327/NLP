{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "bbcIPWYG4Qj6",
            "metadata": {},
            "source": [
                "# RNN 기반 텍스트 분류기 (Text Classification)\n",
                "### 과목: NLP Text Classification\n",
                "---\n",
                "\n",
                "## 1. 딥러닝으로 텍스트를 분류해보자\n",
                "이전에는 나이브 베이즈 같은 통계적 방법을 썼다면, 이제는 **딥러닝(RNN/LSTM)**을 사용하여 분류해봅시다.\n",
                "RNN은 문장의 **\"순서 정보(Context)\"**를 읽을 수 있어서, 문맥을 더 잘 파악할 수 있습니다.\n",
                "\n",
                "**목표**:\n",
                "1. 뉴스 기사 데이터를 읽어온다.\n",
                "2. 컴퓨터가 이해하도록 **숫자(정수 인덱스)**로 바꾼다. (Tokenization)\n",
                "3. **LSTM** 모델을 만들어 학습시킨다.\n",
                "4. (심화) 이미 공부를 많이 한 **사전 학습 임베딩(Pre-trained Embedding)**을 가져와서 성능을 높여본다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6U_oVjNc4c3I",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. 데이터 로드\n",
                "# 20 Newsgroups 데이터셋 중 3가지 주제만 골라서 분류해봅시다.\n",
                "from sklearn.datasets import fetch_20newsgroups\n",
                "\n",
                "categories = ['comp.graphics', 'sci.space', 'rec.sport.baseball']\n",
                "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
                "\n",
                "X = newsgroups.data # 뉴스 본문 (리스트)\n",
                "y = newsgroups.target # 정답 라벨 (0, 1, 2)\n",
                "\n",
                "print(f\"찾아야 할 주제들: {newsgroups.target_names}\")\n",
                "print(f\"첫 번째 뉴스 본문 (앞 200자):\\n{X[0][:200]}...\")\n",
                "print(f\"첫 번째 정답 라벨: {y[0]} ({newsgroups.target_names[y[0]]})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "preprocessing_explain",
            "metadata": {},
            "source": [
                "## 2. 텍스트 전처리 (Preprocessing)\n",
                "딥러닝 모델은 글자를 못 읽습니다. 그래서 **숫자**로 바꿔줘야 합니다.\n",
                "1. **Tokenizer**: 빈도수가 높은 단어 순서대로 번호(Index)를 매깁니다. (예: the -> 1, apple -> 2)\n",
                "2. **Padding**: 모든 문장의 길이를 똑같이 맞춰줍니다. (짧으면 0을 채우고, 길면 자름)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "yXBtkCIp4H51",
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.preprocessing.text import Tokenizer\n",
                "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
                "\n",
                "# 설정값들\n",
                "vocab_size = 10000    # 사용할 단어 사전 크기 (가장 많이 쓰이는 10,000개만 사용)\n",
                "max_len = 200         # 문장의 최대 길이 (200 단어)\n",
                "\n",
                "# 1. 토크나이저 학습 (단어장 만들기)\n",
                "# oov_token='<OOV>': 모르는 단어(사전에 없는 단어)는 '<OOV>'라고 표시함\n",
                "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
                "tokenizer.fit_on_texts(X)\n",
                "\n",
                "# 2. 텍스트 -> 숫자 시퀀스 변환\n",
                "X_encoded = tokenizer.texts_to_sequences(X)\n",
                "\n",
                "# 3. 패딩 (길이 맞추기)\n",
                "X_padded = pad_sequences(X_encoded, maxlen=max_len)\n",
                "\n",
                "print(f\"전체 데이터 크기 (문서 개수, 문장 길이): {X_padded.shape}\")\n",
                "# 결과: (문서개수, 200) -> 모든 문서가 200개 숫자로 된 리스트로 변함"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0suBZZ3V6ZuL",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. PyTorch용 데이터셋 만들기\n",
                "import torch\n",
                "from sklearn.model_selection import train_test_split\n",
                "from torch.utils.data import TensorDataset, DataLoader\n",
                "\n",
                "# Train / Validation / Test 분리 (6:2:2 비율)\n",
                "X_train, X_test, y_train, y_test = train_test_split(torch.tensor(X_padded), torch.tensor(y), test_size=0.2, random_state=42)\n",
                "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
                "\n",
                "# DataLoader 생성 (배치 단위로 데이터를 묶어줌)\n",
                "# TensorDataset: 입력(X)과 정답(y)을 묶어주는 보따리\n",
                "# DataLoader: 보따리에서 데이터를 배치 사이즈만큼 꺼내주는 일꾼\n",
                "train_dataset = TensorDataset(X_train, y_train)\n",
                "val_dataset = TensorDataset(X_val, y_val)\n",
                "test_dataset = TensorDataset(X_test, y_test)\n",
                "\n",
                "batch_size = 64\n",
                "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # Train은 섞어야 좋음\n",
                "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
                "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "model_explain",
            "metadata": {},
            "source": [
                "## 3. LSTM 모델 설계\n",
                "**구조**: Embedding -> LSTM -> Linear (분류)\n",
                "\n",
                "1. **Embedding**: 숫자(1, 23, 49...)를 의미 있는 벡터(숫자 묶음)로 바꿉니다. (컴퓨터가 단어의 의미를 학습하는 공간)\n",
                "2. **LSTM**: 문장을 앞에서부터 읽으면서 문맥을 파악합니다.\n",
                "3. **Linear (FC)**: 읽은 내용을 바탕으로 3개 클래스(그래픽, 야구, 우주) 중 어디에 속하는지 점수를 냅니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "y7BjT5FY8Tnt",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn as nn\n",
                "\n",
                "class LSTMClassifier(nn.Module):\n",
                "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
                "        super().__init__()\n",
                "        # 1. 임베딩 층: 단어 개수(vocab_size) -> 벡터 크기(embedding_dim)로 변환\n",
                "        # padding_idx=0: 0번(패딩)은 학습하지 않음 (의미 없으니까)\n",
                "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
                "        \n",
                "        # 2. LSTM 층: 시계열 데이터 처리\n",
                "        # batch_first=True: 입력 데이터가 (batch, seq_len, input_size) 형태임\n",
                "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
                "        \n",
                "        # 3. 출력 층: 마지막 요약 정보를 보고 3개 카테고리로 분류\n",
                "        self.fc = nn.Linear(hidden_size, 3)\n",
                "\n",
                "    def forward(self, x):\n",
                "        # x: (배치크기, 문장길이)\n",
                "        x = self.embedding(x)       # (배치크기, 문장길이, 임베딩크기)\n",
                "        \n",
                "        # LSTM은 출력(output)과 마지막 상태(hidden, cell)를 반환함\n",
                "        # 여기서 h는 (층개수, 배치크기, 은닉크기) 형태\n",
                "        _, (h, c) = self.lstm(x)\n",
                "        \n",
                "        # 마지막 타임스텝의 은닉 상태(h[-1])를 가져와서 분류기에 넣음\n",
                "        # 문장을 끝까지 다 읽은 후의 요약 정보라고 보면 됨\n",
                "        out = self.fc(h[-1])\n",
                "        return out"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "-vO1L30O8Vf5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. 모델 학습 준비\n",
                "import torch.optim as optim\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"사용 디바이스: {device}\")\n",
                "\n",
                "embedding_dim = 100\n",
                "hidden_size = 128\n",
                "\n",
                "model = LSTMClassifier(vocab_size, embedding_dim, hidden_size).to(device)\n",
                "criterion = nn.CrossEntropyLoss() # 분류 문제의 채점표 (손실함수)\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001) # 최적화 기법 (공부 방법)\n",
                "\n",
                "# 학습 실행\n",
                "epochs = 20 # 시간 관계상 20 에포크만 합시다.\n",
                "\n",
                "for epoch in range(epochs):\n",
                "    model.train() # 학습 모드 ON\n",
                "    train_loss, train_acc = 0, 0\n",
                "    train_total = 0\n",
                "    train_correct = 0\n",
                "\n",
                "    for X_batch, y_batch in train_loader:\n",
                "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
                "        \n",
                "        optimizer.zero_grad()    # 이전 찌꺼기(기울기) 청소\n",
                "        output = model(X_batch)  # 예측\n",
                "        loss = criterion(output, y_batch) # 오차 계산\n",
                "        loss.backward()          # 어디서 틀렸는지 역추적 (역전파)\n",
                "        optimizer.step()         # 수정 (가중치 업데이트)\n",
                "\n",
                "        train_loss += loss.item()\n",
                "        # 정확도 계산\n",
                "        pred = output.argmax(dim=1)\n",
                "        train_correct += (pred == y_batch).sum().item()\n",
                "        train_total += len(y_batch)\n",
                "    \n",
                "    # 에포크마다 리포트 출력\n",
                "    avg_loss = train_loss / len(train_loader)\n",
                "    avg_acc = train_correct / train_total\n",
                "    print(f'Epoch {epoch+1}: Loss {avg_loss:.4f}, Accuracy {avg_acc:.4f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "clNCNefO9nBv",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. 최종 평가 (Test Set)\n",
                "from sklearn.metrics import classification_report\n",
                "\n",
                "model.eval() # 평가 모드 (성적표 받는 시간)\n",
                "all_preds, all_labels = [], []\n",
                "\n",
                "with torch.no_grad(): # 평가할 때는 공부(gradient 계산) 안 함\n",
                "    for X_batch, y_batch in test_loader:\n",
                "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
                "        output = model(X_batch)\n",
                "        pred = output.argmax(dim=1)\n",
                "        \n",
                "        all_preds.extend(pred.cpu().numpy())\n",
                "        all_labels.extend(y_batch.cpu().numpy())\n",
                "\n",
                "print(\"--- 최종 성적표 ---\")\n",
                "print(classification_report(all_labels, all_preds, target_names=newsgroups.target_names))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "pRcaUXF_gGi-",
            "metadata": {},
            "source": [
                "---\n",
                "## (심화) 사전 학습된 임베딩(Pre-trained Embedding) 사용하기\n",
                "내가 처음부터 단어의 의미(임베딩)를 배우는 것보다, 미리 **영어 공부를 많이 한 모델(FastText)**의 지식을 빌려오면 성능이 더 좋아지지 않을까요?\n",
                "- 마치 수능 영어를 칠 때, 알파벳부터 배우는 게 아니라 영단어장을 외우고 시작하는 것과 같습니다.\n",
                "\n",
                "**Note**: `gensim` 라이브러리와 `ted_en_fasttext.model` 파일이 필요합니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a9UBkjQlgQdh",
            "metadata": {},
            "outputs": [],
            "source": [
                "# gensim 설치 (없다면)\n",
                "# %pip install gensim"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "w8s1Ezq6hQlW",
            "metadata": {},
            "outputs": [],
            "source": [
                "from gensim.models import FastText\n",
                "import numpy as np\n",
                "\n",
                "# 1. 미리 학습된 모델 로드 (가정: 같은 폴더에 모델 파일이 있다고 가정)\n",
                "try:\n",
                "    fasttext_model = FastText.load('ted_en_fasttext.model')\n",
                "    print(\"사전 학습 모델 로드 성공!\")\n",
                "except:\n",
                "    print(\"모델 파일이 없습니다. 이 부분은 스킵하거나 모델을 다운로드해야 합니다.\")\n",
                "\n",
                "# 2. 내 단어장(tokenizer)에 맞는 임베딩 행렬 만들기\n",
                "if 'fasttext_model' in locals():\n",
                "    embedding_dim = fasttext_model.vector_size\n",
                "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
                "\n",
                "    count = 0\n",
                "    for word, i in tokenizer.word_index.items():\n",
                "        if i >= vocab_size: break\n",
                "        if word in fasttext_model.wv:\n",
                "            embedding_matrix[i] = fasttext_model.wv[word]\n",
                "            count += 1\n",
                "    \n",
                "    print(f\"{vocab_size}개 단어 중 {count}개의 단어 벡터를 가져왔습니다.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "-wiQxp4Ag9xz",
            "metadata": {},
            "outputs": [],
            "source": [
                "class LSTMClassifierWithPretrained(nn.Module):\n",
                "    def __init__(self, vocab_size, embedding_dim, embedding_matrix, hidden_size):\n",
                "        super().__init__()\n",
                "        # 1. 임베딩 층 선언\n",
                "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
                "        \n",
                "        # 2. **중요**: 사전 학습된 가중치(지식) 덮어쓰기\n",
                "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
                "        self.embedding.weight.requires_grad = True # False로 하면 지식 고정, True면 미세조정(Fine-tuning)\n",
                "\n",
                "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
                "        self.fc = nn.Linear(hidden_size, 3)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self.embedding(x)\n",
                "        _, (h, c) = self.lstm(x)\n",
                "        out = self.fc(h[-1])\n",
                "        return out"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "47_k6ozcg9x1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 모델 교체 후 재학습 (코드는 위와 동일하므로 생략하거나 재실행 가능)\n",
                "# model = LSTMClassifierWithPretrained(...).to(device)\n",
                "# ..."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "python3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}