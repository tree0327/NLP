{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "e1f82b7c",
            "metadata": {},
            "source": [
                "# 01. Seq2Seq ê¸°ë°˜ ê¸°ê³„ ë²ˆì—­ê¸° (Machine Translation)\n",
                "\n",
                "## ğŸ¤– Seq2Seq(Sequence to Sequence)ë€?\n",
                "**Seq2Seq**ëŠ” ì…ë ¥ëœ ì‹œí€€ìŠ¤(ë¬¸ì¥)ë¥¼ ëê¹Œì§€ ë“£ê³  ë¬¸ë§¥ì„ íŒŒì•…í•œ ë’¤, ë‹¤ë¥¸ ë„ë©”ì¸ì˜ ì‹œí€€ìŠ¤(ë²ˆì—­ë¬¸)ë¡œ ë³€í™˜í•´ì£¼ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤. ê¸°ê³„ ë²ˆì—­, ì±—ë´‡, í…ìŠ¤íŠ¸ ìš”ì•½ ë“±ì— ì•„ì£¼ ë„ë¦¬ ì“°ì´ëŠ” ê¸°ì´ˆì ì¸ ëª¨ë¸ì´ì—ìš”.\n",
                "\n",
                "### ğŸ§  í•µì‹¬ ì›ë¦¬: ì¸ì½”ë”(Encoder)ì™€ ë””ì½”ë”(Decoder)\n",
                "Seq2SeqëŠ” í¬ê²Œ ë‘ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤. í†µì—­ì‚¬ì— ë¹„ìœ í•´ì„œ ì„¤ëª…í•´ë³¼ê²Œìš”.\n",
                "\n",
                "1.  **ì¸ì½”ë” (Encoder - ë“£ëŠ” ì‚¬ëŒ)**: \n",
                "    - ì™¸êµ­ì–´(ì˜ì–´) ë¬¸ì¥ì„ ëê¹Œì§€ ì£¼ì˜ ê¹Šê²Œ ë“£ìŠµë‹ˆë‹¤. \n",
                "    - ë¬¸ì¥ì˜ ëª¨ë“  ë‹¨ì–´ë¥¼ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬í•œ ë’¤, ë¬¸ì¥ ì „ì²´ì˜ ì˜ë¯¸ë¥¼ ì••ì¶•í•œ **'ë‹¨ í•˜ë‚˜ì˜ ìƒê°(Context Vector)'**ì„ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤. \n",
                "    - ì´ Context Vectorì—ëŠ” ë¬¸ì¥ì˜ í•µì‹¬ ì •ë³´ê°€ ìˆ«ìë¡œ ìš”ì•½ë˜ì–´ ìˆì£ .\n",
                "\n",
                "2.  **ë””ì½”ë” (Decoder - ë§í•˜ëŠ” ì‚¬ëŒ)**: \n",
                "    - ì¸ì½”ë”ê°€ ê±´ë„¤ì¤€ **Context Vector(ì••ì¶•ëœ ìƒê°)**ë¥¼ ì²« ë‹¨ì„œë¡œ ë°›ìŠµë‹ˆë‹¤.\n",
                "    - ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨êµ­ì–´(í•œêµ­ì–´) ë¬¸ì¥ì„ í•œ ë‹¨ì–´ì”© ìƒì„±í•´ëƒ…ë‹ˆë‹¤.\n",
                "    - \"ì•„, ì• ë‹¨ì–´ê°€ 'ë‚˜ëŠ”'ì´ì—ˆìœ¼ë‹ˆê¹Œ ë‹¤ìŒì€ 'í•™êµì—'ê°€ ìì—°ìŠ¤ëŸ½ê² êµ°\"í•˜ë©° ë¬¸ë§¥ì„ ì´ì–´ê°‘ë‹ˆë‹¤.\n",
                "\n",
                "ì, ì´ì œ ì´ ë©‹ì§„ í†µì—­ì‚¬ë¥¼ í…ì„œí”Œë¡œìš°(TensorFlow)ë¡œ ì§ì ‘ êµ¬í˜„í•´ë´…ì‹œë‹¤!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "09c123d1",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.preprocessing.text import Tokenizer\n",
                "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
                "from tensorflow.keras.utils import to_categorical\n",
                "from tensorflow.keras import layers, models\n",
                "\n",
                "# ëœë¤ ì‹œë“œ ê³ ì • (ì¬í˜„ì„±ì„ ìœ„í•´)\n",
                "np.random.seed(42)\n",
                "tf.random.set_seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "52f654ab",
            "metadata": {},
            "source": [
                "## 1. ì‚¬ì „ í•™ìŠµëœ ì„ë² ë”© ë‹¤ìš´ë¡œë“œ (GloVe)\n",
                "ë‹¨ì–´ë¥¼ ìˆ«ìë¡œ ë°”ê¿€ ë•Œ, ë¯¸ë¦¬ í•™ìŠµëœ **GloVe** ì„ë² ë”©ì„ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì´ ë‹¨ì–´ ì‚¬ì´ì˜ ì˜ë¯¸ ê´€ê³„(ì˜ˆ: ì™•-ë‚¨ì = ì—¬ì™•-ì—¬ì)ë¥¼ ë” ì˜ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 100ì°¨ì› ë²¡í„°ë¥¼ ì‚¬ìš©í•  ê±°ì˜ˆìš”."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "b8a8f15d",
            "metadata": {},
            "outputs": [],
            "source": [
                "!gdown 1qk-14tgVHPXT5jfRUE4Ua2ji4EXwS022"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "783223ec",
            "metadata": {},
            "source": [
                "## 2. ë°ì´í„° ì¤€ë¹„ (ì˜ì–´ - í•œêµ­ì–´)\n",
                "ë²ˆì—­ê¸° í•™ìŠµì„ ìœ„í•´ ì˜ì–´ ë¬¸ì¥ê³¼ ê·¸ì— ëŒ€ì‘í•˜ëŠ” í•œêµ­ì–´ ë²ˆì—­ë¬¸ ìŒì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
                "ì—¬ê¸°ì„œëŠ” `fra.txt` (ì›ë˜ í”„ë‘ìŠ¤ì–´ ì˜ˆì œì§€ë§Œ ì—¬ê¸°ì„  í•œêµ­ì–´ ë°ì´í„°ë¡œ ê°€ì •í•˜ê³  ì²˜ë¦¬) ê°™ì€ í…ìŠ¤íŠ¸ íŒŒì¼ì´ë‚˜ CSVë¥¼ ì½ì–´ì˜µë‹ˆë‹¤. í¸ì˜ìƒ ë°ì´í„°ë¥¼ ì§ì ‘ ì •ì˜í•˜ê±°ë‚˜ ë¡œë“œí•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "62e920d3",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ì˜ˆì œ ë°ì´í„°ì…‹ ë¡œë“œ (ì‹¤ì œ íŒŒì¼ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì • í•„ìš”)\n",
                "# ì—¬ê¸°ì„œëŠ” í•™ìŠµ íë¦„ì„ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ì†ŒëŸ‰ì˜ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •í•˜ê² ìŠµë‹ˆë‹¤.\n",
                "# ì‹¤ì œë¡œëŠ” ìˆ˜ë§Œ ê°œì˜ ë¬¸ì¥ ìŒì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
                "\n",
                "# ë°ì´í„° ì˜ˆì‹œ (ì˜ì–´, í•œêµ­ì–´)\n",
                "raw_data = [\n",
                "    (\"I go.\", \"ë‚˜ëŠ” ê°„ë‹¤.\"),\n",
                "    (\"He runs.\", \"ê·¸ëŠ” ë‹¬ë¦°ë‹¤.\"),\n",
                "    (\"She eats.\", \"ê·¸ë…€ëŠ” ë¨¹ëŠ”ë‹¤.\"),\n",
                "    (\"I am a student.\", \"ë‚˜ëŠ” í•™ìƒì´ë‹¤.\"),\n",
                "    (\"You represent.\", \"ë„ˆëŠ” ëŒ€í‘œí•œë‹¤.\"),\n",
                "    (\"Do you like singing?\", \"ë…¸ë˜í•˜ëŠ” ê±° ì¢‹ì•„í•´ìš”?\"),\n",
                "    (\"Tom could've been seriously injured.\", \"í†°ì€ ì‹¬ê°í•œ ë¶€ìƒì„ ë‹¹í•  ìˆ˜ë„ ìˆì—ˆë‹¤.\"),\n",
                "    (\"This is my school.\", \"ì´ê³³ì€ ë‚´ í•™êµë‹¤.\"),\n",
                "    (\"Social media is a waste of time.\", \"ì†Œì…œ ë¯¸ë””ì–´ëŠ” ì‹œê°„ ë‚­ë¹„ì•¼.\"),\n",
                "    (\"I'm really sorry.\", \"ì •ë§ ë¯¸ì•ˆí•´.\"),\n",
                "    (\"My hair is black.\", \"ë‚´ ë¨¸ë¦¬ì¹´ë½ì€ ê²€ì€ìƒ‰ì´ì•¼.\"),\n",
                "    (\"French is interesting.\", \"í”„ë‘ìŠ¤ì–´ëŠ” ì¬ë¯¸ìˆì–´.\"),\n",
                "    (\"I like bus.\", \"ë‚˜ëŠ” ë²„ìŠ¤ë¥¼ ì¢‹ì•„í•´.\"),\n",
                "    (\"Let's study\", \"ê³µë¶€í•˜ì.\"),\n",
                "    (\"I ate sandwich\", \"ë‚˜ëŠ” ìƒŒë“œìœ„ì¹˜ë¥¼ ë¨¹ì—ˆì–´.\")\n",
                "]\n",
                "\n",
                "# ë°ì´í„° ë¶„ë¦¬\n",
                "eng_mn = [x[0] for x in raw_data]\n",
                "kor_mn = [x[1] for x in raw_data]\n",
                "\n",
                "# ë°ì´í„° í™•ì¸\n",
                "print(f\"ì˜ì–´ ë¬¸ì¥ ê°œìˆ˜: {len(eng_mn)}\")\n",
                "print(f\"í•œêµ­ì–´ ë¬¸ì¥ ê°œìˆ˜: {len(kor_mn)}\")\n",
                "print(eng_mn[:3])\n",
                "print(kor_mn[:3])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2bc12102",
            "metadata": {},
            "source": [
                "### 2-1. íŠ¹ìˆ˜ í† í° ì¶”ê°€ (<sos>, <eos>)\n",
                "ë””ì½”ë”(ë²ˆì—­ì„ ìƒì„±í•˜ëŠ” ë¶€ë¶„)ëŠ” ì–´ë””ì„œ ë§ì„ ì‹œì‘í•˜ê³ , ì–´ë””ì„œ ëë§ºì–´ì•¼ í• ì§€ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤.\n",
                "ê·¸ë˜ì„œ í•œêµ­ì–´ ë¬¸ì¥(Target)ì—ëŠ” ì•ë’¤ë¡œ **íŠ¹ìˆ˜ ì‹ í˜¸**ë¥¼ ë¶™ì—¬ì¤ë‹ˆë‹¤.\n",
                "\n",
                "*   **`<sos>` (Start Of Sequence)**: \"ì, ì´ì œ í†µì—­ ì‹œì‘í•œë‹¤!\" (ì‹œì‘ ì‹ í˜¸)\n",
                "*   **`<eos>` (End Of Sequence)**: \"ì—¬ê¸°ê¹Œì§€ê°€ ëì´ì•¼.\" (ì¢…ë£Œ ì‹ í˜¸)\n",
                "\n",
                "**í›ˆë ¨í•  ë•Œ ë°ì´í„° í˜•íƒœ:**\n",
                "*   **ë””ì½”ë” ì…ë ¥ (Input)**: `<sos>` ë‚˜ëŠ” í•™êµì— ê°„ë‹¤ -> (í˜„ì¬ ìƒíƒœë¥¼ ë³´ê³  ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ìš©)\n",
                "*   **ë””ì½”ë” ì •ë‹µ (Target)**: ë‚˜ëŠ” í•™êµì— ê°„ë‹¤ `<eos>` -> (ëª¨ë¸ì´ ë§ì¶°ì•¼ í•  ì •ë‹µ)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "8472f8d3",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ë””ì½”ë” ì…ë ¥: <sos>ë¡œ ì‹œì‘\n",
                "kor_inputs = [\"<sos> \" + sent for sent in kor_mn]\n",
                "# ë””ì½”ë” ì •ë‹µ(íƒ€ê²Ÿ): <eos>ë¡œ ëë‚¨\n",
                "kor_targets = [sent + \" <eos>\" for sent in kor_mn]\n",
                "\n",
                "eng_inputs = eng_mn  # ì¸ì½”ë” ì…ë ¥ì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
                "\n",
                "print(\"Decoder Input:\", kor_inputs[:3])\n",
                "print(\"Decoder Target:\", kor_targets[:3])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1804f982",
            "metadata": {},
            "source": [
                "## 3. ì •ìˆ˜ ì¸ì½”ë”© (Integer Encoding)\n",
                "ì»´í“¨í„°ëŠ” 'Apple', 'ì‚¬ê³¼' ê°™ì€ ê¸€ìë¥¼ ì´í•´í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ **ë‹¨ì–´ ì§‘í•©(Vocabulary)**ì„ ë§Œë“¤ê³ , ê° ë‹¨ì–´ë§ˆë‹¤ ê³ ìœ í•œ **ë²ˆí˜¸(ID)**ë¥¼ ë¶€ì—¬í•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "c1f72e91",
            "metadata": {},
            "outputs": [],
            "source": [
                "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
                "VOCAB_SIZE = 10000  # ì‚¬ìš©í•  ìµœëŒ€ ë‹¨ì–´ ìˆ˜\n",
                "\n",
                "# --- ì˜ì–´ í† í¬ë‚˜ì´ì € (Encoder Input) ---\n",
                "eng_tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token='<OOV>')\n",
                "eng_tokenizer.fit_on_texts(eng_inputs)          # ì˜ë¬¸ ë°ì´í„°ë¡œ ë‹¨ì–´ì¥ ìƒì„±\n",
                "eng_inputs_seq = eng_tokenizer.texts_to_sequences(eng_inputs)  # í…ìŠ¤íŠ¸ -> ìˆ«ì ì‹œí€€ìŠ¤ ë³€í™˜\n",
                "\n",
                "# --- í•œêµ­ì–´ í† í¬ë‚˜ì´ì € (Decoder Input/Target) ---\n",
                "# filters='' : <sos>, <eos> ê°™ì€ íŠ¹ìˆ˜ ë¬¸ìê°€ ê±¸ëŸ¬ì§€ì§€ ì•Šë„ë¡ í•„í„°ë§ ì œê±°\n",
                "kor_tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token='<OOV>', filters='') \n",
                "kor_tokenizer.fit_on_texts(kor_inputs + kor_targets)   # í•œêµ­ì–´ ì „ì²´ ë°ì´í„°ë¡œ ë‹¨ì–´ì¥ ìƒì„±\n",
                "\n",
                "kor_inputs_seq = kor_tokenizer.texts_to_sequences(kor_inputs)\n",
                "kor_targets_seq = kor_tokenizer.texts_to_sequences(kor_targets)\n",
                "\n",
                "print(\"ì˜ì–´ ë‹¨ì–´ì¥ í¬ê¸°:\", len(eng_tokenizer.word_index))\n",
                "print(\"í•œêµ­ì–´ ë‹¨ì–´ì¥ í¬ê¸°:\", len(kor_tokenizer.word_index))\n",
                "\n",
                "# ì‹¤ì œ ì‚¬ìš©í•  ë‹¨ì–´ ìˆ˜ í™•ì¸\n",
                "eng_num_words = min(VOCAB_SIZE, len(eng_tokenizer.word_index))\n",
                "kor_num_words = min(VOCAB_SIZE, len(kor_tokenizer.word_index))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2d94a10f",
            "metadata": {},
            "source": [
                "## 4. íŒ¨ë”© (Padding) - ê¸¸ì´ ë§ì¶”ê¸°\n",
                "ì‹ ê²½ë§ì€ ì…ë ¥ ê¸¸ì´ë¥¼ ë˜‘ê°™ì´ ë§ì¶°ì£¼ëŠ” ê²ƒì„ ì¢‹ì•„í•©ë‹ˆë‹¤. ë¬¸ì¥ ê¸¸ì´ê°€ ë‹¤ë¥´ë©´ ì§§ì€ ë¬¸ì¥ì˜ ë¹ˆ ê³µê°„ì„ **0**ìœ¼ë¡œ ì±„ì›Œì¤ë‹ˆë‹¤.\n",
                "\n",
                "### âš ï¸ ì¤‘ìš”: íŒ¨ë”© ìœ„ì¹˜ (Pre vs Post)\n",
                "*   **Encoder Input (ì˜ì–´)**: `padding='pre'` (ì•ì— 0 ì±„ì›€)\n",
                "    - LSTM ê°™ì€ ìˆœí™˜ì‹ ê²½ë§ì€ ë§ˆì§€ë§‰ ì…ë ¥ì˜ ì˜í–¥ì„ ê°€ì¥ í¬ê²Œ ë°›ìŠµë‹ˆë‹¤. ë²ˆì—­ì—ì„œ ë³´í†µ ë¬¸ì¥ì˜ ëë¶€ë¶„(ë™ì‚¬ ë“±)ì´ ì¤‘ìš”í•˜ê±°ë‚˜, ì…ë ¥ì˜ ëê³¼ ì¶œë ¥ì˜ ì‹œì‘ ê±°ë¦¬ë¥¼ ì¢íˆê¸° ìœ„í•´ ì¸ì½”ë”ëŠ” ì•ì— íŒ¨ë”©ì„ ë„£ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.\n",
                "*   **Decoder Input (í•œêµ­ì–´)**: `padding='post'` (ë’¤ì— 0 ì±„ì›€)\n",
                "    - ë””ì½”ë”ëŠ” `<sos>`ë¶€í„° ì‹œì‘í•´ì„œ ìˆœì„œëŒ€ë¡œ ë¬¸ì¥ì„ ìƒì„±í•´ì•¼ í•˜ë¯€ë¡œ, ë¬¸ì¥ì´ ëë‚˜ê³  ë’¤ìª½ì— ë¹ˆ ê³µê°„ì´ ìˆì–´ì•¼ ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "cb1c3d19",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´ ì„¤ì • (ì—¬ê¸°ì„  ë°ì´í„°ì— ë§ì¶° ìë™ ê³„ì‚°)\n",
                "eng_max_len = max([len(seq) for seq in eng_inputs_seq])\n",
                "kor_max_len = max([len(seq) for seq in kor_inputs_seq])\n",
                "\n",
                "print(f\"ì˜ì–´ ìµœëŒ€ ê¸¸ì´: {eng_max_len}, í•œêµ­ì–´ ìµœëŒ€ ê¸¸ì´: {kor_max_len}\")\n",
                "\n",
                "# íŒ¨ë”© ì ìš©\n",
                "eng_inputs_padded = pad_sequences(eng_inputs_seq, maxlen=eng_max_len, padding='pre')   # ì¸ì½”ë”ëŠ” ì•ìª½ íŒ¨ë”©\n",
                "kor_inputs_padded = pad_sequences(kor_inputs_seq, maxlen=kor_max_len, padding='post')  # ë””ì½”ë”ëŠ” ë’¤ìª½ íŒ¨ë”©\n",
                "kor_targets_padded = pad_sequences(kor_targets_seq, maxlen=kor_max_len, padding='post')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "92f72a12",
            "metadata": {},
            "source": [
                "## 5. GloVe ì„ë² ë”© ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±\n",
                "ì‚¬ì „ í•™ìŠµëœ GloVe ë²¡í„°ë¥¼ ê°€ì ¸ì™€ì„œ, ìš°ë¦¬ ë‹¨ì–´ì¥ì— ìˆëŠ” ë‹¨ì–´ë“¤ì— ë§¤ì¹­ì‹œì¼œì¤ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë¸ì´ ì²˜ìŒë¶€í„° ë§¨ë•…ì— í—¤ë”©í•˜ì§€ ì•Šê³ , ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ì–´ëŠ ì •ë„ ì•Œê³  ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "a92c730e",
            "metadata": {},
            "outputs": [],
            "source": [
                "EMBEDDING_DIM = 100\n",
                "\n",
                "# GloVe íŒŒì¼ ì½ì–´ì„œ ë”•ì…”ë„ˆë¦¬ë¡œ ì €ì¥\n",
                "embeddings_index = {}\n",
                "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
                "    for line in f:\n",
                "        values = line.split()\n",
                "        word = values[0]\n",
                "        coefs = np.asarray(values[1:], dtype='float32')\n",
                "        embeddings_index[word] = coefs\n",
                "\n",
                "print(f'{len(embeddings_index)}ê°œì˜ GloVe ë‹¨ì–´ ë²¡í„° ë¡œë“œ ì™„ë£Œ.')\n",
                "\n",
                "# ìš°ë¦¬ ì˜ì–´ ë‹¨ì–´ì¥ì— ë§ì¶° ì„ë² ë”© ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±\n",
                "en_embedding_matrix = np.zeros((eng_num_words + 1, EMBEDDING_DIM))\n",
                "for word, i in eng_tokenizer.word_index.items():\n",
                "    if i > eng_num_words:\n",
                "        continue\n",
                "    embedding_vector = embeddings_index.get(word)\n",
                "    if embedding_vector is not None:\n",
                "        en_embedding_matrix[i] = embedding_vector"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3a0b1291",
            "metadata": {},
            "source": [
                "## 6. Seq2Seq ëª¨ë¸ êµ¬ì¶• (Training Model)\n",
                "\n",
                "### 6-1. ì¸ì½”ë” (Encoder)\n",
                "ì¸ì½”ë”ì˜ ì—­í• ì€ ë¬¸ì¥ì„ ëê¹Œì§€ ì½ê³  **'Context Vector'**ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
                "LSTM ë ˆì´ì–´ì—ì„œ **`return_state=True`**ê°€ í•µì‹¬ì…ë‹ˆë‹¤! \n",
                "ì›ë˜ LSTMì€ ë§¤ ì‹œì ì˜ ì¶œë ¥(Output)ì„ ë‚´ë³´ë‚´ì§€ë§Œ, ì¸ì½”ë”ëŠ” ì¶œë ¥ê°’ì€ í•„ìš” ì—†ê³  **ë§ˆì§€ë§‰ ë‚´ë¶€ ìƒíƒœ(Hidden State, Cell State)**ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ ë‘ ìƒíƒœ ê°’ì´ ë°”ë¡œ ë¬¸ì¥ì˜ ì—‘ê¸°ìŠ¤ì¸ Context Vectorì…ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "f5d1382b",
            "metadata": {},
            "outputs": [],
            "source": [
                "LATENT_DIM = 256  # ì€ë‹‰ ìƒíƒœ í¬ê¸° (Context Vector ì°¨ì›)\n",
                "\n",
                "# --- Encoder ---\n",
                "encoder_inputs = layers.Input(shape=(eng_max_len,), name='Encoder_Input')\n",
                "\n",
                "# GloVe ì„ë² ë”© ì ìš© (trainable=Trueë¡œ ë‘ë©´ ë¯¸ì„¸ ì¡°ì • ê°€ëŠ¥)\n",
                "encoder_embedding = layers.Embedding(eng_num_words + 1, \n",
                "                                     EMBEDDING_DIM, \n",
                "                                     weights=[en_embedding_matrix], \n",
                "                                     trainable=True,\n",
                "                                     mask_zero=True, # 0 íŒ¨ë”© ë¬´ì‹œ\n",
                "                                     name='Encoder_Embedding')(encoder_inputs)\n",
                "\n",
                "# LSTM (return_state=True í•„ìˆ˜!)\n",
                "# encoder_outputs: ë§¤ ì‹œì ì˜ ì¶œë ¥ (ì—¬ê¸°ì„  ì•ˆ ì”€)\n",
                "# state_h: ë§ˆì§€ë§‰ Hidden State (ë‹¨ê¸° ê¸°ì–µ)\n",
                "# state_c: ë§ˆì§€ë§‰ Cell State (ì¥ê¸° ê¸°ì–µ)\n",
                "encoder_outputs, state_h, state_c = layers.LSTM(LATENT_DIM, \n",
                "                                                return_state=True, \n",
                "                                                name='Encoder_LSTM')(encoder_embedding)\n",
                "\n",
                "# ì¸ì½”ë”ì˜ ìµœì¢… ìƒíƒœ (Context Vector)\n",
                "encoder_states = [state_h, state_c]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "58c21a4f",
            "metadata": {},
            "source": [
                "### 6-2. ë””ì½”ë” (Decoder)\n",
                "ë””ì½”ë”ëŠ” **'Teacher Forcing(êµì‚¬ ê°•ìš”)'** ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.\n",
                "ì •ë‹µ(`<sos>` ë‚˜ëŠ” í•™êµì— ê°„ë‹¤)ì„ ì…ë ¥ìœ¼ë¡œ ì£¼ë©´ì„œ, ë‹¤ìŒ ë‹¨ì–´(ë‚˜ëŠ” í•™êµì— ê°„ë‹¤ `<eos>`)ë¥¼ ë§ì¶”ë„ë¡ í›ˆë ¨ì‹œí‚µë‹ˆë‹¤.\n",
                "\n",
                "1.  **`initial_state`**: ë””ì½”ë”ì˜ ì²« ì‹œì‘ ìƒíƒœë¥¼ **ì¸ì½”ë”ì˜ ë§ˆì§€ë§‰ ìƒíƒœ(Context Vector)**ë¡œ ì„¤ì •í•©ë‹ˆë‹¤. ì¦‰, ì¸ì½”ë”ê°€ ì½ì€ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ê±°ì£ .\n",
                "2.  **`return_sequences=True`**: ë””ì½”ë”ëŠ” ë²ˆì—­ë¬¸ì„ í•œ ë‹¨ì–´ì”© ëª¨ë‘ ìƒì„±í•´ì•¼ í•˜ë¯€ë¡œ, ëª¨ë“  ì‹œì ì˜ ì¶œë ¥ê°’ì„ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.\n",
                "3.  **`return_state=True`**: í•™ìŠµ ë• í•„ìš” ì—†ì§€ë§Œ, ë‚˜ì¤‘ì— ì¶”ë¡ (Inference) ë‹¨ê³„ì—ì„œ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ ìœ„í•´ í˜„ì¬ ìƒíƒœë¥¼ ë„˜ê²¨ë°›ì•„ì•¼ í•˜ë¯€ë¡œ ë¯¸ë¦¬ ì„¤ì •í•´ë‘¡ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "93b2a24c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Decoder ---\n",
                "decoder_inputs = layers.Input(shape=(kor_max_len,), name='Decoder_Input')\n",
                "\n",
                "# ë””ì½”ë” ì„ë² ë”© (ì²˜ìŒë¶€í„° í•™ìŠµ)\n",
                "decoder_embedding_layer = layers.Embedding(kor_num_words + 1, \n",
                "                                           EMBEDDING_DIM, \n",
                "                                           mask_zero=True,\n",
                "                                           name='Decoder_Embedding')\n",
                "decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
                "\n",
                "# ë””ì½”ë” LSTM\n",
                "# initial_state=encoder_states : ì¸ì½”ë”ì˜ ê¸°ì–µì„ ì´ì–´ë°›ìŒ\n",
                "# return_sequences=True : ëª¨ë“  ì‹œì ì˜ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•´ì•¼ í•¨\n",
                "decoder_lstm = layers.LSTM(LATENT_DIM, \n",
                "                           return_sequences=True, \n",
                "                           return_state=True, \n",
                "                           name='Decoder_LSTM')\n",
                "\n",
                "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
                "\n",
                "# ì¶œë ¥ì¸µ (ë‹¤ì¤‘ ë¶„ë¥˜)\n",
                "decoder_dense = layers.Dense(kor_num_words + 1, activation='softmax', name='Decoder_Output')\n",
                "decoder_outputs = decoder_dense(decoder_outputs)\n",
                "\n",
                "# --- ì „ì²´ ëª¨ë¸ ì •ì˜ ---\n",
                "# ì…ë ¥: [ì˜ì–´(ì¸ì½”ë”), í•œêµ­ì–´(ë””ì½”ë”)] -> ì¶œë ¥: [í•œêµ­ì–´(ì •ë‹µ)]\n",
                "model = models.Model([encoder_inputs, decoder_inputs], decoder_outputs, name='Seq2Seq_Training')\n",
                "\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "273a5a81",
            "metadata": {},
            "source": [
                "## 7. ëª¨ë¸ í•™ìŠµ (Training)\n",
                "ì—¬ê¸°ì„œëŠ” `sparse_categorical_crossentropy`ë¥¼ ì‚¬ìš©í•´ ì›-í•« ì¸ì½”ë”© ì—†ì´ ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë°”ë¡œ ì˜¤ì°¨ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "ef281b37",
            "metadata": {},
            "outputs": [],
            "source": [
                "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
                "\n",
                "# í•™ìŠµ ì‹œì‘\n",
                "history = model.fit([eng_inputs_padded, kor_inputs_padded], \n",
                "                    kor_targets_padded,\n",
                "                    batch_size=32,\n",
                "                    epochs=50,\n",
                "                    validation_split=0.2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "4a73e351",
            "metadata": {},
            "outputs": [],
            "source": [
                "# í•™ìŠµ ê²°ê³¼ ì‹œê°í™”\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(history.history['loss'], label='Train Loss')\n",
                "plt.plot(history.history['val_loss'], label='Val Loss')\n",
                "plt.title('Loss')\n",
                "plt.legend()\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
                "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
                "plt.title('Accuracy')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9d182b8c",
            "metadata": {},
            "source": [
                "## 8. ì¶”ë¡  ëª¨ë¸ (Inference Model) êµ¬ì„±\n",
                "í•™ìŠµ ëª¨ë¸ê³¼ ì¶”ë¡  ëª¨ë¸ì€ ë‹¤ë¦…ë‹ˆë‹¤! \n",
                "í•™ìŠµ ë• ì •ë‹µì„ í•œêº¼ë²ˆì— ë„£ì–´ì¤¬ì§€ë§Œ(Teacher Forcing), ì‹¤ì œ ë²ˆì—­ ë•ŒëŠ” **ì´ì „ ì‹œì ì— ëª¨ë¸ì´ ë±‰ì€ ë‹¨ì–´ë¥¼ ë‹¤ì‹œ ë‹¤ìŒ ì‹œì ì˜ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì£¼ëŠ” ë£¨í”„(Loop)**ë¥¼ ëŒì•„ì•¼ í•©ë‹ˆë‹¤.\n",
                "\n",
                "ë”°ë¼ì„œ **ì¸ì½”ë”**ì™€ **ë””ì½”ë”**ë¥¼ ë¶„ë¦¬í•´ì„œ ê°ê° ë³„ë„ ëª¨ë¸ë¡œ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8c3b129a",
            "metadata": {},
            "source": [
                "### 8-1. ì¸ì½”ë” ì¶”ë¡  ëª¨ë¸\n",
                "ì˜ì–´ ë¬¸ì¥ì„ ë„£ìœ¼ë©´ **Context Vector(ìƒíƒœë“¤)**ë§Œ ë±‰ì–´ë‚´ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. ì´ ìƒíƒœê°’ë“¤ì„ ë””ì½”ë”ì˜ ì´ˆê¸° ì—°ë£Œë¡œ ì“¸ ê²ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "18f9c1d2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ì…ë ¥: ì˜ì–´ ë¬¸ì¥\n",
                "# ì¶œë ¥: ì¸ì½”ë” ìƒíƒœ (h, c)\n",
                "encoder_model = models.Model(encoder_inputs, encoder_states)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "312c9b2d",
            "metadata": {},
            "source": [
                "### 8-2. ë””ì½”ë” ì¶”ë¡  ëª¨ë¸\n",
                "ì´ ëª¨ë¸ì€ ë§¤ ìŠ¤í…ë§ˆë‹¤ í•˜ë‚˜ì”© ì‹¤í–‰ë©ë‹ˆë‹¤.\n",
                "1.  **ì…ë ¥**: í˜„ì¬ ë‹¨ì–´ í† í° + ì´ì „ ì‹œì ì˜ ìƒíƒœ(h, c)\n",
                "2.  **ë™ì‘**: ì…ë ¥ë°›ì€ ìƒíƒœë¥¼ ì´ˆê¸° ìƒíƒœë¡œ ì„¸íŒ…í•˜ê³  LSTMì„ ëŒë¦¼\n",
                "3.  **ì¶œë ¥**: ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ í™•ë¥  + í˜„ì¬ ì‹œì ì˜ ìƒˆë¡œìš´ ìƒíƒœ(h, c)\n",
                "\n",
                "ì´ ìƒˆë¡œìš´ ìƒíƒœ(h, c)ëŠ” ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•  ë•Œ ë‹¤ì‹œ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°‘ë‹ˆë‹¤. (ìˆœí™˜ êµ¬ì¡°)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "713a29b4",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. ë””ì½”ë”ê°€ ë°›ì„ ì´ì „ ì‹œì ì˜ ìƒíƒœë“¤ ì •ì˜ (Input Layer)\n",
                "decoder_state_input_h = layers.Input(shape=(LATENT_DIM,))\n",
                "decoder_state_input_c = layers.Input(shape=(LATENT_DIM,))\n",
                "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
                "\n",
                "# 2. í•™ìŠµëœ ì„ë² ë”© ë ˆì´ì–´ ê°€ì ¸ì˜¤ê¸° (ì¬ì‚¬ìš©)\n",
                "# ì¶”ë¡  ë• í•œ ë‹¨ì–´ì”© ë“¤ì–´ì˜¤ë¯€ë¡œ shapeëŠ” (1, 1)\n",
                "decoder_inputs_single = layers.Input(shape=(1,))\n",
                "decoder_embedding_inference = decoder_embedding_layer(decoder_inputs_single)\n",
                "\n",
                "# 3. í•™ìŠµëœ LSTM ë ˆì´ì–´ ê°€ì ¸ì˜¤ê¸° (ì¬ì‚¬ìš©)\n",
                "decoder_outputs_inference, state_h_inf, state_c_inf = decoder_lstm(\n",
                "    decoder_embedding_inference, initial_state=decoder_states_inputs\n",
                ")\n",
                "decoder_states_inference = [state_h_inf, state_c_inf]\n",
                "\n",
                "# 4. í•™ìŠµëœ Dense ë ˆì´ì–´ ê°€ì ¸ì˜¤ê¸° (ì¬ì‚¬ìš©)\n",
                "decoder_outputs_inference = decoder_dense(decoder_outputs_inference)\n",
                "\n",
                "# 5. ëª¨ë¸ ìƒì„±\n",
                "# ì…ë ¥: [í˜„ì¬ ë‹¨ì–´, ì§€ë‚œ h, ì§€ë‚œ c]\n",
                "# ì¶œë ¥: [ë‹¤ìŒ ë‹¨ì–´ í™•ë¥ , í˜„ì¬ h, í˜„ì¬ c]\n",
                "decoder_inference_model = models.Model(\n",
                "    [decoder_inputs_single] + decoder_states_inputs,\n",
                "    [decoder_outputs_inference] + decoder_states_inference\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "528c11aa",
            "metadata": {},
            "source": [
                "## 9. ì‹¤ì œ ë²ˆì—­ê¸° êµ¬ë™ (Translate Function)\n",
                "ì´ì œ ë§Œë“¤ì–´ë‘” ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ ì—°ê²°í•´ì„œ ë²ˆì—­ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
                "1.  **ì¸ì½”ë”**ê°€ ì˜ì–´ ë¬¸ì¥ì„ Context Vectorë¡œ ì••ì¶•.\n",
                "2.  **ë””ì½”ë”**ì— ì²« ì…ë ¥ìœ¼ë¡œ `<sos>`ì™€ Context Vectorë¥¼ ì£¼ì….\n",
                "3.  ë””ì½”ë”ê°€ ë‚´ë±‰ì€ ë‹¨ì–´ë¥¼ ê²°ê³¼ì— ì¶”ê°€í•˜ê³ , ì´ë²ˆì— ë‚´ë±‰ì€ ìƒíƒœë¥¼ ì €ì¥.\n",
                "4.  ë‚´ë±‰ì€ ë‹¨ì–´ì™€ ì €ì¥í•œ ìƒíƒœë¥¼ ë‹¤ì‹œ ë””ì½”ë” ì…ë ¥ìœ¼ë¡œ ì‚¬ìš© (ë¬´í•œ ë£¨í”„).\n",
                "5.  `<eos>`ê°€ ë‚˜ì˜¤ê±°ë‚˜ ìµœëŒ€ ê¸¸ì´ì— ë„ë‹¬í•˜ë©´ ë©ˆì¶¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "e4c1928b",
            "metadata": {},
            "outputs": [],
            "source": [
                "def decode_sequence(input_seq):\n",
                "    # 1. ì¸ì½”ë”ë¡œ ì…ë ¥ ë¬¸ì¥ì„ ìƒíƒœ ë²¡í„°(Context Vector)ë¡œ ë³€í™˜\n",
                "    states_value = encoder_model.predict(input_seq)\n",
                "\n",
                "    # 2. ë””ì½”ë”ì˜ ì²« ì…ë ¥ìœ¼ë¡œ <sos> í† í° ì¤€ë¹„\n",
                "    target_seq = np.zeros((1, 1))\n",
                "    target_seq[0, 0] = kor_tokenizer.word_index['<sos>']\n",
                "\n",
                "    stop_condition = False\n",
                "    decoded_sentence = \"\"\n",
                "\n",
                "    while not stop_condition:\n",
                "        # 3. ë””ì½”ë” ëª¨ë¸ì— [í˜„ì¬ ë‹¨ì–´, ì´ì „ ìƒíƒœ]ë¥¼ ë„£ì–´ ì˜ˆì¸¡ ìˆ˜í–‰\n",
                "        # output_tokens: ë‹¤ìŒ ë‹¨ì–´ í™•ë¥  ë¶„í¬, h/c: í˜„ì¬ ìƒíƒœ ì—…ë°ì´íŠ¸\n",
                "        output_tokens, h, c = decoder_inference_model.predict([target_seq] + states_value)\n",
                "\n",
                "        # 4. ê°€ì¥ í™•ë¥  ë†’ì€ ë‹¨ì–´ ì„ íƒ (Greedy Search)\n",
                "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
                "        sampled_word = kor_tokenizer.index_word.get(sampled_token_index, \"<OOV>\")\n",
                "\n",
                "        # 5. ì¢…ë£Œ ì¡°ê±´ ê²€ì‚¬ (<eos>ì— ë„ë‹¬í•˜ê±°ë‚˜ ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼ ì‹œ)\n",
                "        if (sampled_word == '<eos>' or len(decoded_sentence) > kor_max_len):\n",
                "            stop_condition = True\n",
                "        else:\n",
                "            decoded_sentence += sampled_word + \" \"\n",
                "\n",
                "        # 6. ë‹¤ìŒ ë£¨í”„ë¥¼ ìœ„í•´ ì…ë ¥ ì—…ë°ì´íŠ¸\n",
                "        # ì´ë²ˆì— ì˜ˆì¸¡í•œ ë‹¨ì–´ê°€ ë‹¤ìŒ ìŠ¤í…ì˜ ì…ë ¥ì´ ë¨\n",
                "        target_seq = np.zeros((1, 1))\n",
                "        target_seq[0, 0] = sampled_token_index\n",
                "\n",
                "        # 7. ìƒíƒœ ì—…ë°ì´íŠ¸\n",
                "        states_value = [h, c]\n",
                "\n",
                "    return decoded_sentence.strip()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b182f7c3",
            "metadata": {},
            "source": [
                "## 10. í…ŒìŠ¤íŠ¸: ë²ˆì—­ ê²°ê³¼ í™•ì¸"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "f9a2318d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# í•™ìŠµ ë°ì´í„° ì¤‘ ëª‡ ê°œë¥¼ ê³¨ë¼ í…ŒìŠ¤íŠ¸\n",
                "for seq_index in [0, 1, 2, 5, 8]:\n",
                "    input_seq = eng_inputs_padded[seq_index: seq_index + 1]\n",
                "    decoded_sentence = decode_sequence(input_seq)\n",
                "    \n",
                "    print(\"-\" * 35)\n",
                "    print('ì…ë ¥ ë¬¸ì¥ (ì˜ì–´):', eng_inputs[seq_index])\n",
                "    print('ì •ë‹µ ë¬¸ì¥ (í•œêµ­ì–´):', kor_mn[seq_index])\n",
                "    print('ë²ˆì—­ ê²°ê³¼ (ê¸°ê³„):', decoded_sentence)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}