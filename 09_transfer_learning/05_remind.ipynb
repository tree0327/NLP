{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "06c45e1a",
            "metadata": {
                "id": "intro"
            },
            "source": [
                "# 05. NLP íŒ¨ëŸ¬ë‹¤ì„ì˜ ì§„í™”: ë°ì´í„°ëŠ” ì–´ë–»ê²Œ ì§€ëŠ¥ì´ ë˜ëŠ”ê°€?\n",
                "### ê³¼ëª©: NLP Transfer Learning\n",
                "---\n",
                "**ì´ ë…¸íŠ¸ë¶ í•˜ë‚˜ë¡œ ì™„ë²½í•˜ê²Œ ì´í•´í•˜ê¸°**\n",
                "ì´ íŒŒì¼ì€ ë‹¨ìˆœí•œ ì½”ë“œ ëª¨ìŒì´ ì•„ë‹™ë‹ˆë‹¤. **\"ë°ì´í„°ê°€ ì–´ë–»ê²Œ ì²˜ë¦¬ë˜ê³ , ë¶„ì„ë˜ì–´, í™œìš©ë˜ëŠ”ì§€\"** ê·¸ íë¦„ê³¼ ì§„í™” ê³¼ì •ì„ ì´ì•¼ê¸°ì²˜ëŸ¼ í’€ì—ˆìŠµë‹ˆë‹¤.\n",
                "\n",
                "**í•µì‹¬ í¬ì¸íŠ¸ 4ê°€ì§€**\n",
                "1.  **ë°ì´í„° ì •ì œì™€ ë¶„ì„:** ê° ì‹œëŒ€ë³„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì–´ë–»ê²Œ ìˆ«ì/ì˜ë¯¸ë¡œ ë°”ê¿¨ëŠ”ê°€?\n",
                "2.  **ì°¨ì´ì ê³¼ í•„ìš”ì„±:** ì™œ ì´ì „ ê¸°ìˆ ì„ ë²„ë¦¬ê³  ìƒˆë¡œìš´ ê¸°ìˆ ì´ ë‚˜ì™”ëŠ”ê°€?\n",
                "3.  **í™œìš©:** ë¶„ì„ëœ ê²°ê³¼ëŠ” ì–´ë–»ê²Œ ì‚¬ìš©ë˜ëŠ”ê°€?\n",
                "4.  **ê¸°ìˆ ì˜ ê³ ë„í™”:** ì½”ë“œëŠ” ì–´ë–»ê²Œ ë³€í•´ì™”ëŠ”ê°€?\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6ab5893d",
            "metadata": {},
            "source": [
                "### ğŸ¬ ì‹¤ìŠµ ë°ì´í„°: ì•„ì£¼ ì§§ì€ ì˜í™” ë¦¬ë·°\n",
                "ìš°ë¦¬ëŠ” ì´ 4ê°œì˜ ë¬¸ì¥ì„ ê°€ì§€ê³  **\"ê¸ì •(1)\"ì¸ì§€ \"ë¶€ì •(0)\"ì¸ì§€** ë§ì¶”ëŠ” ëª¨ë¸ì„ ë§Œë“¤ ê²ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4825b32f",
            "metadata": {
                "id": "data_setup"
            },
            "outputs": [],
            "source": [
                "# ì…ë ¥ ë°ì´í„° (X): ì»´í“¨í„°ê°€ ì½ì–´ì•¼ í•  ê¸€ìë“¤\n",
                "reviews = [\n",
                "    \"ì´ ì˜í™” ì§„ì§œ ì¬ë°Œë‹¤ ìµœê³ ì˜ ëª…ì‘\",      # 1. ê¸ì •\n",
                "    \"ìŠ¤í† ë¦¬ ì „ê°œê°€ ë„ˆë¬´ ì§€ë£¨í•˜ê³  í•˜í’ˆë§Œ ë‚˜ì˜´\", # 2. ë¶€ì •\n",
                "    \"ë°°ìš°ë“¤ì˜ ì—°ê¸°ë ¥ì´ ì •ë§ ëŒ€ë‹¨í•˜ë‹¤ ì••ë„ì ì„\", # 3. ê¸ì •\n",
                "    \"ëˆ ì•„ê¹ë‹¤ ì‹œê°„ì´ ë„ˆë¬´ ì•„ê¹Œì›Œ ìµœì•…\"      # 4. ë¶€ì •\n",
                "]\n",
                "\n",
                "# ì •ë‹µ ë°ì´í„° (y): ì»´í“¨í„°ê°€ ë§ì¶°ì•¼ í•  ëª©í‘œ\n",
                "labels = [1, 0, 1, 0]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0b0139e2",
            "metadata": {},
            "source": [
                "---\n",
                "## 1ï¸âƒ£ Python ì‹œëŒ€: \"ì‚¬ëŒì´ ê·œì¹™ì„ ë‹¤ ì •í•´ì¤„ê²Œ\" (Rule-based)\n",
                "\n",
                "### 1. ë°ì´í„° ì •ì œì™€ ë¶„ì„ ë°©ì‹\n",
                "- **ì •ì œ (Preprocessing):** ë¬¸ì¥ì„ ë„ì–´ì“°ê¸° ê¸°ì¤€ìœ¼ë¡œ ë‹¨ìˆœíˆ ìë¦…ë‹ˆë‹¤. (`split`)\n",
                "- **ë¶„ì„ (Analysis):** ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì‚¬ëŒì´ ë¨¸ë¦¿ì†ì— ìˆëŠ” 'ê¸ì • ë‹¨ì–´ì¥'ì„ ì½”ë“œì— ì§ì ‘ ì ì–´ë„£ìŠµë‹ˆë‹¤.\n",
                "\n",
                "### 2. ì°¨ì´ì ê³¼ í•„ìš”ì„±\n",
                "- **í•„ìš”ì„±:** ì»´í“¨í„°ì—ê²Œ ë³µì¡í•œ ìˆ˜í•™ì„ ê°€ë¥´ì¹˜ê¸° ì „, ê°€ì¥ ì§ê´€ì ì¸ ë°©ë²•ì´ì—ˆìŠµë‹ˆë‹¤.\n",
                "- **í•œê³„:** ì‚¬ëŒì´ ì„¸ìƒì˜ ëª¨ë“  ë‹¨ì–´ë¥¼ ë‹¤ ì½”ë”©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì˜ˆ: 'í•µë…¸ì¼' ê°™ì€ ì‹ ì¡°ì–´ ë“±ì¥ ì‹œ ì¸ì‹ ë¶ˆê°€)\n",
                "\n",
                "### 3. í™œìš©\n",
                "- ê°„ë‹¨í•œ í‚¤ì›Œë“œ í•„í„°ë§(ìš•ì„¤ íƒì§€ ë“±)ì— ì—¬ì „íˆ ì“°ì…ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "88969ee5",
            "metadata": {
                "id": "python_approach"
            },
            "outputs": [],
            "source": [
                "def python_rule_based(text):\n",
                "    # [1] ë°ì´í„° ì •ì œ (Preprocessing)\n",
                "    # ë¬¸ì¥ì„ ìª¼ê°­ë‹ˆë‹¤. \"ì´ ì˜í™” ì§„ì§œ\" -> ['ì´', 'ì˜í™”', 'ì§„ì§œ']\n",
                "    words = text.split()\n",
                "    \n",
                "    # [2] ë¶„ì„ (Modeling ëŒ€ì‹  Rule Definition)\n",
                "    # ì‚¬ëŒì´ ì§ì ‘ ê·œì¹™ì„ ë§Œë“­ë‹ˆë‹¤. (í•™ìŠµ ê³¼ì • ì—†ìŒ)\n",
                "    pos_keywords = ['ì¬ë°Œ', 'ìµœê³ ', 'ëŒ€ë‹¨', 'ì••ë„']\n",
                "    neg_keywords = ['ì§€ë£¨', 'í•˜í’ˆ', 'ì•„ê¹', 'ìµœì•…']\n",
                "    \n",
                "    score = 0\n",
                "    for word in words:\n",
                "        # ê¸ì • ë‹¨ì–´ê°€ ìˆìœ¼ë©´ +1ì \n",
                "        for pos in pos_keywords:\n",
                "            if pos in word: score += 1\n",
                "        # ë¶€ì • ë‹¨ì–´ê°€ ìˆìœ¼ë©´ -1ì \n",
                "        for neg in neg_keywords:\n",
                "            if neg in word: score -= 1\n",
                "            \n",
                "    # [3] í™œìš© (Output)\n",
                "    # ì ìˆ˜ê°€ 0ë³´ë‹¤ í¬ë©´ ê¸ì •ìœ¼ë¡œ íŒë‹¨\n",
                "    return \"ê¸ì •\" if score > 0 else \"ë¶€ì •\"\n",
                "\n",
                "# ê²°ê³¼ í™•ì¸\n",
                "print(\"=== [1] íŒŒì´ì¬ ê·œì¹™ ê¸°ë°˜ ===\")\n",
                "for r in reviews:\n",
                "    print(f\"'{r}' -> {python_rule_based(r)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b5f8d14a",
            "metadata": {},
            "source": [
                "---\n",
                "## 2ï¸âƒ£ ë¨¸ì‹ ëŸ¬ë‹ ì‹œëŒ€: \"í†µê³„ë¡œ í™•ë¥ ì„ ê³„ì‚°í•˜ì\" (Statistical ML)\n",
                "\n",
                "### 1. ë°ì´í„° ì •ì œì™€ ë¶„ì„ ë°©ì‹\n",
                "- **ì •ì œ:** **`CountVectorizer`**ë¥¼ ë„ì…í•©ë‹ˆë‹¤. ê¸€ìë¥¼ 'ë‹¨ì–´ ê°€ë°©(Bag of Words)'ì— ë„£ê³  ë¹ˆë„ìˆ˜ë¥¼ ì…‰ë‹ˆë‹¤.\n",
                "- **ë¶„ì„:** **`Naive Bayes`** ê°™ì€ í†µê³„ ëª¨ë¸ì„ ì”ë‹ˆë‹¤. \"'ìµœê³ 'ë¼ëŠ” ë‹¨ì–´ê°€ ë‚˜ì˜¬ ë•Œ ê¸ì •ì¼ í™•ë¥ ì´ 80%ë„¤?\"ë¼ê³  ì»´í“¨í„°ê°€ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
                "\n",
                "### 2. ì°¨ì´ì ê³¼ í•„ìš”ì„±\n",
                "- **ì§„í™”:** ì‚¬ëŒì´ ê·œì¹™ì„ ë§Œë“œëŠ” ê±´ í•œê³„ê°€ ìˆì–´ì„œ, **ë°ì´í„°ì—ì„œ í†µê³„ì  íŒ¨í„´ì„ ìŠ¤ìŠ¤ë¡œ ì°¾ê²Œ** ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.\n",
                "- **í•œê³„:** ë‹¨ì–´ì˜ **ìˆœì„œ**ë¥¼ ë¬´ì‹œí•©ë‹ˆë‹¤. \"ë°¥ ë¨¹ê³  ë°° ì•„íŒŒ\"ì™€ \"ë°° ì•„íŒŒ ë°¥ ë¨¹ê³ \"ë¥¼ ë˜‘ê°™ì´ ì¸ì‹í•©ë‹ˆë‹¤.\n",
                "\n",
                "### 3. í™œìš©\n",
                "- ìŠ¤íŒ¸ ë©”ì¼ í•„í„°ë§, ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ë“±ì— ì•„ì£¼ ë§ì´ ì“°ì˜€ìŠµë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "72ad9d5f",
            "metadata": {
                "id": "ml_approach"
            },
            "outputs": [],
            "source": [
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "from sklearn.naive_bayes import MultinomialNB\n",
                "\n",
                "# [1] ë°ì´í„° ì •ì œ (Preprocessing)\n",
                "# ì´ì œ ë‹¨ì–´ë¥¼ ì‚¬ëŒì´ ìª¼ê°œì§€ ì•ŠìŠµë‹ˆë‹¤. Vectorizerê°€ 'ë‹¨ì–´ì¥'ì„ ë§Œë“¤ê³  ìˆ«ìë¡œ ë°”ê¿‰ë‹ˆë‹¤.\n",
                "vectorizer = CountVectorizer()\n",
                "X_vec = vectorizer.fit_transform(reviews)\n",
                "\n",
                "print(\"\\n=== [2] ë¨¸ì‹ ëŸ¬ë‹ (í†µê³„ ê¸°ë°˜) ===\")\n",
                "print(\"ì»´í“¨í„°ê°€ ë§Œë“  ë‹¨ì–´ì¥:\", vectorizer.get_feature_names_out())\n",
                "print(\"ìˆ«ìë¡œ ë³€í•œ ë¬¸ì¥(ë¹ˆë„ìˆ˜):\", X_vec[0].toarray())\n",
                "\n",
                "# [2] ë¶„ì„ (Modeling)\n",
                "# ê·œì¹™ ëŒ€ì‹ , ë°ì´í„°(X_vec)ì™€ ì •ë‹µ(labels)ì„ ì£¼ê³  í™•ë¥ ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.\n",
                "ml_model = MultinomialNB()\n",
                "ml_model.fit(X_vec, labels)\n",
                "\n",
                "# [3] í™œìš© (Inference)\n",
                "# ìƒˆë¡œìš´ ë¬¸ì¥ì´ ë“¤ì–´ì˜¤ë©´ í™•ë¥ ì„ ê³„ì‚°í•´ì„œ ë‹µì„ ì¤ë‹ˆë‹¤.\n",
                "test_sent = [\"ì˜í™” ì •ë§ ìµœê³ \"]\n",
                "test_vec = vectorizer.transform(test_sent)\n",
                "pred = ml_model.predict(test_vec)\n",
                "print(f\"í™œìš© ê²°ê³¼: '{test_sent[0]}' -> {'ê¸ì •' if pred[0]==1 else 'ë¶€ì •'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "53d4d57b",
            "metadata": {},
            "source": [
                "---\n",
                "## 3ï¸âƒ£ ë”¥ëŸ¬ë‹ ì‹œëŒ€: \"ë‹¨ì–´ì˜ ì˜ë¯¸ì™€ ë¬¸ë§¥ì„ ì´í•´í•˜ì\" (Deep Learning)\n",
                "\n",
                "### 1. ë°ì´í„° ì •ì œì™€ ë¶„ì„ ë°©ì‹\n",
                "- **ì •ì œ:** **`Tokenizer`**ë¡œ ë‹¨ì–´ì— ê³ ìœ  ë²ˆí˜¸ë¥¼ ë¶™ì´ê³ , **`Embedding`**ì´ë¼ëŠ” ê¸°ìˆ ë¡œ ë‹¨ì–´ë¥¼ 'ì˜ë¯¸ ë²¡í„°'ë¡œ ë°”ê¿‰ë‹ˆë‹¤.\n",
                "- **ë¶„ì„:** **`RNN/LSTM`** ì‹ ê²½ë§ì„ ì¨ì„œ ë¬¸ì¥ì„ ì•ì—ì„œë¶€í„° ìˆœì„œëŒ€ë¡œ ì½ìŠµë‹ˆë‹¤. ë¬¸ë§¥(Conext)ì„ ê¸°ì–µí•©ë‹ˆë‹¤.\n",
                "\n",
                "### 2. ì°¨ì´ì ê³¼ í•„ìš”ì„±\n",
                "- **ì§„í™”:** ë¨¸ì‹ ëŸ¬ë‹ì´ ë†“ì³¤ë˜ **ìˆœì„œì™€ ë‰˜ì•™ìŠ¤**ë¥¼ ì¡ê¸° ìœ„í•´ ì‹ ê²½ë§(ë‡Œ êµ¬ì¡°)ì„ ëª¨ë°©í–ˆìŠµë‹ˆë‹¤.\n",
                "- **í•œê³„:** í•™ìŠµ ë°ì´í„°ê°€ ì—„ì²­ë‚˜ê²Œ ë§ì´ í•„ìš”í•˜ê³ , í•™ìŠµ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤.\n",
                "\n",
                "### 3. í™œìš©\n",
                "- ë²ˆì—­ê¸°(íŒŒíŒŒê³  ì´ˆì°½ê¸°), ìŒì„± ì¸ì‹, ì±—ë´‡ ë“±ì— í˜ëª…ì„ ì¼ìœ¼ì¼°ìŠµë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "22cd8ff7",
            "metadata": {
                "id": "dl_approach"
            },
            "outputs": [],
            "source": [
                "from tensorflow.keras.preprocessing.text import Tokenizer\n",
                "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
                "import numpy as np\n",
                "\n",
                "# [1] ë°ì´í„° ì •ì œ (Preprocessing)\n",
                "# ë¬¸ì¥ì„ ìˆ«ìë¡œ ë°”ê¾¸ê³ , ê¸¸ì´ë¥¼ ë˜‘ê°™ì´ ë§ì¶¥ë‹ˆë‹¤. (Padding)\n",
                "tokenizer = Tokenizer()\n",
                "tokenizer.fit_on_texts(reviews)\n",
                "sequences = tokenizer.texts_to_sequences(reviews)\n",
                "padded = pad_sequences(sequences, padding='post', maxlen=5)\n",
                "\n",
                "print(\"\\n=== [3] ë”¥ëŸ¬ë‹ (ì‹ ê²½ë§) ===\")\n",
                "print(\"ì •ì œëœ ë°ì´í„°(Sequence):\\n\", padded)\n",
                "\n",
                "# [2] ë¶„ì„ (Modeling)\n",
                "# Embedding(ì˜ë¯¸ íŒŒì•…) -> LSTM(ìˆœì„œ íŒŒì•…) -> Dense(ì¶œë ¥)\n",
                "vocab_size = len(tokenizer.word_index) + 1\n",
                "dl_model = Sequential([\n",
                "    Embedding(input_dim=vocab_size, output_dim=4, input_length=5),\n",
                "    LSTM(8),\n",
                "    Dense(1, activation='sigmoid')\n",
                "])\n",
                "dl_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
                "dl_model.fit(padded, np.array(labels), epochs=10, verbose=0)\n",
                "\n",
                "# [3] í™œìš© (Inference)\n",
                "# 0.5ë³´ë‹¤ í¬ë©´ ê¸ì •ìœ¼ë¡œ íŒë‹¨\n",
                "pred = dl_model.predict(padded)[0][0]\n",
                "print(f\"í™œìš© ê²°ê³¼ (ì²« ë¬¸ì¥): {pred:.4f} -> {'ê¸ì •' if pred>0.5 else 'ë¶€ì •'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d6f36583",
            "metadata": {},
            "source": [
                "---\n",
                "## 4ï¸âƒ£ LLM ì‹œëŒ€: \"ì´ë¯¸ ë‹¤ ì•„ëŠ” ì²œì¬ë¥¼ ë°ë ¤ë‹¤ ì“°ì\" (Transfer Learning)\n",
                "\n",
                "### 1. ë°ì´í„° ì •ì œì™€ ë¶„ì„ ë°©ì‹\n",
                "- **ì •ì œ:** **`AutoTokenizer`**ë¥¼ ì”ë‹ˆë‹¤. BERTê°€ í•™ìŠµí•  ë•Œ ì¼ë˜ ê·¸ ê·œì¹™ ê·¸ëŒ€ë¡œ ìª¼ê°œê³ , íŠ¹ìˆ˜ í† í°(`[CLS]`)ì„ ë¶™ì…ë‹ˆë‹¤.\n",
                "- **ë¶„ì„:** **`Transformer(BERT)`** ëª¨ë¸ì„ ì”ë‹ˆë‹¤. ì²˜ìŒë¶€í„° ë°°ìš°ëŠ” ê²Œ ì•„ë‹ˆë¼, ì´ë¯¸ ìˆ˜ì‹­ì–µ ê°œì˜ ë¬¸ì¥ì„ ì½ì€ ëª¨ë¸ì„ ê°€ì ¸ì™€ì„œ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤.\n",
                "\n",
                "### 2. ì°¨ì´ì ê³¼ í•„ìš”ì„±\n",
                "- **ì§„í™”:** \"ì²˜ìŒë¶€í„° ê°€ë¥´ì¹˜ì§€ ë§ê³ , ì˜í•˜ëŠ” ì• (Pre-trained) ë°ë ¤ì™€ì„œ ë‚´ ì¼ë§Œ ê°€ë¥´ì¹˜ì(Fine-tuning)\"ëŠ” í˜ëª…ì ì¸ ìƒê°.\n",
                "- **ê²°ê³¼:** ì ì€ ë°ì´í„°ë¡œë„ ì••ë„ì ì¸ ì„±ëŠ¥ì„ ëƒ…ë‹ˆë‹¤.\n",
                "\n",
                "### 3. í™œìš©\n",
                "- ChatGPT, í˜„ì¬ì˜ ëª¨ë“  AI ì„œë¹„ìŠ¤, ê³ ì„±ëŠ¥ ë¬¸ì„œ ë¶„ë¥˜, ê²€ìƒ‰ ì—”ì§„ ë“±."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "87721a08",
            "metadata": {
                "id": "llm_approach"
            },
            "outputs": [],
            "source": [
                "from transformers import AutoTokenizer, TFBertForSequenceClassification\n",
                "import tensorflow as tf\n",
                "\n",
                "# ëª¨ë¸ ì´ë¦„ (í•œêµ­ì–´ BERT)\n",
                "MODEL_NAME = \"klue/bert-base\"\n",
                "\n",
                "# [1] ë°ì´í„° ì •ì œ (Preprocessing)\n",
                "# ì²œì¬ ëª¨ë¸ì´ ì‚¬ìš©í•˜ëŠ” ì „ìš© ë„êµ¬(Tokenizer)ë¥¼ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
                "# ì•Œì•„ì„œ [CLS], [SEP] ê°™ì€ íŠ¹ìˆ˜ í† í°ì„ ë¶™ì´ê³  ìˆ«ìë¡œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "inputs = tokenizer(reviews, padding=True, truncation=True, return_tensors=\"tf\")\n",
                "\n",
                "print(\"\\n=== [4] LLM (ì „ì´ í•™ìŠµ) ===\")\n",
                "print(\"ì •ì œëœ ë°ì´í„° (BERT Input):\\n\", inputs['input_ids'].numpy()[0])\n",
                "\n",
                "# [2] ë¶„ì„ (Modeling)\n",
                "# ëª¨ë¸ì˜ 'ë‡Œ(Weights)'ë¥¼ ê·¸ëŒ€ë¡œ ë‹¤ìš´ë¡œë“œ ë°›ìŠµë‹ˆë‹¤.\n",
                "llm_model = TFBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2, from_pt=True)\n",
                "\n",
                "# [3] í™œìš© (Inference)\n",
                "# ë³„ë„ì˜ í•™ìŠµ ì—†ì´ë„ ê¸°ë³¸ì ì¸ ì–¸ì–´ ëŠ¥ë ¥ì´ ë›°ì–´ë‚˜ì„œ ë°”ë¡œ ì¶”ë¡ í•´ë´…ë‹ˆë‹¤.\n",
                "outputs = llm_model(inputs)\n",
                "probs = tf.nn.softmax(outputs.logits, axis=-1)\n",
                "predictions = tf.argmax(probs, axis=-1)\n",
                "\n",
                "print(f\"í™œìš© ê²°ê³¼: {predictions.numpy()} (ëª¨ë¸ì´ ê¸°ë³¸ ì§€ì‹ìœ¼ë¡œ íŒë‹¨í•œ ê²°ê³¼)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "276393e8",
            "metadata": {},
            "source": [
                "## ğŸ† ì´ì •ë¦¬: ê¸°ìˆ ì˜ ê³ ë„í™” ê³¼ì •\n",
                "\n",
                "| ì‹œëŒ€ | ê¸°ìˆ  ì´ë¦„ | í•µì‹¬ ë³€í™” (Why?) | ì½”ë“œì˜ íŠ¹ì§• |\n",
                "|:---:|:---:|:---:|:---|\n",
                "| **1** | **Python** | \"ê·œì¹™ì´ í•„ìš”í•´\" | `if`ë¬¸ ë„ë°°, ëª¨ë“  ë‹¨ì–´ë¥¼ ì‚¬ëŒì´ ì…ë ¥ |\n",
                "| **2** | **ML** | \"ê·œì¹™ ëª» ì§œê² ì–´, í†µê³„ ë‚´ì\" | `Vectorizer`, í™•ë¥  ê³„ì‚°, ë‹¨ì–´ ê°€ë°©(BoW) |\n",
                "| **3** | **DL** | \"ë¬¸ë§¥ì„ ì´í•´í•´ì•¼ì§€!\" | `Embedding`, `LSTM`, ìˆœì„œì™€ ë‰˜ì•™ìŠ¤ íŒŒì•… |\n",
                "| **4** | **LLM** | \"ê·¸ëƒ¥ ì²œì¬ ë°ë ¤ì™€\" | `Pre-trained` ë¡œë“œ, ì½”ë“œ ì–‘ì€ ì¤„ê³  ì„±ëŠ¥ì€ í­ë°œ |\n",
                "\n",
                "ë³´ì‹œë‹¤ì‹œí”¼ ê¸°ìˆ ì´ ë°œì „í• ìˆ˜ë¡ **\"ì‚¬ëŒì´ í•  ì¼ì€ ì¤„ì–´ë“¤ê³ , ì»´í“¨í„°ê°€ ë¯¸ë¦¬ í•´ë†“ì€ ì¼(ì‚¬ì „í•™ìŠµ)ì„ ê°€ì ¸ë‹¤ ì“°ëŠ” ë°©ì‹\"**ìœ¼ë¡œ ê³ ë„í™”ë˜ì—ˆìŠµë‹ˆë‹¤."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "python3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
