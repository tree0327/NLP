{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "04e5897c",
            "metadata": {},
            "source": [
                "# 01. BERT Overview & Transformer ë§›ë³´ê¸°\n",
                "### ê³¼ëª©: NLP Transfer Learning\n",
                "---\n",
                "**í•™ìŠµ ëª©í‘œ**\n",
                "1. **BERT**ê°€ ë¬´ì—‡ì¸ì§€, ê¸°ì¡´ ëª¨ë¸ê³¼ ë¬´ì—‡ì´ ë‹¤ë¥¸ì§€ ì´í•´í•œë‹¤.\n",
                "2. **Tokenizer**ì˜ ì—­í• ê³¼ Special Token(`[CLS], [SEP], [MASK]`)ì„ ì´í•´í•œë‹¤.\n",
                "3. **Masked Language Model(MLM)**ê³¼ **Next Sentence Prediction(NSP)**ê°€ ì–´ë–»ê²Œ í•™ìŠµë˜ëŠ”ì§€ ì½”ë“œë¡œ ì‹¤ìŠµí•œë‹¤."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "db5e90de",
            "metadata": {
                "id": "ueBwA9VrXKlA"
            },
            "source": [
                "# 1. BERTë€ ë¬´ì—‡ì¸ê°€?\n",
                "\n",
                "**BERT**(Bidirectional Encoder Representations from Transformers)ëŠ” êµ¬ê¸€ì´ ê°œë°œí•œ ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ì…ë‹ˆë‹¤.\n",
                "\n",
                "### ğŸ ì‰¬ìš´ ë¹„ìœ : \"ì–¸ì–´ ì²œì¬ ê¹€AI\"\n",
                "- ì˜›ë‚  ëª¨ë¸(GPT-1 ë“±): ì±…ì„ ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œë§Œ ì½ì„ ìˆ˜ ìˆì—ˆì–´ìš”. (ë‹¨ë°©í–¥)\n",
                "- **BERT**: ì±…ì„ í•œ ë²ˆì— í¼ì³ì„œ **ë¬¸ì¥ì˜ ì•ë’¤ ë¬¸ë§¥ì„ ë™ì‹œì—** ë´…ë‹ˆë‹¤. (ì–‘ë°©í–¥, Bidirectional)\n",
                "- ê·¸ë˜ì„œ ë¹ˆì¹¸ ì±„ìš°ê¸°(`[MASK]`)ë‚˜ ë¬¸ì¥ì˜ ì˜ë¯¸ íŒŒì•…ì„ ì—„ì²­ë‚˜ê²Œ ì˜í•©ë‹ˆë‹¤.\n",
                "\n",
                "### BERTì˜ 3ê°€ì§€ í•µì‹¬ íŠ¹ì§•\n",
                "1. **ì–‘ë°©í–¥ì„± (Bidirectionality):** ë¬¸ë§¥ì„ ì–‘ìª½ì—ì„œ íŒŒì•…í•˜ë¯€ë¡œ \"ë°°\"ê°€ ë¨¹ëŠ” ë°°ì¸ì§€, íƒ€ëŠ” ë°°ì¸ì§€ ì •í™•íˆ ì•Œ ìˆ˜ ìˆìŒ.\n",
                "2. **Transformer Encoder:** ë¬¸ì¥ì„ ì´í•´í•˜ëŠ”(Encoding) ëŠ¥ë ¥ì´ íƒì›”í•œ Transformerì˜ ì¸ì½”ë” ë¶€ë¶„ë§Œ ì‚¬ìš©.\n",
                "3. **ì „ì´ í•™ìŠµ (Transfer Learning):** ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¡œ ì‚¬ì „ í•™ìŠµ(Pre-training)ëœ ëª¨ë¸ì„ ê°€ì ¸ì™€ì„œ, ë‚´ ëª©ì ì— ë§ê²Œ ë¯¸ì„¸ ì¡°ì •(Fine-tuning)í•˜ì—¬ ì‚¬ìš©."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9cc7e458",
            "metadata": {},
            "source": [
                "### 2. ì¤€ë¹„ë¬¼ ì±™ê¸°ê¸° (ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b8a0c55e",
            "metadata": {
                "id": "LIadvRY9W_V9"
            },
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "# Hugging Face Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ì—†ë‹¤ë©´ ì‹¤í–‰)\n",
                "# !pip install transformers -q"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6c855922",
            "metadata": {
                "id": "2vxDWhboadS5"
            },
            "source": [
                "# 3. Tokenizer (í† í¬ë‚˜ì´ì €) ì´í•´í•˜ê¸°\n",
                "\n",
                "ì»´í“¨í„°ëŠ” 'ì‚¬ê³¼', 'í•™êµ' ê°™ì€ ê¸€ìë¥¼ ì´í•´í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ìˆ«ìë¡œ ë°”ê¿”ì¤˜ì•¼ í•˜ì£ .\n",
                "ì´ ì—­í• ì„ í•˜ëŠ” ê²ƒì´ **Tokenizer**ì…ë‹ˆë‹¤.\n",
                "\n",
                "- **ì—­í™œ:** ë¬¸ì¥ -> ì¡°ê°(Token) -> ìˆ«ì(ID)\n",
                "- **WordPiece:** BERTëŠ” ë‹¨ì–´ë¥¼ ë” ì‘ì€ ë‹¨ìœ„(Sub-word)ë¡œ ìª¼ê°œì„œ, ëª¨ë¥´ëŠ” ë‹¨ì–´(OOV)ê°€ ë‚˜ì™€ë„ ëŒ€ì²˜í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b5773656",
            "metadata": {
                "id": "ujdYoHtLaaUE"
            },
            "outputs": [],
            "source": [
                "from transformers import BertTokenizer\n",
                "\n",
                "model_name = 'bert-base-uncased' # 'uncased'ëŠ” ì†Œë¬¸ìë¡œë§Œ ì´ë£¨ì–´ì§„ ëª¨ë¸ì´ë¼ëŠ” ëœ»ì…ë‹ˆë‹¤.\n",
                "\n",
                "# 1. ì‚¬ì „ í•™ìŠµëœ(Pre-trained) BERTì˜ í† í¬ë‚˜ì´ì €(ë‹¨ì–´ì¥+ê·œì¹™)ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
                "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
                "tokenizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "da41b00a",
            "metadata": {
                "id": "qPYe1bOzclh8"
            },
            "outputs": [],
            "source": [
                "# 2. ë¬¸ì¥ì„ ìˆ«ìë¡œ ë°”ê¿”ë´…ì‹œë‹¤. (Encoding)\n",
                "text = 'Here is the sentence I want embedding for.'\n",
                "\n",
                "# tokenizer.encode() -> ìë™ìœ¼ë¡œ [CLS], [SEP] ê°™ì€ íŠ¹ìˆ˜ í† í°ì„ ë¶™ì—¬ì„œ ì •ìˆ˜ IDë¡œ ë³€í™˜í•´ì¤ë‹ˆë‹¤.\n",
                "encoded_ids = tokenizer.encode(text)\n",
                "print(\"Encoded IDs:\", encoded_ids)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e4e74e99",
            "metadata": {
                "id": "tElBxVKMdIqd"
            },
            "outputs": [],
            "source": [
                "# 3. ì‹¤ì œë¡œ ì–´ë–»ê²Œ ìª¼ê°œì¡ŒëŠ”ì§€ ëˆˆìœ¼ë¡œ í™•ì¸í•´ë³¼ê¹Œìš”? (Tokenize)\n",
                "tokens = tokenizer.tokenize(text)\n",
                "print(\"Tokens:\", tokens)\n",
                "\n",
                "# 'embedding'ì´ë¼ëŠ” ë‹¨ì–´ê°€ 'em', '##bed', '##ding'ìœ¼ë¡œ ìª¼ê°œì§„ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
                "# '##'ì€ ì• ë‹¨ì–´ì— ë¶™ëŠ”ë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤. (Sub-word Tokenization)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1717ec3c",
            "metadata": {
                "id": "sM4uPGBwco-R"
            },
            "outputs": [],
            "source": [
                "# 4. ëª¨ë¸ì— ë„£ê¸° ì¢‹ì€ ì¢…í•© ì„ ë¬¼ ì„¸íŠ¸ (tokenizer())\n",
                "# input_ids, token_type_ids, attention_maskë¥¼ í•œ ë²ˆì— ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.\n",
                "inputs = tokenizer(text)\n",
                "inputs"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "81a21721",
            "metadata": {
                "id": "tgoO6BKkeIym"
            },
            "source": [
                "# 4. Masked Language Model (MLM): ë¹ˆì¹¸ ì±„ìš°ê¸° ë†€ì´\n",
                "\n",
                "BERTê°€ ë˜‘ë˜‘í•´ì§„ ë¹„ê²°ì…ë‹ˆë‹¤.\n",
                "ì…ë ¥ ë¬¸ì¥ì˜ ì¼ë¶€ë¥¼ `[MASK]`ë¡œ ê°€ë¦¬ê³ , ê·¸ ìë¦¬ì— ì›ë˜ ì–´ë–¤ ë‹¨ì–´ê°€ ìˆì—ˆëŠ”ì§€ ë§íˆê²Œ í•©ë‹ˆë‹¤.\n",
                "\n",
                "> ì˜ˆì‹œ: \"ì¶•êµ¬ëŠ” ì •ë§ ì¬ë°ŒëŠ” **[MASK]**ë‹¤.\" -> ì •ë‹µ: ìŠ¤í¬ì¸ "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "de5af404",
            "metadata": {
                "id": "LMnnk4CVeMUp"
            },
            "outputs": [],
            "source": [
                "from transformers import BertForMaskedLM\n",
                "\n",
                "# Masked LM ì „ìš© í—¤ë“œ(Head)ê°€ ë‹¬ë¦° BERT ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
                "model = BertForMaskedLM.from_pretrained(model_name)\n",
                "model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "64d513ed",
            "metadata": {
                "id": "QOojdJFnho5h"
            },
            "outputs": [],
            "source": [
                "# 1. ì‹¤ìŠµí•  ë¬¸ì¥ ì¤€ë¹„ ([MASK] í¬í•¨)\n",
                "text = 'Soccer is a really fun [MASK].'\n",
                "\n",
                "# 2. ëª¨ë¸ì— ì…ë ¥í•  ìˆ˜ ìˆë„ë¡ í…ì„œ(Tensor) í˜•íƒœë¡œ ë³€í™˜ (return_tensors='pt')\n",
                "inputs = tokenizer(text, return_tensors='pt')\n",
                "\n",
                "print(\"ì…ë ¥ í† í°:\", tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))\n",
                "inputs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "117e368f",
            "metadata": {
                "id": "s5-DzlBgiJ8w"
            },
            "outputs": [],
            "source": [
                "# ì°¸ê³ : BERTê°€ ì‚¬ìš©í•˜ëŠ” íŠ¹ìˆ˜ í† í°ì˜ ID í™•ì¸\n",
                "print('CLS (ë¬¸ì¥ ì‹œì‘): ', tokenizer.cls_token_id)\n",
                "print('SEP (ë¬¸ì¥ ë): ', tokenizer.sep_token_id)\n",
                "print('MASK (ë¹ˆì¹¸): ', tokenizer.mask_token_id)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7ef359ee",
            "metadata": {
                "id": "P7Tglby4ifnU"
            },
            "outputs": [],
            "source": [
                "# 3. ëª¨ë¸ì— ë„£ê³  ì˜ˆì¸¡í•˜ê¸° (Forward)\n",
                "output = model(**inputs)\n",
                "\n",
                "# logits: ëª¨ë¸ì´ 3ë§Œ ê°œì˜ ë‹¨ì–´ì¥ ì¤‘ì—ì„œ ê° ë‹¨ì–´ê°€ ì •ë‹µì¼ ì ìˆ˜ë¥¼ ê³„ì‚°í•œ ê°’\n",
                "print(\"ì¶œë ¥ í¬ê¸° (Batch, Seq_len, Vocab_size):\", output.logits.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "39679949",
            "metadata": {},
            "source": [
                "### ğŸ¤– ë” ì‰½ê²Œ ì˜ˆì¸¡í•˜ê¸°: Pipeline ì‚¬ìš©\n",
                "ì½”ë“œë¡œ ì¼ì¼ì´ logitsì—ì„œ argmaxë¥¼ ë½‘ëŠ” ê±´ ê·€ì°®ì£ ? Hugging Faceì˜ **Pipeline**ì„ ì“°ë©´ í•œ ë°©ì— í•´ê²°ë©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "58398bc8",
            "metadata": {
                "id": "AdCi5GdXjIU9"
            },
            "outputs": [],
            "source": [
                "from transformers import FillMaskPipeline\n",
                "\n",
                "# ë¹ˆì¹¸ ì±„ìš°ê¸°(Fill-Mask) ì „ìš© íŒŒì´í”„ë¼ì¸ ìƒì„±\n",
                "pipe = FillMaskPipeline(model=model, tokenizer=tokenizer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f62bccb2",
            "metadata": {
                "id": "F-wCUDpIjcbP"
            },
            "outputs": [],
            "source": [
                "# ê²°ê³¼ í™•ì¸: \"Soccer is a really fun [MASK].\"\n",
                "# scoreê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ Top 5 í›„ë³´ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
                "pipe(text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6fcffbd9",
            "metadata": {
                "id": "-QrOfkxkjqFJ"
            },
            "outputs": [],
            "source": [
                "# ë‹¤ë¥¸ ì˜ˆì‹œë„ í•´ë³¼ê¹Œìš”?\n",
                "pipe(\"I went to [MASK] this morning.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "356fad71",
            "metadata": {
                "id": "73jC7v-vkLsq"
            },
            "source": [
                "# 5. AutoClass (AutoTokenizer, AutoModel)\n",
                "\n",
                "\"ë§¤ë²ˆ `BertTokenizer`, `RobertaTokenizer` ì´ë¦„ì„ ë‹¤ ì™¸ì›Œì•¼ í•˜ë‚˜ìš”?\"\n",
                "ì•„ë‹™ë‹ˆë‹¤! `AutoTokenizer`ë¥¼ ì“°ë©´ ëª¨ë¸ ì´ë¦„ë§Œ ë³´ê³  ì•Œì•„ì„œ ë”± ë§ëŠ” í´ë˜ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "af69ab8b",
            "metadata": {
                "id": "fAptFwwUkOwm"
            },
            "outputs": [],
            "source": [
                "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
                "\n",
                "model_name = 'bert-base-uncased'\n",
                "\n",
                "# Auto í´ë˜ìŠ¤ ì‚¬ìš© (ì¶”ì²œí•˜ëŠ” ë°©ë²•!)\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
                "\n",
                "print(\"ì‹¤ì œ ë¡œë“œëœ í† í¬ë‚˜ì´ì €:\", tokenizer.__class__.__name__)\n",
                "print(\"ì‹¤ì œ ë¡œë“œëœ ëª¨ë¸:\", model.__class__.__name__)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "be7def24",
            "metadata": {
                "id": "uNuGD7oxlTQu"
            },
            "source": [
                "# 6. Next Sentence Prediction (NSP): ë¬¸ì¥ ì´ì–´ì§“ê¸° íŒë³„\n",
                "\n",
                "BERTëŠ” **ë‘ ë¬¸ì¥** ì‚¬ì´ì˜ ê´€ê³„ë„ ì´í•´í•©ë‹ˆë‹¤.\n",
                "ë¬¸ì¥ A ë‹¤ìŒì— ë¬¸ì¥ Bê°€ ì˜¤ëŠ” ê²Œ ìì—°ìŠ¤ëŸ¬ìš´ì§€ ì•„ë‹Œì§€(True/False)ë¥¼ ë§íˆëŠ” ë¬¸ì œì…ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "56dfbb89",
            "metadata": {
                "id": "j2Oxwn1ll345"
            },
            "outputs": [],
            "source": [
                "from transformers import AutoTokenizer, AutoModelForNextSentencePrediction\n",
                "\n",
                "# ì´ë²ˆì—” NSP í—¤ë“œê°€ ë‹¬ë¦° ëª¨ë¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
                "model = AutoModelForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
                "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d3e57e8f",
            "metadata": {
                "id": "JLKP6UT-mKWZ"
            },
            "outputs": [],
            "source": [
                "# ë‘ ë¬¸ì¥ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
                "sentence1 = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
                "sentence2 = \"pizza is eaten with the use of a knife and fork. In casual settings, however, it is cut into wedges to be eaten while held in the hand.\"\n",
                "\n",
                "# ë‘ ë¬¸ì¥ì„ ê°™ì´ ë„£ì–´ì£¼ë©´, ìë™ìœ¼ë¡œ [SEP] í† í°ìœ¼ë¡œ êµ¬ë¶„í•´ì¤ë‹ˆë‹¤.\n",
                "inputs = tokenizer(sentence1, sentence2, return_tensors='pt')\n",
                "inputs"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7598a74d",
            "metadata": {},
            "source": [
                "### ğŸ‘€ ì…ë ¥ì´ ì–´ë–»ê²Œ ë“¤ì–´ê°€ëŠ”ì§€ ì‹œê°í™”"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "022891d4",
            "metadata": {
                "id": "lQeplxvsnCsY"
            },
            "outputs": [],
            "source": [
                "# [CLS] ë¬¸ì¥1 [SEP] ë¬¸ì¥2 [SEP] êµ¬ì¡°ì¸ì§€ í™•ì¸í•´ë´…ì‹œë‹¤.\n",
                "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
                "print(tokens[:20]) # ì•ë¶€ë¶„ë§Œ ì¶œë ¥"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fc875de0",
            "metadata": {
                "id": "K3URl3X7osmd"
            },
            "outputs": [],
            "source": [
                "import torch.nn.functional as F\n",
                "import torch\n",
                "\n",
                "# ëª¨ë¸ ì˜ˆì¸¡\n",
                "output = model(**inputs)\n",
                "\n",
                "# ê²°ê³¼ëŠ” 0(ì´ì–´ì§) ë˜ëŠ” 1(ì•ˆ ì´ì–´ì§)ë¡œ ë‚˜ì˜µë‹ˆë‹¤. (ëª¨ë¸ë§ˆë‹¤ ì¸ë±ìŠ¤ ì˜ë¯¸ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)\n",
                "# bert-base-uncased: 0 = IsNext (ì´ì–´ì§), 1 = NotNext (ì•ˆ ì´ì–´ì§)\n",
                "probs = F.softmax(output.logits, dim=-1)\n",
                "print(\"í™•ë¥ :\", probs)\n",
                "\n",
                "pred = torch.argmax(probs, dim=-1)\n",
                "print(\"ì˜ˆì¸¡ê°’ (0ì´ë©´ ì´ì–´ì§):\", pred.item())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9d3d8bba",
            "metadata": {
                "id": "CdCSWWhrpoJ0"
            },
            "outputs": [],
            "source": [
                "# ì—‰ëš±í•œ ë¬¸ì¥ì„ ë„£ì–´ë³¼ê¹Œìš”?\n",
                "sentence3 = \"The sky is blue due to the shorter wavelength of blue light.\"\n",
                "\n",
                "inputs = tokenizer(sentence1, sentence3, return_tensors='pt')\n",
                "output = model(**inputs)\n",
                "\n",
                "probs = F.softmax(output.logits, dim=-1)\n",
                "pred = torch.argmax(probs, dim=-1)\n",
                "\n",
                "print(\"ì˜ˆì¸¡ê°’ (1ì´ë©´ ì•ˆ ì´ì–´ì§):\", pred.item())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b91d91c6",
            "metadata": {},
            "source": [
                "### ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤!\n",
                "ì´ì œ ì—¬ëŸ¬ë¶„ì€ BERTê°€ 'ë‹¨ì–´'ë¥¼ ì–´ë–»ê²Œ ì´í•´í•˜ê³ (Masked LM), 'ë¬¸ì¥'ì„ ì–´ë–»ê²Œ ì—°ê²°í•˜ëŠ”ì§€(NSP) ì™„ë²½í•˜ê²Œ ì´í•´í–ˆìŠµë‹ˆë‹¤."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "python3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
