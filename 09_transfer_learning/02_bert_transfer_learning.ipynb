{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "bca7673f",
            "metadata": {
                "id": "intro"
            },
            "source": [
                "# 02. ì „ì´ í•™ìŠµ (Transfer Learning) ì‹¤ìŠµ: ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° ê°ì„± ë¶„ì„\n",
                "### ê³¼ëª©: NLP Transfer Learning\n",
                "---\n",
                "**í•™ìŠµ ëª©í‘œ**\n",
                "1. **ì „ì´ í•™ìŠµ(Transfer Learning)**ì˜ ê°œë…ì„ ì´í•´í•˜ê³ , ì™œ ê°•ë ¥í•œì§€ ê¹¨ë‹«ëŠ”ë‹¤.\n",
                "2. ì‚¬ì „ í•™ìŠµëœ **BERT (klue/bert-base)** ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¨ë‹¤.\n",
                "3. **NSMC (ë„¤ì´ë²„ ì˜í™” ë¦¬ë·°)** ë°ì´í„°ì…‹ìœ¼ë¡œ BERTë¥¼ ë¯¸ì„¸ ì¡°ì •(Fine-tuning)í•˜ì—¬ ê°ì„± ë¶„ì„ ëª¨ë¸ì„ ë§Œë“ ë‹¤.\n",
                "\n",
                "### ğŸ ì‰¬ìš´ ë¹„ìœ : \"ì²œì¬ ì˜ëŒ€ìƒì˜ í”¼ë¶€ê³¼ ê°œì›ê¸°\"\n",
                "- **ì‚¬ì „ í•™ìŠµ (Pre-training):** ë˜‘ë˜‘í•œ ì˜ëŒ€ìƒ(BERT)ì´ 6ë…„ ë™ì•ˆ ìˆ˜ë§Œ ê°€ì§€ ì˜í•™ ì„œì (ìœ„í‚¤í”¼ë””ì•„, ë‰´ìŠ¤ ë“±)ì„ ì½ìœ¼ë©° 'ì¼ë°˜ì ì¸ ì˜í•™ ì§€ì‹'ì„ ìŒ“ìŠµë‹ˆë‹¤.\n",
                "- **ì „ì´ í•™ìŠµ (Transfer Learning):** ì´ ì˜ëŒ€ìƒì´ 'í”¼ë¶€ê³¼' ì „ë¬¸ì˜ê°€ ë˜ê¸°ë¡œ ê²°ì‹¬í•©ë‹ˆë‹¤.\n",
                "- **ë¯¸ì„¸ ì¡°ì • (Fine-tuning):** ì´ì œ ì˜í•™ ì„œì ì€ ê·¸ë§Œ ë³´ê³ , 'í”¼ë¶€ê³¼ ì „ê³µ ì„œì (NSMC ë°ì´í„°)'ë§Œ ì§‘ì¤‘ì ìœ¼ë¡œ ë´…ë‹ˆë‹¤. ì´ë¯¸ ê¸°ì´ˆê°€ íŠ¼íŠ¼í•´ì„œ ê¸ˆë°© ì „ë¬¸ê°€ê°€ ë©ë‹ˆë‹¤.\n",
                "- **ê²°ê³¼:** ì²˜ìŒë¶€í„° ì˜í•™ì„ ë°°ìš°ëŠ” ê²ƒë³´ë‹¤(ì²˜ìŒë¶€í„° í•™ìŠµ), ì˜ëŒ€ìƒ ë°ë ¤ì™€ì„œ í”¼ë¶€ê³¼ë§Œ ê°€ë¥´ì¹˜ëŠ” ê²Œ(ì „ì´ í•™ìŠµ) í›¨ì”¬ ë¹ ë¥´ê³  ì„±ëŠ¥ì´ ì¢‹ìŠµë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "aa368941",
            "metadata": {},
            "source": [
                "### 1. ë°ì´í„° ì¤€ë¹„í•˜ê¸° (NSMC)\n",
                "ë„¤ì´ë²„ ì˜í™” ê¸ì •/ë¶€ì • ë¦¬ë·° ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1e6b2544",
            "metadata": {
                "id": "load_data"
            },
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from tensorflow.keras.utils import get_file\n",
                "\n",
                "# 1. ì¸í„°ë„·ì—ì„œ ë°ì´í„° íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ì¼€ë¼ìŠ¤ ìœ í‹¸ë¦¬í‹° ì‚¬ìš©)\n",
                "# train: í•™ìŠµìš© ë°ì´í„°, test: í‰ê°€ìš© ë°ì´í„°\n",
                "train_file = get_file('ratings_train.txt', origin='https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt')\n",
                "test_file = get_file('ratings_test.txt', origin='https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt')\n",
                "\n",
                "# 2. íŒë‹¤ìŠ¤ë¡œ ì½ì–´ì˜¤ê¸° (êµ¬ë¶„ìëŠ” íƒ­ë¬¸ì '\\t')\n",
                "ratings_train_df = pd.read_csv(train_file, sep='\\t')\n",
                "ratings_test_df = pd.read_csv(test_file, sep='\\t')\n",
                "\n",
                "# ë°ì´í„° ëª¨ì–‘ í™•ì¸\n",
                "print(\"í•™ìŠµ ë°ì´í„° ê°œìˆ˜:\", len(ratings_train_df))\n",
                "print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°œìˆ˜:\", len(ratings_test_df))\n",
                "ratings_train_df.head() # ìƒìœ„ 5ê°œ ì¶œë ¥"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d6f021f4",
            "metadata": {
                "id": "preprocess_data"
            },
            "outputs": [],
            "source": [
                "# 3. ê²°ì¸¡ì¹˜ ì œê±° (ë¹ˆ ì¤„ì´ ìˆì„ ìˆ˜ ìˆìŒ)\n",
                "ratings_train_df = ratings_train_df.dropna(how='any')\n",
                "ratings_test_df = ratings_test_df.dropna(how='any')\n",
                "\n",
                "print(\"ê²°ì¸¡ì¹˜ ì œê±° í›„ í•™ìŠµ ë°ì´í„°:\", len(ratings_train_df))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4b96e9a7",
            "metadata": {
                "id": "sampling"
            },
            "outputs": [],
            "source": [
                "# [ì‹¤ìŠµ í¸ì˜ë¥¼ ìœ„í•´ ë°ì´í„° ì¼ë¶€ë§Œ ì‚¬ìš©]\n",
                "# ì „ì²´ ë°ì´í„°ë¥¼ ë‹¤ ì“°ë©´ ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¬ë‹ˆ, í•™ìŠµìš© 1.5ë§Œ ê°œ, í…ŒìŠ¤íŠ¸ìš© 5ì²œ ê°œë§Œ ë½‘ì•„ì„œ ì”ë‹ˆë‹¤.\n",
                "ratings_train_df = ratings_train_df.sample(n=15000, random_state=0)\n",
                "ratings_test_df = ratings_test_df.sample(n=5000, random_state=0)\n",
                "\n",
                "# ë°ì´í„°(X)ì™€ ë¼ë²¨(y) ë¶„ë¦¬\n",
                "X_train = ratings_train_df['document'].values.tolist()\n",
                "y_train = ratings_train_df['label'].values.tolist()\n",
                "\n",
                "X_test = ratings_test_df['document'].values.tolist()\n",
                "y_test = ratings_test_df['label'].values.tolist()\n",
                "\n",
                "print(\"ìƒ˜í”Œë§ í›„:\", len(X_train), len(X_test))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6df63294",
            "metadata": {},
            "source": [
                "### 2. í† í¬ë‚˜ì´ì € & ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
                "í•œêµ­ì–´ ì²˜ë¦¬ì— íŠ¹í™”ëœ **KLUE-BERT** ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8bcafbf0",
            "metadata": {
                "id": "load_bert"
            },
            "outputs": [],
            "source": [
                "from transformers import AutoTokenizer, TFBertForSequenceClassification\n",
                "\n",
                "MODEL_NAME = 'klue/bert-base' # í•œêµ­ì–´ BERT ëª¨ë¸\n",
                "\n",
                "# 1. í† í¬ë‚˜ì´ì € ë¡œë“œ (í•œêµ­ì–´ ë‹¨ì–´ì¥ + ìª¼ê°œëŠ” ê·œì¹™)\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "\n",
                "# 2. ëª¨ë¸ ë¡œë“œ (ë¨¸ë¦¬)\n",
                "# num_labels=2: ê¸ì •(1), ë¶€ì •(0) 2ê°œë¡œ ë¶„ë¥˜í•˜ë‹ˆê¹Œ 2ê°œ.\n",
                "# from_pt=True: íŒŒì´í† ì¹˜ë¡œ í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ í…ì„œí”Œë¡œìš°ë¡œ ë³€í™˜í•´ì„œ ê°€ì ¸ì˜¨ë‹¤ëŠ” ëœ»\n",
                "model = TFBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2, from_pt=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eb302549",
            "metadata": {
                "id": "tokenize_data"
            },
            "outputs": [],
            "source": [
                "# 3. ë°ì´í„° í† í°í™” (ê¸€ì -> ìˆ«ì)\n",
                "# padding=True: ë¬¸ì¥ ê¸¸ì´ë¥¼ ë§ì¶°ì¤Œ (ì§§ì€ ë¬¸ì¥ì€ ë’¤ì— 0ì„ ì±„ì›€)\n",
                "# truncation=True: ë„ˆë¬´ ê¸´ ë¬¸ì¥ì€ ìë¦„ (BERTëŠ” ìµœëŒ€ 512ìê¹Œì§€ë§Œ ê°€ëŠ¥)\n",
                "\n",
                "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, return_tensors='tf')\n",
                "X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, return_tensors='tf')\n",
                "\n",
                "# ê²°ê³¼ í™•ì¸: ì²« ë²ˆì§¸ ë¬¸ì¥ì´ ì–´ë–»ê²Œ ë°”ë€Œì—ˆë‚˜?\n",
                "print(\"ì›ë³¸:\", X_train[0])\n",
                "print(\"ìˆ«ì(ì¸ì½”ë”©):\", X_train_tokenized['input_ids'][0])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "22f34a83",
            "metadata": {},
            "source": [
                "### 3. ê³ ì„±ëŠ¥ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ë§Œë“¤ê¸° (tf.data)\n",
                "ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ í…ì„œí”Œë¡œìš°ì˜ `Dataset` ê¸°ëŠ¥ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6b9143b5",
            "metadata": {
                "id": "tf_dataset"
            },
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "\n",
                "# 1. ë°ì´í„°ì…‹ ìƒì„± (ì…ë ¥ê°’ dict + ë¼ë²¨)\n",
                "train_ds = tf.data.Dataset.from_tensor_slices((dict(X_train_tokenized), y_train))\n",
                "test_ds = tf.data.Dataset.from_tensor_slices((dict(X_test_tokenized), y_test))\n",
                "\n",
                "# 2. ì…”í”Œ(ì„ê¸°) & ë°°ì¹˜(ë¬¶ê¸°) & í”„ë¦¬í˜ì¹˜(ë¯¸ë¦¬ë¡œë”©)\n",
                "# prefetch: í•™ìŠµí•˜ëŠ” ë™ì•ˆ ë‹¤ìŒ ë°°ì¹˜ë¥¼ ë¯¸ë¦¬ ë©”ëª¨ë¦¬ì— ì˜¬ë ¤ë†”ì„œ ëŠê¹€ ì—†ì´ í•™ìŠµí•˜ê²Œ í•¨\n",
                "train_dataset = train_ds.shuffle(10000).batch(64).prefetch(tf.data.AUTOTUNE)\n",
                "test_dataset = test_ds.batch(64).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "print(\"ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4c643a66",
            "metadata": {},
            "source": [
                "### 4. í•™ìŠµ ì‹œì‘ (Fine-tuning)\n",
                "ì´ì œ ì˜ëŒ€ìƒ(BERT)ì—ê²Œ í”¼ë¶€ê³¼ ì „ê³µ ì„œì (ì˜í™” ë¦¬ë·°)ì„ ê°€ë¥´ì¹  ì‹œê°„ì…ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6fcedfc0",
            "metadata": {
                "id": "training"
            },
            "outputs": [],
            "source": [
                "from transformers import create_optimizer\n",
                "\n",
                "# 1. ìµœì í™” ë„êµ¬(Optimizer) ì„¤ì •\n",
                "# Warmup: ì²˜ìŒì—” ì²œì²œíˆ ë°°ìš°ë‹¤ê°€(ì „ì²´ ìŠ¤í…ì˜ 10%) ì ì  ì†ë„ë¥¼ ëƒ„. í•™ìŠµ ì•ˆì •ì„±ì„ ìœ„í•´ ì¤‘ìš”!\n",
                "num_train_steps = len(train_dataset) * 3  # Epochs = 3\n",
                "num_warmup_steps = int(num_train_steps * 0.1)\n",
                "\n",
                "optimizer, _ = create_optimizer(\n",
                "    init_lr=5e-5,            # í•™ìŠµë¥  (ë„ˆë¬´ í¬ë©´ ê¸°ì¡´ ì§€ì‹ì„ ê¹Œë¨¹ìŒ)\n",
                "    num_train_steps=num_train_steps,\n",
                "    num_warmup_steps=num_warmup_steps,\n",
                "    weight_decay_rate=0.01\n",
                ")\n",
                "\n",
                "# 2. ëª¨ë¸ ì»´íŒŒì¼ (í•™ìŠµ ì¤€ë¹„)\n",
                "model.compile(\n",
                "    optimizer=optimizer,\n",
                "    loss=model.hf_compute_loss, # HuggingFace ëª¨ë¸ ì „ìš© ì†ì‹¤ í•¨ìˆ˜\n",
                "    metrics=['accuracy']        # ì •í™•ë„ ì¸¡ì •\n",
                ")\n",
                "\n",
                "# 3. ì‹¤ì œ í•™ìŠµ ì§„í–‰ (fit)\n",
                "# ë°ì´í„°ë¥¼ ë„£ê³  3ë²ˆ ë°˜ë³µ(Epochs=3)í•´ì„œ ê³µë¶€ì‹œí‚µë‹ˆë‹¤.\n",
                "history = model.fit(\n",
                "    train_dataset,\n",
                "    validation_data=test_dataset,\n",
                "    epochs=3\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9f7c1bf3",
            "metadata": {},
            "source": [
                "### 5. ì‹¤ì „ í…ŒìŠ¤íŠ¸ (Inference)\n",
                "í•™ìŠµëœ ëª¨ë¸ì´ ì§„ì§œë¡œ ë¦¬ë·°ë¥¼ ì˜ ë¶„ì„í•˜ëŠ”ì§€ í™•ì¸í•´ë´…ì‹œë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c1e45ef8",
            "metadata": {
                "id": "inference_pipeline"
            },
            "outputs": [],
            "source": [
                "from transformers import TextClassificationPipeline\n",
                "\n",
                "# ë¼ë²¨ ì´ë¦„ ë¶™ì—¬ì£¼ê¸°\n",
                "model.config.id2label = {0: \"ë¶€ì •\", 1: \"ê¸ì •\"}\n",
                "model.config.label2id = {\"ë¶€ì •\": 0, \"ê¸ì •\": 1}\n",
                "\n",
                "# ì‚¬ìš©í•˜ê¸° í¸í•˜ê²Œ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë§Œë“¤ê¸°\n",
                "sentiment_classifier = TextClassificationPipeline(\n",
                "    tokenizer=tokenizer,\n",
                "    model=model,\n",
                "    framework='tf',\n",
                "    top_k=1  # ê°€ì¥ í™•ë¥  ë†’ì€ 1ê°œë§Œ ì¶œë ¥\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4b2f73a1",
            "metadata": {
                "id": "inference_test"
            },
            "outputs": [],
            "source": [
                "# ì§ì ‘ ë¬¸ì¥ì„ ì¨ì„œ í…ŒìŠ¤íŠ¸í•´ë³´ì„¸ìš”!\n",
                "reviews = [\n",
                "    \"ì´ ì˜í™” ì§„ì§œ ì¸ìƒ ì˜í™”ë‹¤... ëˆˆë¬¼ ì½§ë¬¼ ì™ ëºìŒ ã… ã… \",\n",
                "    \"ëˆ ì•„ê¹Œì›Œ ì£½ëŠ” ì¤„ ì•Œì•˜ë‹¤. ë„ëŒ€ì²´ ë­˜ ë§í•˜ê³  ì‹¶ì€ ê±´ì§€ ëª¨ë¥´ê² ìŒ.\",\n",
                "    \"ë°°ìš°ë“¤ ì—°ê¸°ëŠ” ì¢‹ì€ë° ìŠ¤í† ë¦¬ê°€ ì¢€ ì‚°ìœ¼ë¡œ ê°€ë„¤.\"\n",
                "]\n",
                "\n",
                "for review in reviews:\n",
                "    result = sentiment_classifier(review)\n",
                "    print(f\"ë¦¬ë·°: {review}\\nê²°ê³¼: {result}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "759d47b5",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7ac61b18",
            "metadata": {},
            "source": [
                "### (ì„ íƒ) ë‚´ ëª¨ë¸ ì €ì¥í•˜ê³  ê³µìœ í•˜ê¸°\n",
                "`push_to_hub`ë¥¼ ì‚¬ìš©í•˜ë©´ Hugging Face Hubì— ë‚´ ëª¨ë¸ì„ ì—…ë¡œë“œí•´ì„œ ì–¸ì œë“  ë‹¤ì‹œ êº¼ë‚´ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
                "*(ì´ ë‹¨ê³„ëŠ” Hugging Face ê³„ì •ê³¼ í† í°ì´ í•„ìš”í•©ë‹ˆë‹¤)*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "83532273",
            "metadata": {
                "id": "push_to_hub"
            },
            "outputs": [],
            "source": [
                "# from huggingface_hub import notebook_login\n",
                "# notebook_login() # ì—¬ê¸°ì„œ ë¡œê·¸ì¸\n",
                "\n",
                "# REPO_NAME = 'my-first-bert-nsmc' \n",
                "# model.push_to_hub(REPO_NAME)\n",
                "# tokenizer.push_to_hub(REPO_NAME)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "python3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
