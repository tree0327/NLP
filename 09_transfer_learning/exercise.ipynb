{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 05. BERT & GPT 기본 구조 뜯어보기\n",
                "### 과목: NLP Transfer Learning\n",
                "---\n",
                "**학습 목표**\n",
                "1. **BERT**(Encoder)가 문장을 숫자로 바꾸는 과정을 확인한다. (`last_hidden_state`)\n",
                "2. **GPT**(Decoder)가 문장을 생성하는 과정을 확인한다. (`generate`)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. BERT: 문장을 이해하는 모델 (Encoder)\n",
                "BERT는 문장을 입력받아 **모든 토큰의 의미를 포함한 벡터**로 변환합니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "61edffee"
            },
            "outputs": [],
            "source": [
                "from transformers import BertTokenizer, BertModel\n",
                "\n",
                "# 1. 사전학습된 BERT 토크나이저와 모델을 불러옵니다.\n",
                "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
                "model = BertModel.from_pretrained('bert-base-uncased')\n",
                "\n",
                "# 2. 입력 문장 준비\n",
                "text = \"Natural language processing is fascinating.\"\n",
                "\n",
                "# 3. 토큰화 및 텐서 변환 (return_tensors=\"pt\": 파이토치 텐서로 반환)\n",
                "inputs = tokenizer(text, return_tensors=\"pt\")\n",
                "\n",
                "print(\"입력 토큰 ID:\", inputs['input_ids'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "70542499"
            },
            "outputs": [],
            "source": [
                "# 4. 모델 통과 (Forward Pass)\n",
                "# **inputs: dictionary의 키-값을 풀어서 인자로 전달\n",
                "outputs = model(**inputs)\n",
                "\n",
                "# 5. 결과 확인\n",
                "# last_hidden_state: 마지막 레이어의 출력 벡터 (문맥 정보가 담김)\n",
                "# 크기(Shape): (Batch_size, Sequence_len, Hidden_dim)\n",
                "print(\"출력 벡터 크기:\", outputs.last_hidden_state.shape)\n",
                "print(\"첫 번째 토큰([CLS])의 벡터:\", outputs.last_hidden_state[0, 0, :5]) # 앞부분 5개만 출력"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. GPT: 문장을 생성하는 모델 (Decoder)\n",
                "GPT는 앞 단어들을 보고 **다음에 올 단어를 예측**하며 문장을 만들어갑니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "90c1a68a"
            },
            "outputs": [],
            "source": [
                "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
                "\n",
                "# 1. GPT-2 로드\n",
                "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
                "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
                "\n",
                "# 2. 시작 문장(Prompt) 입력\n",
                "input_text = \"Artificial Intelligence is\"\n",
                "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
                "\n",
                "print(\"입력 토큰:\", inputs['input_ids'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "8b0b0304"
            },
            "outputs": [],
            "source": [
                "# 3. 문장 생성 (Generate)\n",
                "outputs = model.generate(\n",
                "    inputs['input_ids'],      # 입력 토큰\n",
                "    max_length=20,            # 최대 길이 (입력 포함)\n",
                "    num_return_sequences=1,   # 생성할 문장 개수\n",
                "    pad_token_id=tokenizer.eos_token_id # 패딩 토큰 설정 (GPT는 기본 패딩 토큰이 없어서 EOS로 설정)\n",
                ")\n",
                "\n",
                "print(\"생성된 토큰 ID:\", outputs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "6d0cbdc4"
            },
            "outputs": [],
            "source": [
                "# 4. 디코딩 (숫자 -> 글자)\n",
                "# skip_special_tokens=True: 특수 토큰 제거\n",
                "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "\n",
                "print(\"\\n--- 생성 결과 ---\")\n",
                "print(generated_text)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "python3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}