{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "236748af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리된 텍스트: hello 자연어 처리 is fun do you agree\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 원시 텍스트\n",
    "raw_text = \"Hello!!! 자연어 처리 is FUN.   Do you agree? :)\"\n",
    "\n",
    "# 불필요한 기호 제거 : \\w(문자/숫자/_), \\s(공백)만 남기고 나머지 특수문자는 제거\n",
    "clean_text = re.sub(r\"[^\\w\\s]\", \"\", raw_text)\n",
    "\n",
    "# 소문자 변환 : 영어 대문자를 소문자로 통일\n",
    "clean_text = clean_text.lower()\n",
    "\n",
    "# 공백 제거 : 연속 공백/앞뒤 공백 제거 후 단어 사이를 공백 1칸으로 정리\n",
    "clean_text = \" \".join(clean_text.split())\n",
    "\n",
    "print(\"처리된 텍스트:\", clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88acbcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['자연어 처리는 재미있다!', '하지만 배우기는 어렵다.', 'Python으로 가능합니다.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"자연어 처리는 재미있다! 하지만 배우기는 어렵다. Python으로 가능합니다.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f3d8d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords          # 불용어 목록\n",
    "from nltk.tokenize import word_tokenize    # 문장을 토큰(단어) 단위로 분리하는 토크나이저\n",
    "import string\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5358b271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{33: None,\n",
       " 34: None,\n",
       " 35: None,\n",
       " 36: None,\n",
       " 37: None,\n",
       " 38: None,\n",
       " 39: None,\n",
       " 40: None,\n",
       " 41: None,\n",
       " 42: None,\n",
       " 43: None,\n",
       " 44: None,\n",
       " 45: None,\n",
       " 46: None,\n",
       " 47: None,\n",
       " 58: None,\n",
       " 59: None,\n",
       " 60: None,\n",
       " 61: None,\n",
       " 62: None,\n",
       " 63: None,\n",
       " 64: None,\n",
       " 91: None,\n",
       " 92: None,\n",
       " 93: None,\n",
       " 94: None,\n",
       " 95: None,\n",
       " 96: None,\n",
       " 123: None,\n",
       " 124: None,\n",
       " 125: None,\n",
       " 126: None}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str.maketrans(\"\", \"\", string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "440c5b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'자연어 처리는 데이터 과학의 한 분야다 여러 전처리가 필요하다'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원시 텍스트\n",
    "raw_text = \"자연어 처리123는 데이터 과학의 한 분야다! 여러 전처리가 필요하다.\"\n",
    "\n",
    "# 특수문자 제거\n",
    "clean_text = raw_text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "# 숫자 제거\n",
    "clean_text = re.sub(r\"\\d+\", \"\", clean_text)\n",
    "\n",
    "clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b94d3473",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No such file or directory: 'C:\\\\Users\\\\Playdata\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords\\\\korean'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 불용어 제거\u001b[39;00m\n\u001b[32m      2\u001b[39m tokens = word_tokenize(clean_text)    \u001b[38;5;66;03m# 문장을 토큰 리스트로 토큰화\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m stop_words = \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkorean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m      4\u001b[39m filtered_tokens = [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]    \u001b[38;5;66;03m# 불용어에 해당하지 않는 토큰만 남김\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m전처리 결과:\u001b[39m\u001b[33m\"\u001b[39m, filtered_tokens)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\nlp\\nlp_venv\\Lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:21\u001b[39m, in \u001b[36mWordListCorpusReader.words\u001b[39m\u001b[34m(self, fileids, ignore_lines_startswith)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwords\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileids=\u001b[38;5;28;01mNone\u001b[39;00m, ignore_lines_startswith=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m     20\u001b[39m         line\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m line_tokenize(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileids\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     22\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line.startswith(ignore_lines_startswith)\n\u001b[32m     23\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\nlp\\nlp_venv\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:218\u001b[39m, in \u001b[36mCorpusReader.raw\u001b[39m\u001b[34m(self, fileids)\u001b[39m\n\u001b[32m    216\u001b[39m contents = []\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[32m    219\u001b[39m         contents.append(fp.read())\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m concat(contents)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\nlp\\nlp_venv\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:231\u001b[39m, in \u001b[36mCorpusReader.open\u001b[39m\u001b[34m(self, file)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    224\u001b[39m \u001b[33;03mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[32m    225\u001b[39m \u001b[33;03mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    228\u001b[39m \u001b[33;03m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    230\u001b[39m encoding = \u001b[38;5;28mself\u001b[39m.encoding(file)\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_root\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m.open(encoding)\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\nlp\\nlp_venv\\Lib\\site-packages\\nltk\\data.py:333\u001b[39m, in \u001b[36mFileSystemPathPointer.join\u001b[39m\u001b[34m(self, fileid)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mjoin\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileid):\n\u001b[32m    332\u001b[39m     _path = os.path.join(\u001b[38;5;28mself\u001b[39m._path, fileid)\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFileSystemPathPointer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\nlp\\nlp_venv\\Lib\\site-packages\\nltk\\data.py:311\u001b[39m, in \u001b[36mFileSystemPathPointer.__init__\u001b[39m\u001b[34m(self, _path)\u001b[39m\n\u001b[32m    309\u001b[39m _path = os.path.abspath(_path)\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(_path):\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo such file or directory: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m % _path)\n\u001b[32m    312\u001b[39m \u001b[38;5;28mself\u001b[39m._path = _path\n",
      "\u001b[31mOSError\u001b[39m: No such file or directory: 'C:\\\\Users\\\\Playdata\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords\\\\korean'"
     ]
    }
   ],
   "source": [
    "# 불용어 제거\n",
    "tokens = word_tokenize(clean_text)    # 문장을 토큰 리스트로 토큰화\n",
    "stop_words = set(stopwords.words(\"korean\"))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]    # 불용어에 해당하지 않는 토큰만 남김\n",
    "\n",
    "print(\"전처리 결과:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0007a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Using cached konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting JPype1>=0.7.0 (from konlpy)\n",
      "  Using cached jpype1-1.6.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting lxml>=4.1.0 (from konlpy)\n",
      "  Using cached lxml-6.0.2-cp312-cp312-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from konlpy) (2.4.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\playdata\\nlp\\nlp_venv\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
      "Using cached konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "Using cached jpype1-1.6.0-cp312-cp312-win_amd64.whl (355 kB)\n",
      "Using cached lxml-6.0.2-cp312-cp312-win_amd64.whl (4.0 MB)\n",
      "Installing collected packages: lxml, JPype1, konlpy\n",
      "\n",
      "   ---------------------------------------- 0/3 [lxml]\n",
      "   ---------------------------------------- 0/3 [lxml]\n",
      "   ---------------------------------------- 0/3 [lxml]\n",
      "   ---------------------------------------- 0/3 [lxml]\n",
      "   ---------------------------------------- 0/3 [lxml]\n",
      "   ------------- -------------------------- 1/3 [JPype1]\n",
      "   ------------- -------------------------- 1/3 [JPype1]\n",
      "   ------------- -------------------------- 1/3 [JPype1]\n",
      "   ------------- -------------------------- 1/3 [JPype1]\n",
      "   ------------- -------------------------- 1/3 [JPype1]\n",
      "   ------------- -------------------------- 1/3 [JPype1]\n",
      "   -------------------------- ------------- 2/3 [konlpy]\n",
      "   -------------------------- ------------- 2/3 [konlpy]\n",
      "   -------------------------- ------------- 2/3 [konlpy]\n",
      "   ---------------------------------------- 3/3 [konlpy]\n",
      "\n",
      "Successfully installed JPype1-1.6.0 konlpy-0.6.0 lxml-6.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba5d8e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['자연어', '처리', '데이터', '과학', '의', '분야', '다', '여러', '전', '처리', '필요하다']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re, string\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "# 원시 텍스트\n",
    "raw_text = \"자연어 처리123는 데이터 과학의 한 분야다! 여러 전처리가 필요하다.\"\n",
    "\n",
    "# 특수문자 제거\n",
    "clean_text = raw_text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "# 숫자 제거\n",
    "clean_text = re.sub(r\"\\d+\", \"\", clean_text)\n",
    "\n",
    "tokens = okt.morphs(clean_text)    # 형태소 단위 토큰화\n",
    "\n",
    "stop_words = {\"은\",\"는\",\"이\",\"가\",\"을\",\"를\",\"에\",\"에서\",\"와\",\"과\",\"도\",\"한\",\"하다\",\"되다\",\"있다\",\"없다\"}\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]    # 불용어에 해당하지 않는 토큰만 남김\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b739a76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소 분석 결과: ['자연어', '처리', '는', '정말', '재미있다', '!']\n",
      "품사 태깅 결과: [('자연어', 'Noun'), ('처리', 'Noun'), ('는', 'Josa'), ('정말', 'Noun'), ('재미있다', 'Adjective'), ('!', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "# 텍스트\n",
    "text = \"자연어 처리는 정말 재미있다!\"\n",
    "\n",
    "# 형태소 분석\n",
    "okt = Okt()\n",
    "morphs = okt.morphs(text)    # 문장을 형태소(최소 의미 단위) 리스트로 분리\n",
    "pos = okt.pos(text)          # (형태소, 품사) 튜플 리스트로 품사 태깅 수행\n",
    "\n",
    "print(\"형태소 분석 결과:\", morphs)\n",
    "print(\"품사 태깅 결과:\", pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27b62208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words:\\n [[0 0 0 1 1 1]\n",
      " [0 1 0 1 0 1]\n",
      " [1 0 1 0 0 0]]\n",
      "TF-IDF:\\n [[0.         0.         0.         0.51785612 0.68091856 0.51785612]\n",
      " [0.         0.68091856 0.         0.51785612 0.         0.51785612]\n",
      " [0.70710678 0.         0.70710678 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# CounterVectorizer(BoW), TFidfVectorizer(희귀도)\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# 샘플 데이터\n",
    "corpus = [\"자연어 처리는 재미있다.\", \"자연어 처리는 어렵다.\", \"데이터는 유용하다.\"]\n",
    "\n",
    "# Bag-of-Words : 문서-단어 행렬을 단어 등장 횟수로 만드는 BoW 벡터화 클래스\n",
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform(corpus)      # 코퍼스에 맞춰 단어사전 학습 + BoW 벡터로 변환(희소행렬)\n",
    "print(\"Bag-of-Words:\\\\n\", bow.toarray())    # 희소행렬을 배열로 변환해 출력\n",
    "\n",
    "# TF-IDF : 단어 빈도 (TF)와 문서 희소성 (IDF)를 반영해 중요 단어에 가중치를 주는 벡터화 클래스\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(corpus)  # 코퍼스에 맞춰 단어사전 학습 + TF-IDF 벡터로 변환 (희소행렬)\n",
    "print(\"TF-IDF:\\\\n\", tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47725c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['데이터는', '어렵다', '유용하다', '자연어', '재미있다', '처리는'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b5df4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['데이터는', '어렵다', '유용하다', '자연어', '재미있다', '처리는'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fb8b27",
   "metadata": {},
   "source": [
    "- TF-IDF 지표\n",
    "    - 값 = 빈도(TF) * 희소성(IDF) 기반의 가중치(중요도)\n",
    "    - 여러 문서에 흔하게 나오는 단어는 점수가 낮아지고, 특정 문서에만 주로 나오는 단어는 점수가 높아진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e8ee05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 빈도: Counter({'재미있다.': 2, '자연어': 1, '처리는': 1, '자연어는': 1, '어렵지만': 1})\n",
      "문장 길이: [3, 3]\n"
     ]
    }
   ],
   "source": [
    "# 샘플 텍스트\n",
    "text = \"자연어 처리는 재미있다. 자연어는 어렵지만 재미있다.\"\n",
    "\n",
    "# 단어 빈도 계산\n",
    "from collections import Counter\n",
    "words = text.split()\n",
    "word_count = Counter(words)\n",
    "\n",
    "# 문장 길이 계산\n",
    "sentences = text.split(\". \")\n",
    "# 각 문장을 공백 기준으로 나누고, 단어 길이 계산\n",
    "sentence_lengths = [len(sentence.split()) for sentence in sentences]\n",
    "\n",
    "print(\"단어 빈도:\", word_count)\n",
    "print(\"문장 길이:\", sentence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3010f407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "할인 관련 메시지입니다.\n"
     ]
    }
   ],
   "source": [
    "# 간단한 규칙기반\n",
    "text = \"고객님, 오늘의 할인 코드는 SAVE20입니다.\"\n",
    "\n",
    "if \"할인\" in text:\n",
    "    print(\"할인 관련 메시지입니다.\")\n",
    "else:\n",
    "    print(\"일반 메시지입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218ca7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
