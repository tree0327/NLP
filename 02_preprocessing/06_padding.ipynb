{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f957d38",
   "metadata": {},
   "source": [
    "# Padding\n",
    "\n",
    "- 자연어 처리에서 각 문장(문서)의 길이는 서로 다를 수 있음\n",
    "- 하지만 대부분의 모델은 고정 길이 입력을 기준으로 배치(batch) 단위 학습을 효율적으로 수행함\n",
    "- 따라서 모든 문장의 길이를 동일한 길이(maxlen) 로 맞춰주는 작업이 필요함 → Padding\n",
    "\n",
    "**Padding 개념**\n",
    "- Padding: 짧은 문장에 PAD 같은 특수 토큰(보통 0) 을 채워 길이를 맞춤\n",
    "- Truncation(잘라내기): 너무 긴 문장은 maxlen 기준으로 일정 길이까지만 남기고 자름\n",
    "- Padding 방향\n",
    "\t- post padding: 뒤에 채움(일반적으로 많이 사용)\n",
    "\t- pre padding: 앞에 채움(모델/설정에 따라 사용)\n",
    "\n",
    "**사용시기**\n",
    "- 문장/문서를 시퀀스(정수 인덱스)로 바꾼 뒤 모델 입력으로 넣을 때\n",
    "\t- 예: Embedding + RNN/LSTM/GRU, 1D CNN, Transformer 계열 등\n",
    "- 미니배치 학습(DataLoader/fit)에서 텐서 크기를 맞춰야 할 때\n",
    "\t- 배치로 묶으려면 (batch, seq_len) 형태로 길이가 동일해야 함\n",
    "- 평가/추론에서도 동일하게 적용\n",
    "\t- 학습 때 사용한 maxlen 기준으로 테스트/서비스 입력도 동일 처리 필요\n",
    "\n",
    "**코드 내 사용 위치**\n",
    "- 텍스트 정제(소문자화/특수문자 처리 등)\n",
    "- 토큰화(단어/서브워드)\n",
    "- 정수 인코딩(Tokenizer, vocab 매핑)\n",
    "- Padding/Truncation 적용\n",
    "- 모델 입력(Embedding/Encoder) → 학습/평가\n",
    "\n",
    "**Padding 이점**\n",
    "- 일관된 입력 형식: 모든 문장이 동일한 길이의 시퀀스로 변환되어 모델 입력이 단순해짐\n",
    "- 병렬 연산 최적화: 배치 단위 텐서 연산이 가능해져 GPU/행렬 연산 효율이 좋아짐\n",
    "- 유연한 데이터 처리: 다양한 길이의 문서를 동일한 파이프라인으로 처리 가능\n",
    "\n",
    "**주의사항(중요)**\n",
    "- PAD는 의미 없는 값이므로 학습에 영향을 주면 안 됨\n",
    "\t- Masking(마스킹) 으로 PAD 위치를 손실 계산/어텐션 계산에서 제외하는 경우가 많음\n",
    "- maxlen을 너무 크게 잡으면 PAD가 과도하게 늘어 연산 낭비 + 성능 저하가 생길 수 있음\n",
    "\t- 문장 길이 분포를 보고 적절한 maxlen을 선택하는 것이 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03538246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99e953d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1+cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7da09862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리된 문장(토큰 리스트)\n",
    "preprocessed_sentences = [\n",
    "    ['barber', 'person'],\n",
    "    ['barber', 'good', 'person'],\n",
    "    ['barber', 'huge', 'person'],\n",
    "    ['knew', 'secret'],\n",
    "    ['secret', 'kept', 'huge', 'secret'],\n",
    "    ['huge', 'secret'],\n",
    "    ['barber', 'kept', 'word'],\n",
    "    ['barber', 'kept', 'word'],\n",
    "    ['barber', 'kept', 'secret'],\n",
    "    ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
    "    ['barber', 'went', 'huge', 'mountain']\n",
    "]    # 문장별로 토큰화/정제된 결과를 리스트로 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883780bf",
   "metadata": {},
   "source": [
    "##### 직접 구현\n",
    "- Tokenizer + OOV 처리 + 시퀀스 변환 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a72bf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter    # 단어 빈도 계산용\n",
    "\n",
    "class TokenizerForPadding:\n",
    "    def __init__(self, num_words=None, oov_token='<OOV>'):\n",
    "        self.num_words = num_words     # 사용할 최대 단어 수 (어휘 제한), None이면 제한 없음\n",
    "        self.oov_token = oov_token     # 사전에 없는 단어를 치환할 OOV 토큰 문자열\n",
    "        self.word_index = {}           # 단어 -> 인덱스 매핑 딕셔너리\n",
    "        self.index_word = {}           # 인덱스 -> 단어 매핑 딕셔너리\n",
    "        self.word_counts = Counter()   # 단어 빈도수\n",
    "    \n",
    "    # 단어사전 생성 및 단어 -> 인덱스, 인덱스 -> 단어 딕셔너리 생성하는 함수\n",
    "    def fit_on_texts(self, texts):\n",
    "        \n",
    "        # 빈도수 세기\n",
    "        for sentence in texts:    # 문장(토큰 리스트) 단위로 순회\n",
    "            self.word_counts.update(word for word in sentence if word)    # 빈 문자열 제외하고 단ㅇ러 빈도 누적\n",
    "\n",
    "        # 빈도수 기반으로 vocabulary 생성\n",
    "        vocab = [self.oov_token] + [word for word, _ in self.word_counts.most_common(self.num_words - 2 if self.num_words else None)]  # 빈도 상위 단어\n",
    "\n",
    "        self.word_index = {word: i+1 for i, word in enumerate(vocab)}  # 1번(<OOV>)부터 단어 -> 인덱스\n",
    "        self.index_word = {i: word for word, i in self.word_index.items()}  # 인덱스 -> 단어 역매핑\n",
    "    \n",
    "    def texts_to_sequences(self, texts):\n",
    "        return [[self.word_index.get(word, self.word_index[self.oov_token]) for word in sentence] for sentence in texts]  # 단어를 인덱스로 변환 (없으면 OOV 인덱스)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd53402",
   "metadata": {},
   "source": [
    "vocab = [self.oov_token] + [word for word, _ in self.word_counts.most_common(self.num_words - 2 if self.num_words else None)]  \n",
    "[self.oov_token] : OOV 토큰을 voca의 첫 원소로 추가  \n",
    "self.word_counts.most_common(k) : 등장 횟수가 많은 순서대로 (단어, 빈도) 튜플을 k개 반환  \n",
    "self.num_words - 2 if self.num_words else None : num_words를 지정했으면 상위단어로 num_words -2개 반환  \n",
    "-2의 이유는 하나(0)은 padding 용도, 다른 하나는 <OOV> 용도로 자리 2개를 비워놓는다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b334bb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시퀀스 길이를 맞추기 위한 Padding/Truncation(잘라내기) 함수\n",
    "def pad_sequences(sequences, maxlen=None, padding='pre', truncating='pre', value=0):\n",
    "    if maxlen is None:                                 # 최대 길이를 따로 설정안하면\n",
    "        maxlen = max(len(seq) for seq in sequences)    # 입력 시퀀스들 중 가장 긴 길이를 maxlen으로 사용\n",
    "\n",
    "    padded_sequences = []    # 패딩/트렁케이션 적용 결과 리스트\n",
    "\n",
    "    for seq in sequences:\n",
    "        \n",
    "        if len(seq) > maxlen:                      # 시퀀스가 maxlen보다 길면 (Truncation)\n",
    "            if truncating == 'pre':                # pre 인 경우\n",
    "                seq = seq[-maxlen:]                # seq는 뒤에서 maxlen개만 남김\n",
    "            else:                                  # post 인 경우\n",
    "                seq = seq[:maxlen]                 # 앞에서 maxlen개를 남김\n",
    "        \n",
    "        else:                                      # 시퀀스가 maxlen보다 작으면 (Padding)\n",
    "            pad_length = maxlen - len(seq)         # 패딩 길이 계산\n",
    "\n",
    "            if padding == 'pre':                   # pre 인 경우\n",
    "                seq = [value] * pad_length + seq   # 앞쪽에 0 패딩을 추가 (PAD ... + 원본)\n",
    "            else:                                  # post 인 경우\n",
    "                seq = seq + [value] * pad_length   # 뒤쪽에 0 패딩을 추가 (원본 + PAD ...)\n",
    "        \n",
    "        padded_sequences.append(seq)               # 처리된 시퀀스를 결과 리스트에 추가\n",
    "    \n",
    "    return torch.tensor(padded_sequences)          # 최종 결과를 Pytorch 텐서로 변환해 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611e5515",
   "metadata": {},
   "source": [
    "반환값은 (문장개수, maxlen) 형태의 2차원 텐서.  \n",
    "짧은 시퀀스는 value로 채워지고 (padding), 긴 시퀀스는 truncating 기준으로 잘린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b31ac592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 6],\n",
       " [2, 9, 6],\n",
       " [2, 4, 6],\n",
       " [10, 3],\n",
       " [3, 5, 4, 3],\n",
       " [4, 3],\n",
       " [2, 5, 7],\n",
       " [2, 5, 7],\n",
       " [2, 5, 3],\n",
       " [8, 8, 4, 3, 11, 2, 12],\n",
       " [2, 13, 4, 14]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TokenizerForPadding(num_words=15)     # 상위 15개 단어 기반으로 단어사전을 만들 토크나이저 생성\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)    # 전처리 문장들을 학습해 단어 빈도/인덱스 사전 구축\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_sentences)    # 각 문장을 단어 -> 정수 인덱스로 변환해 시퀀스 리스트 생성\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c73a39d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  6,  0],\n",
       "        [ 2,  9,  6],\n",
       "        [ 2,  4,  6],\n",
       "        [10,  3,  0],\n",
       "        [ 5,  4,  3],\n",
       "        [ 4,  3,  0],\n",
       "        [ 2,  5,  7],\n",
       "        [ 2,  5,  7],\n",
       "        [ 2,  5,  3],\n",
       "        [11,  2, 12],\n",
       "        [13,  4, 14]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시퀀스 길이를 3으로 맞추는 패딩/트렁케이션 적용\n",
    "padded = pad_sequences(sequences, padding='post', truncating='pre', maxlen=3)    # 길이가 3보다 짧으면 뒤(post)에 0 패딩, 길면 앞(pre)을 버리고 뒤 3개만 유지\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14b507d",
   "metadata": {},
   "source": [
    "- 예제처럼 긴문장의 뒤를 남길 때\n",
    "\t- 핵심 정보가 문장 끝에 몰리는 데이터가 몰리는 경우\n",
    "\t\t- 리뷰/감정: “... but I hated it”, “... not good” 처럼 결론이 뒤에 오는 경우\n",
    "\t\t- 질의/명령: “I want to book a ticket tomorrow” 같은 핵심 슬롯이 뒤에 붙는 경우\n",
    "\t- RNN/LSTM 처럼 순차적으로 보는 모델은 마지막 쪽 토큰 영향이 더 크게 남는 경우가 많음\n",
    "- 앞을 남길 때 (post truncating)\n",
    "\t- 뉴스/보고서처럼 서론(앞부분)에 핵심이 많은 데이터\n",
    "\t- 제목/헤드라인 기반 과제\n",
    "\t- 길이가 길어도 초반 컨텍스트가 중요한 태스크\n",
    "\n",
    "- 정답은 없음 → 데이터 특성에 맞춰 선택. 보통은 둘 다 돌려보고 성능 좋은 쪽 선택\n",
    "- 실제로는 maxlen을 충분히 크게 잡고(예: 50/100/200) truncation 정책을 정한다.\n",
    "- Transformer(BERT류)는 보통 앞(초반) 유지를 더 많이 쓰는 편"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cf47f9",
   "metadata": {},
   "source": [
    "### Keras Tokenizer 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f8c6210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리된 문장(토큰 리스트)\n",
    "preprocessed_sentences = [\n",
    "    ['barber', 'person'],\n",
    "    ['barber', 'good', 'person'],\n",
    "    ['barber', 'huge', 'person'],\n",
    "    ['knew', 'secret'],\n",
    "    ['secret', 'kept', 'huge', 'secret'],\n",
    "    ['huge', 'secret'],\n",
    "    ['barber', 'kept', 'word'],\n",
    "    ['barber', 'kept', 'word'],\n",
    "    ['barber', 'kept', 'secret'],\n",
    "    ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
    "    ['barber', 'went', 'huge', 'mountain']\n",
    "]    # 문장별로 토큰화/정제된 결과를 리스트로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "007f83f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 8, 5],\n",
       " [1, 3, 5],\n",
       " [9, 2],\n",
       " [2, 4, 3, 2],\n",
       " [3, 2],\n",
       " [1, 4, 6],\n",
       " [1, 4, 6],\n",
       " [1, 4, 2],\n",
       " [7, 7, 3, 2, 10, 1, 11],\n",
       " [1, 12, 3, 13]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer    # Keras 토크나이저 (단어 인덱싱/시퀀스 변환)\n",
    "\n",
    "tokenizer = Tokenizer()    # 기본 설정 (단어 개수 제한 없음, OOV 토큰 미지정)\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)    # 단어 빈도 기반 인덱스 (word_index) 생성\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_sentences)  # 문장(토큰 리스트)를 정수 인덱스로 변환\n",
    "sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2291e434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1,\n",
       " 'secret': 2,\n",
       " 'huge': 3,\n",
       " 'kept': 4,\n",
       " 'person': 5,\n",
       " 'word': 6,\n",
       " 'keeping': 7,\n",
       " 'good': 8,\n",
       " 'knew': 9,\n",
       " 'driving': 10,\n",
       " 'crazy': 11,\n",
       " 'went': 12,\n",
       " 'mountain': 13}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86254de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'barber'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.index_word\n",
    "tokenizer.index_word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa31c5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  0],\n",
       "       [ 1,  8,  5],\n",
       "       [ 1,  3,  5],\n",
       "       [ 9,  2,  0],\n",
       "       [ 2,  4,  3],\n",
       "       [ 3,  2,  0],\n",
       "       [ 1,  4,  6],\n",
       "       [ 1,  4,  6],\n",
       "       [ 1,  4,  2],\n",
       "       [ 7,  7,  3],\n",
       "       [ 1, 12,  3]], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keras pad_sequences 로 길이를 맞추기\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded = pad_sequences(\n",
    "    sequences,          # 정수 인덱스 시퀀스 리스트 ([문장별 [idx, idx, ...], ...])\n",
    "    padding='post',     # max보다 짧으면 뒤(post)에 0을 채워 패딩\n",
    "    truncating='post',  # 시퀀스 길이가 3보다 길면 뒤(post)를 자른다.\n",
    "    maxlen=3            # 모든 시퀀스 길이는 3\n",
    ")\n",
    "\n",
    "padded   # (문장개수, 3) 형태의 2D 배열 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ffe1f1",
   "metadata": {},
   "source": [
    "### [연습] 어린왕자 데이터 샘플 패딩처리\n",
    "\n",
    "1. 텍스트 전처리 (토큰화/불용어처리/정제/정규화)\n",
    "2. 정수 인코딩 by Tokenizer (tensorflow.keras)\n",
    "3. 패딩 처리 by pad_sequences (tensorflow.keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "437b927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\"The Little Prince, written by Antoine de Saint-Exupéry, is a poetic tale about a young prince who travels from his home planet to Earth. The story begins with a pilot stranded in the Sahara Desert after his plane crashes. While trying to fix his plane, he meets a mysterious young boy, the Little Prince.\n",
    "\n",
    "The Little Prince comes from a small asteroid called B-612, where he lives alone with a rose that he loves deeply. He recounts his journey to the pilot, describing his visits to several other planets. Each planet is inhabited by a different character, such as a king, a vain man, a drunkard, a businessman, a geographer, and a fox. Through these encounters, the Prince learns valuable lessons about love, responsibility, and the nature of adult behavior.\n",
    "\n",
    "On Earth, the Little Prince meets various creatures, including a fox, who teaches him about relationships and the importance of taming, which means building ties with others. The fox's famous line, \"You become responsible, forever, for what you have tamed,\" resonates with the Prince's feelings for his rose.\n",
    "\n",
    "Ultimately, the Little Prince realizes that the essence of life is often invisible and can only be seen with the heart. After sharing his wisdom with the pilot, he prepares to return to his asteroid and his beloved rose. The story concludes with the pilot reflecting on the lessons learned from the Little Prince and the enduring impact of their friendship.\n",
    "\n",
    "The narrative is a beautifully simple yet profound exploration of love, loss, and the importance of seeing beyond the surface of things.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a85d6eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['little', 'prince', 'written', 'antoine', 'saint-exupéry', 'poetic', 'tale', 'young', 'prince', 'travels', 'home', 'planet', 'earth'], ['story', 'begins', 'pilot', 'stranded', 'sahara', 'desert', 'plane', 'crashes'], ['trying', 'fix', 'plane', 'meets', 'mysterious', 'young', 'boy', 'little', 'prince'], ['little', 'prince', 'comes', 'small', 'asteroid', 'called', 'b-612', 'lives', 'alone', 'rose', 'loves', 'deeply'], ['recounts', 'journey', 'pilot', 'describing', 'visits', 'several', 'planets'], ['planet', 'inhabited', 'different', 'character', 'king', 'vain', 'man', 'drunkard', 'businessman', 'geographer', 'fox'], ['encounters', 'prince', 'learns', 'valuable', 'lessons', 'love', 'responsibility', 'nature', 'adult', 'behavior'], ['earth', 'little', 'prince', 'meets', 'various', 'creatures', 'including', 'fox', 'teaches', 'relationships', 'importance', 'taming', 'means', 'building', 'ties', 'others'], ['fox', 'famous', 'line', 'become', 'responsible', 'forever', 'tamed', 'resonates', 'prince', 'feelings', 'rose'], ['ultimately', 'little', 'prince', 'realizes', 'essence', 'life', 'often', 'invisible', 'seen', 'heart'], ['sharing', 'wisdom', 'pilot', 'prepares', 'return', 'asteroid', 'beloved', 'rose'], ['story', 'concludes', 'pilot', 'reflecting', 'lessons', 'learned', 'little', 'prince', 'enduring', 'impact', 'friendship'], ['narrative', 'beautifully', 'simple', 'yet', 'profound', 'exploration', 'love', 'loss', 'importance', 'seeing', 'beyond', 'surface', 'things']]\n",
      "{'little': 6, 'prince': 9, 'written': 1, 'antoine': 1, 'saint-exupéry': 1, 'poetic': 1, 'tale': 1, 'young': 2, 'travels': 1, 'home': 1, 'planet': 2, 'earth': 2, 'story': 2, 'begins': 1, 'pilot': 4, 'stranded': 1, 'sahara': 1, 'desert': 1, 'plane': 2, 'crashes': 1, 'trying': 1, 'fix': 1, 'meets': 2, 'mysterious': 1, 'boy': 1, 'comes': 1, 'small': 1, 'asteroid': 2, 'called': 1, 'b-612': 1, 'lives': 1, 'alone': 1, 'rose': 3, 'loves': 1, 'deeply': 1, 'recounts': 1, 'journey': 1, 'describing': 1, 'visits': 1, 'several': 1, 'planets': 1, 'inhabited': 1, 'different': 1, 'character': 1, 'king': 1, 'vain': 1, 'man': 1, 'drunkard': 1, 'businessman': 1, 'geographer': 1, 'fox': 3, 'encounters': 1, 'learns': 1, 'valuable': 1, 'lessons': 2, 'love': 2, 'responsibility': 1, 'nature': 1, 'adult': 1, 'behavior': 1, 'various': 1, 'creatures': 1, 'including': 1, 'teaches': 1, 'relationships': 1, 'importance': 2, 'taming': 1, 'means': 1, 'building': 1, 'ties': 1, 'others': 1, 'famous': 1, 'line': 1, 'become': 1, 'responsible': 1, 'forever': 1, 'tamed': 1, 'resonates': 1, 'feelings': 1, 'ultimately': 1, 'realizes': 1, 'essence': 1, 'life': 1, 'often': 1, 'invisible': 1, 'seen': 1, 'heart': 1, 'sharing': 1, 'wisdom': 1, 'prepares': 1, 'return': 1, 'beloved': 1, 'concludes': 1, 'reflecting': 1, 'learned': 1, 'enduring': 1, 'impact': 1, 'friendship': 1, 'narrative': 1, 'beautifully': 1, 'simple': 1, 'yet': 1, 'profound': 1, 'exploration': 1, 'loss': 1, 'seeing': 1, 'beyond': 1, 'surface': 1, 'things': 1}\n"
     ]
    }
   ],
   "source": [
    "# NLTK 전처리 : 문장/단어 토큰화 + 불용어/짧은 토큰 제거 + 단어 빈도 사전\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 문장 토큰화\n",
    "sentences = sent_tokenize(raw_text)\n",
    "\n",
    "# 영어 불용어 리스트\n",
    "en_stopwords = stopwords.words('english')\n",
    "\n",
    "# 단어사전 (key=단어, value=빈도)\n",
    "vocab = {}\n",
    "\n",
    "# 토큰화/정제/정규화 처리 결과\n",
    "preprocessed_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence = sentence.lower()                                          # 소문자 변환\n",
    "    tokens = word_tokenize(sentence)                                     # 문장을 단어(토큰) 리스트로 변환\n",
    "    tokens = [token for token in tokens if token not in en_stopwords]    # 불용어 토큰 제거\n",
    "    tokens = [token for token in tokens if len(token) > 2]               # 길이 2 이하 토큰 제거 (노이즈 감소)\n",
    "\n",
    "    # 전처리된 토큰들로 빈도 집계 \n",
    "    for token in tokens:\n",
    "        # token이 vocab에 없으면 빈도 1로 초기화, vocab에 이미 존재하면 +1\n",
    "        if token not in vocab:\n",
    "            vocab[token] = 1\n",
    "        else:\n",
    "            vocab[token] += 1\n",
    "\n",
    "    preprocessed_sentences.append(tokens)\n",
    "\n",
    "print(preprocessed_sentences)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c283a0a0",
   "metadata": {},
   "source": [
    "preprocessed_sentences : 문장별 토큰 리스트  \n",
    "vocab : 전체 단어의 빈도 딕셔너리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f577e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 2, 1, 1, 1, 1, 1, 7, 2, 1, 1, 8, 9],\n",
       " [10, 1, 4, 1, 1, 1, 11, 1],\n",
       " [1, 1, 11, 12, 1, 7, 1, 3, 2],\n",
       " [3, 2, 1, 1, 13, 1, 1, 1, 1, 5, 1, 1],\n",
       " [1, 1, 4, 1, 1, 1, 1],\n",
       " [8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6],\n",
       " [1, 2, 1, 1, 14, 1, 1, 1, 1, 1],\n",
       " [9, 3, 2, 12, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [6, 1, 1, 1, 1, 1, 1, 1, 2, 1, 5],\n",
       " [1, 3, 2, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 4, 1, 1, 13, 1, 5],\n",
       " [10, 1, 4, 1, 14, 1, 3, 2, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keras Tokenizer : 상위 단어 제한 + OOV 처리 + 정수 시퀀스 변환\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=15, oov_token='<OOV>')              # 상위 15개 단어, oov 토큰은 '<OOV>'로 사용\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)                      # 코퍼스에서 단어 빈도 기반 인덱스 사전 생성\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_sentences)    # 문장(토큰 리스트)을 정수 인덱스 시퀀스로 변환\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9aad52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  3,  2,  1,  1,  1,  1,  1,  7,  2,  1,  1,  8,  9],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0, 10,  1,  4,  1,  1,  1, 11,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  1,  1, 11, 12,  1,  7,  1,  3,  2],\n",
       "       [ 0,  0,  0,  0,  3,  2,  1,  1, 13,  1,  1,  1,  1,  5,  1,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  4,  1,  1,  1,  1],\n",
       "       [ 0,  0,  0,  0,  0,  8,  1,  1,  1,  1,  1,  1,  1,  1,  1,  6],\n",
       "       [ 0,  0,  0,  0,  0,  0,  1,  2,  1,  1, 14,  1,  1,  1,  1,  1],\n",
       "       [ 9,  3,  2, 12,  1,  1,  1,  6,  1,  1,  1,  1,  1,  1,  1,  1],\n",
       "       [ 0,  0,  0,  0,  0,  6,  1,  1,  1,  1,  1,  1,  1,  2,  1,  5],\n",
       "       [ 0,  0,  0,  0,  0,  0,  1,  3,  2,  1,  1,  1,  1,  1,  1,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  4,  1,  1, 13,  1,  5],\n",
       "       [ 0,  0,  0,  0,  0, 10,  1,  4,  1, 14,  1,  3,  2,  1,  1,  1],\n",
       "       [ 0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시퀀스 패딩/길이 맞춤 유틸 함수\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded = pad_sequences(sequences)    # 기본옵션 : pre padding, maxlen=최장길이, value = 0\n",
    "padded    # (문장 개수, 최장길이) 형태의 2D 배열"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
