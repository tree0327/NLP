{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ« 5. ì •ìˆ˜ ì¸ì½”ë”© (Integer Encoding): ë‹¨ì–´ì— ë²ˆí˜¸í‘œ ë¶™ì´ê¸°\n",
                "\n",
                "> **\"ì»´í“¨í„°ì•¼, 'ì‚¬ê³¼'ëŠ” 1ë²ˆ, 'ë°”ë‚˜ë‚˜'ëŠ” 2ë²ˆì´ì•¼!\"**\n",
                "\n",
                "ì»´í“¨í„°ëŠ” ê¸€ìë¥¼ ì‹«ì–´í•´. 100% ìˆ«ìë¡œ ë°”ê¿”ì¤˜ì•¼ ê³„ì‚°ì„ í•˜ì§€.\n",
                "ê·¸ë˜ì„œ ëª¨ë“  ë‹¨ì–´ì—ê²Œ **ê³ ìœ í•œ ID ë²ˆí˜¸**ë¥¼ ë¶€ì—¬í•˜ëŠ” ì‘ì—…ì´ì•¼.\n",
                "\n",
                "### ë°©ì‹\n",
                "- ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ì¼ìˆ˜ë¡ ì•ë²ˆí˜¸(1ë²ˆ, 2ë²ˆ...)ë¥¼ ì£¼ëŠ” ê²Œ êµ­ë£°ì´ì•¼. (íš¨ìœ¨ì„± ë•Œë¬¸!)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. ìˆ˜ë™ìœ¼ë¡œ ë§Œë“¤ì–´ë³´ê¸° (ì›ë¦¬ ì´í•´) ğŸ› ï¸\n",
                "\n",
                "íŒ¨í‚¤ì§€ ì“°ê¸° ì „ì— ì§ì ‘ êµ¬í˜„í•´ë³´ë©´ì„œ ì›ë¦¬ë¥¼ ê¹¨ìš°ì³ë³´ì."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "c943e4f3",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ë‹¨ì–´ ë¹ˆë„: [('my', 4), ('love', 3), ('dog', 3), ('i', 2), ('you', 2), ('cat', 1), ('do', 1), ('think', 1), ('is', 1), ('amazing', 1)]\n"
                    ]
                }
            ],
            "source": [
                "from nltk.tokenize import word_tokenize\n",
                "from collections import Counter\n",
                "\n",
                "sentences = [\n",
                "    \"I love my dog\",\n",
                "    \"I love my cat\",\n",
                "    \"You love my dog\",\n",
                "    \"Do you think my dog is amazing\"\n",
                "]\n",
                "\n",
                "# 1. ë‹¨ì–´ë“¤ ì‹¹ ë‹¤ ëª¨ìœ¼ê¸°\n",
                "vocab = []\n",
                "for sent in sentences:\n",
                "    vocab.extend(word_tokenize(sent.lower()))\n",
                "\n",
                "# 2. ë¹ˆë„ìˆ˜ ì„¸ê¸° & ë§ì´ ë‚˜ì˜¨ ìˆœì„œ ì •ë ¬\n",
                "word_counts = Counter(vocab)\n",
                "sorted_vocab = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
                "\n",
                "print(\"ë‹¨ì–´ ë¹ˆë„:\", sorted_vocab)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "bc09ecfb",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ë‹¨ì–´ ì‚¬ì „: {'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n"
                    ]
                }
            ],
            "source": [
                "# 3. ë²ˆí˜¸í‘œ ë¶€ì—¬ (1ë²ˆë¶€í„° ì‹œì‘)\n",
                "word_to_index = {}\n",
                "i = 0\n",
                "\n",
                "for word, count in sorted_vocab:\n",
                "    i += 1\n",
                "    word_to_index[word] = i\n",
                "\n",
                "print(\"ë‹¨ì–´ ì‚¬ì „:\", word_to_index)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "0697f9d6",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ë³€í™˜ ê²°ê³¼: [[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"
                    ]
                }
            ],
            "source": [
                "# 4. ë¬¸ì¥ì„ ìˆ«ìë¡œ ë³€í™˜!\n",
                "encoded_sentences = []\n",
                "for sent in sentences:\n",
                "    temp = []\n",
                "    for word in word_tokenize(sent.lower()):\n",
                "        temp.append(word_to_index[word])\n",
                "    encoded_sentences.append(temp)\n",
                "\n",
                "print(\"ë³€í™˜ ê²°ê³¼:\", encoded_sentences)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8f817ccd",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "260ee585",
            "metadata": {},
            "source": [
                "## 2. Kerasë¡œ í•œ ë°©ì— í•˜ê¸° (feat. ê³ ì„±ëŠ¥ ë¡œë´‡) ğŸ¤–ğŸš€\n",
                "\n",
                "ë°©ê¸ˆ ìš°ë¦¬ê°€ **'í† í°í™” â†’ ì •ë ¬ â†’ ì¸ë±ì‹±'** í•˜ë©° ê³ ìƒí–ˆë˜ 10ì¤„ì§œë¦¬ ì½”ë“œë¥¼,\n",
                "KerasëŠ” **ë”± 2~3ì¤„**ë¡œ ëë‚´ë²„ë ¤. ì‹¤ë¬´ì—ì„œëŠ” ë¬´ì¡°ê±´ ì´ê±¸ ì“´ë‹¤ê³  ë³´ë©´ ë¼.\n",
                "\n",
                "### 2-1. ê¸°ë³¸ ì‚¬ìš©ë²•\n",
                "Kerasì˜ `Tokenizer`ëŠ” ìš°ë¦¬ê°€ ë§Œë“  ìˆ˜ë™ ì½”ë“œì™€ ë˜‘ê°™ì€ ì›ë¦¬ë¡œ ì‘ë™í•˜ì§€ë§Œ, í›¨ì”¬ ë˜‘ë˜‘í•´."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "409a3725",
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.preprocessing.text import Tokenizer\n",
                "\n",
                "# 1. ë¡œë´‡(Tokenizer) ì†Œí™˜!\n",
                "tokenizer = Tokenizer()\n",
                "\n",
                "# 2. ë¡œë´‡ì—ê²Œ ë‹¨ì–´ ê°€ë¥´ì¹˜ê¸° (fit_on_texts)\n",
                "# - ì´ í•œ ì¤„ë¡œ 'í† í°í™” + ë¹ˆë„ìˆ˜ ê³„ì‚° + ì •ë ¬ + ì¸ë±ìŠ¤ ë¶€ì—¬'ê°€ ë‹¤ ëë‚¨!\n",
                "tokenizer.fit_on_texts(sentences)\n",
                "\n",
                "print(\"ë‹¨ì–´ ì‚¬ì „(word_index):\", tokenizer.word_index)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5f1a3495",
            "metadata": {},
            "source": [
                "### ğŸ’¡ ê²°ê³¼ í•´ì„\n",
                "- `fit_on_texts`ë¥¼ í•˜ë©´ ë‚´ë¶€ì ìœ¼ë¡œ ë¹ˆë„ìˆ˜ë¥¼ ì„¸ì„œ **ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ë‹¨ì–´ê°€ 1ë²ˆ**ì´ ë¼.\n",
                "- `word_index`ë¥¼ ì°ì–´ë³´ë©´ ìš°ë¦¬ê°€ ì•„ê¹Œ í˜ë“¤ê²Œ ë§Œë“  `sorted` ê²°ê³¼ì™€ ë˜‘ê°™ì§€?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "ac363d64",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. ë¬¸ì¥ì„ ìˆ«ìë¡œ ë³€í™˜í•˜ê¸° (texts_to_sequences)\n",
                "# - ì´ì œ ë°°ìš´ ê±¸ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì¥ì„ ìˆ«ìë¡œ ë°”ê¿”ì¤˜!\n",
                "encoded = tokenizer.texts_to_sequences(sentences)\n",
                "\n",
                "print(\"ë³€í™˜ ê²°ê³¼:\", encoded)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a6141641",
            "metadata": {},
            "source": [
                "### 2-2. Kerasê°€ ë” ì¢‹ì€ ì  (ê³ ê¸‰ ê¸°ëŠ¥) ğŸ’\n",
                "\n",
                "ìš°ë¦¬ê°€ ì§  ì½”ë“œì—ëŠ” í° ë¬¸ì œê°€ í•˜ë‚˜ ìˆì–´. **\"ëª¨ë¥´ëŠ” ë‹¨ì–´(Out-Of-Vocabulary)\"**ê°€ ë‚˜ì˜¤ë©´ ì—ëŸ¬ê°€ ë‚˜ê±°ë‚˜ ë¬´ì‹œí•´ë²„ë¦°ë‹¤ëŠ” ê±°ì•¼.\n",
                "í•˜ì§€ë§Œ KerasëŠ” **`oov_token`** í•˜ë‚˜ë§Œ ì„¤ì •í•˜ë©´, ëª¨ë¥´ëŠ” ë‹¨ì–´ë„ \"ëª¨ë¦„\"ì´ë¼ëŠ” ìˆ«ìë¡œ ë°”ê¿”ì¤˜!\n",
                "\n",
                "> **ì˜ˆì‹œ**: \"I love **zebra**\" (zebraëŠ” ë°°ìš´ ì  ì—†ìŒ)\n",
                "> - ì¼ë°˜: `[4, 2]` ('zebra' ì¦ë°œ)\n",
                "> - Keras(OOV ì„¤ì • ì‹œ): `[4, 2, 1]` ('zebra'ë¥¼ 'OOV'ì— í•´ë‹¹í•˜ëŠ” 1ë²ˆìœ¼ë¡œ ë³€í™˜)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "abd41890",
            "metadata": {},
            "outputs": [],
            "source": [
                "# OOV(Out-Of-Vocabulary) ì„¤ì •í•˜ê¸°\n",
                "# \"ëª¨ë¥´ëŠ” ë‹¨ì–´ê°€ ë‚˜ì˜¤ë©´ 'OOV'ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ë²ˆí˜¸(1ë²ˆ)ë¥¼ ì¤˜ë¼!\"\n",
                "tokenizer_new = Tokenizer(oov_token='OOV') \n",
                "tokenizer_new.fit_on_texts(sentences)\n",
                "\n",
                "# 'word_index'ì— ìë™ìœ¼ë¡œ 'OOV'ê°€ ì¶”ê°€ë¨ (ë³´í†µ 1ë²ˆ)\n",
                "print(\"OOV í¬í•¨ ë‹¨ì–´ì¥:\", tokenizer_new.word_index)\n",
                "\n",
                "# ì•ˆ ë°°ìš´ ë‹¨ì–´('zebra')ê°€ ìˆëŠ” ë¬¸ì¥ ë³€í™˜\n",
                "new_sentence = [\"I love zebra\"]\n",
                "print(\"ë³€í™˜ ê²°ê³¼:\", tokenizer_new.texts_to_sequences(new_sentence))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ“ ë§ˆë¬´ë¦¬ í€´ì¦ˆ\n",
                "\n",
                "**Q. ì •ìˆ˜ ì¸ì½”ë”©ì—ì„œ ë³´í†µ 'ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ë‹¨ì–´'ì—ê²Œ ë¶€ì—¬í•˜ëŠ” ë²ˆí˜¸ëŠ”?**\n",
                "1. 1ë²ˆ (ì•ë²ˆí˜¸)\n",
                "2. 10000ë²ˆ (ë’·ë²ˆí˜¸)\n",
                "3. ëœë¤\n",
                "\n",
                "<details>\n",
                "<summary>ì •ë‹µ í™•ì¸</summary>\n",
                "\n",
                "**1ë²ˆ** (ìì£¼ ì“°ëŠ” ê±´ ë¹¨ë¦¬ ì°¾ê²Œ ì•ë²ˆí˜¸!)\n",
                "\n",
                "</details>"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "nlp_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
